import * as __WEBPACK_EXTERNAL_MODULE_agora_rte_extension__ from "agora-rte-extension";
import * as __WEBPACK_EXTERNAL_MODULE_agora_rtc_sdk_ng__ from "agora-rtc-sdk-ng";
import * as __WEBPACK_EXTERNAL_MODULE_agora_extension_ai_denoiser__ from "agora-extension-ai-denoiser";
import * as __WEBPACK_EXTERNAL_MODULE_uuid__ from "uuid";
import * as __WEBPACK_EXTERNAL_MODULE_axios__ from "axios";
import * as __WEBPACK_EXTERNAL_MODULE_ws__ from "ws";
import * as __WEBPACK_EXTERNAL_MODULE_reconnecting_websocket__ from "reconnecting-websocket";
import * as __WEBPACK_EXTERNAL_MODULE_os__ from "os";
import * as __WEBPACK_EXTERNAL_MODULE_node_fetch__ from "node-fetch";
import "crypto";
import "jsonwebtoken";
import * as __WEBPACK_EXTERNAL_MODULE_agora_rtc_sdk_ng_esm__ from "agora-rtc-sdk-ng/esm";
import * as __WEBPACK_EXTERNAL_MODULE_opus_encdec_src_oggOpusEncoder_js__ from "opus-encdec/src/oggOpusEncoder.js";
import * as __WEBPACK_EXTERNAL_MODULE_opus_encdec_dist_libopus_encoder_js__ from "opus-encdec/dist/libopus-encoder.js";
import * as __WEBPACK_EXTERNAL_MODULE_opus_encdec_src_oggOpusDecoder_js__ from "opus-encdec/src/oggOpusDecoder.js";
import * as __WEBPACK_EXTERNAL_MODULE_opus_encdec_dist_libopus_decoder_js__ from "opus-encdec/dist/libopus-decoder.js";
// The require scope
var __webpack_require__ = {};
/************************************************************************/ // webpack/runtime/define_property_getters
(()=>{
    __webpack_require__.d = function(exports, definition) {
        for(var key in definition)if (__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) Object.defineProperty(exports, key, {
            enumerable: true,
            get: definition[key]
        });
    };
})();
// webpack/runtime/has_own_property
(()=>{
    __webpack_require__.o = function(obj, prop) {
        return Object.prototype.hasOwnProperty.call(obj, prop);
    };
})();
// webpack/runtime/make_namespace_object
(()=>{
    // define __esModule on exports
    __webpack_require__.r = function(exports) {
        if ('undefined' != typeof Symbol && Symbol.toStringTag) Object.defineProperty(exports, Symbol.toStringTag, {
            value: 'Module'
        });
        Object.defineProperty(exports, '__esModule', {
            value: true
        });
    };
})();
/************************************************************************/ // NAMESPACE OBJECT: ./src/ws-tools/utils/index.ts
var utils_namespaceObject = {};
__webpack_require__.r(utils_namespaceObject);
__webpack_require__.d(utils_namespaceObject, {
    checkDenoiserSupport: ()=>checkDenoiserSupport,
    checkDevicePermission: ()=>checkDevicePermission,
    downsampleTo8000: ()=>downsampleTo8000,
    encodeG711A: ()=>encodeG711A,
    encodeG711U: ()=>encodeG711U,
    float32ToInt16Array: ()=>float32ToInt16Array,
    floatTo16BitPCM: ()=>floatTo16BitPCM,
    getAudioDevices: ()=>getAudioDevices,
    isBrowserExtension: ()=>utils_isBrowserExtension,
    isHarmonOS: ()=>isHarmonOS,
    isMobile: ()=>isMobile,
    setValueByPath: ()=>setValueByPath
});
/* eslint-disable @typescript-eslint/no-explicit-any */ // 禁用日志上传与打印日志
__WEBPACK_EXTERNAL_MODULE_agora_rtc_sdk_ng__["default"].disableLogUpload();
__WEBPACK_EXTERNAL_MODULE_agora_rtc_sdk_ng__["default"].setLogLevel(3);
__WEBPACK_EXTERNAL_MODULE_agora_rte_extension__.logger.setLogLevel(3);
/**
 * Check audio device permissions
 * @returns {Promise<{audio: boolean}>} Whether audio device permission is granted
 */ const checkDevicePermission = async ()=>{
    const result = {
        audio: true
    };
    try {
        var _navigator_permissions;
        // Check if browser supports mediaDevices API
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            console.error('Browser does not support mediaDevices API');
            result.audio = false;
        }
        // Check permission status first through permissions API
        const permissionStatus = await (null === (_navigator_permissions = navigator.permissions) || void 0 === _navigator_permissions ? void 0 : _navigator_permissions.query({
            name: 'microphone'
        })) || {
            state: 'prompt'
        };
        // If permission has been denied
        if ('denied' === permissionStatus.state) {
            console.error('Microphone permission denied');
            result.audio = false;
        }
        // If permission status is prompt or granted, try to get device
        if ('prompt' === permissionStatus.state || 'granted' === permissionStatus.state) {
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: true
            });
            // After obtaining successfully, close the audio stream
            if (stream) stream.getTracks().forEach((track)=>track.stop());
        }
    } catch (error) {
        // User denied authorization or other errors
        console.error('Failed to get audio permission:', error);
        result.audio = false;
    }
    return result;
};
/**
 * Get list of audio devices
 * @returns {Promise<{audioInputs: MediaDeviceInfo[], audioOutputs: MediaDeviceInfo[]}>} Audio devices
 */ const getAudioDevices = async ()=>{
    try {
        // Request microphone permission first, so we can get the complete device information
        const { audio: audioPermission } = await checkDevicePermission();
        if (!audioPermission) throw new Error('Microphone permission denied');
        // Get all media devices
        const devices = await navigator.mediaDevices.enumerateDevices();
        if (!(null == devices ? void 0 : devices.length)) return {
            audioInputs: [],
            audioOutputs: []
        };
        return {
            audioInputs: devices.filter((i)=>i.deviceId && 'audioinput' === i.kind),
            audioOutputs: devices.filter((i)=>i.deviceId && 'audiooutput' === i.kind)
        };
    } catch (error) {
        console.error('Failed to get audio devices:', error);
        return {
            audioInputs: [],
            audioOutputs: []
        };
    }
};
/**
 * Convert floating point numbers to 16-bit PCM
 * @param float32Array - Array of floating point numbers
 * @returns {ArrayBuffer} 16-bit PCM
 */ const floatTo16BitPCM = (float32Array)=>{
    const buffer = new ArrayBuffer(2 * float32Array.length);
    const view = new DataView(buffer);
    let offset = 0;
    for(let i = 0; i < float32Array.length; i++, offset += 2){
        const s = Math.max(-1, Math.min(1, float32Array[i]));
        view.setInt16(offset, s < 0 ? 0x8000 * s : 0x7fff * s, true);
    }
    return buffer;
};
/**
 * Convert Float32Array to Int16Array (without going through ArrayBuffer)
 */ function float32ToInt16Array(float32) {
    const int16 = new Int16Array(float32.length);
    for(let i = 0; i < float32.length; i++){
        const s = Math.max(-1, Math.min(1, float32[i]));
        int16[i] = s < 0 ? 0x8000 * s : 0x7fff * s;
    }
    return int16;
}
/**
 * Simple linear extraction method to downsample Float32Array from 48000Hz to 8000Hz
 * @param input Float32Array 48000Hz
 * @returns Float32Array 8000Hz
 */ function downsampleTo8000(input) {
    const ratio = 6; // 6
    const outputLength = Math.floor(input.length / ratio);
    const output = new Float32Array(outputLength);
    for(let i = 0; i < outputLength; i++)output[i] = input[Math.floor(i * ratio)];
    return output;
}
/**
 * Check if device is mobile
 * @returns {boolean} Whether device is mobile
 */ const isMobile = ()=>/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
const isHarmonOS = ()=>/harmony|hmos|huawei/i.test(navigator.userAgent);
/**
 * Check if AI denoising is supported
 * @param assetsPath - Public path for denoising plugin
 * @returns {boolean} Whether AI denoising is supported
 */ const checkDenoiserSupport = (assetsPath)=>{
    if (void 0 !== window.__denoiserSupported) return window.__denoiserSupported;
    // Pass in the public path where the Wasm file is located to create an AIDenoiserExtension instance, path does not end with / "
    const external = window.__denoiser || new __WEBPACK_EXTERNAL_MODULE_agora_extension_ai_denoiser__.AIDenoiserExtension({
        assetsPath: null != assetsPath ? assetsPath : 'https://lf3-static.bytednsdoc.com/obj/eden-cn/613eh7lpqvhpeuloz/websocket'
    });
    window.__denoiser = external;
    external.onloaderror = (e)=>{
        // If the Wasm file fails to load, you can disable the plugin, for example:
        console.error('Denoiser load error', e);
        window.__denoiserSupported = false;
    };
    // Check compatibility
    if (external.checkCompatibility()) {
        // Register the plugin
        // see https://github.com/AgoraIO/API-Examples-Web/blob/main/src/example/extension/aiDenoiser/agora-extension-ai-denoiser/README.md
        __WEBPACK_EXTERNAL_MODULE_agora_rtc_sdk_ng__["default"].registerExtensions([
            external
        ]);
        window.__denoiserSupported = true;
        return true;
    }
    // The current browser may not support the AI denoising plugin, you can stop executing subsequent logic
    console.error('Does not support AI Denoiser!');
    window.__denoiserSupported = false;
    return false;
};
const utils_isBrowserExtension = ()=>'undefined' != typeof chrome && !!chrome.runtime && !!chrome.runtime.id;
/**
 * Convert 16-bit linear PCM data to G.711 A-law
 * @param {Int16Array|Array} pcmData - 16-bit signed PCM sample data
 * @returns {Uint8Array} - G.711 A-law encoded data
 */ function encodeG711A(pcmData) {
    const aLawData = new Uint8Array(pcmData.length);
    // A-law compression table - used to optimize performance
    const LOG_TABLE = [
        1,
        1,
        2,
        2,
        3,
        3,
        3,
        3,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        6,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        7
    ];
    for(let i = 0; i < pcmData.length; i++){
        let sample = pcmData[i];
        const sign = sample < 0 ? 0 : 0x80;
        // Get the absolute value of the sample and limit it to the 16-bit range
        if (0 === sign) sample = -sample;
        if (sample > 32767) sample = 32767;
        // Use linear quantization for small signals, logarithmic quantization for large signals
        let compressedValue;
        if (sample < 256) compressedValue = sample >> 4;
        else {
            // Determine the "exponent" part of the sample
            const exponent = LOG_TABLE[sample >> 8 & 0x7f];
            const mantissa = sample >> exponent + 3 & 0x0f;
            compressedValue = exponent << 4 | mantissa;
        }
        // Invert even bits (this is a feature of A-law)
        aLawData[i] = (sign | compressedValue) ^ 0x55;
    }
    return aLawData;
}
/**
 * Encode 16-bit PCM to G.711 μ-law (g711u)
 * @param pcm16 - Int16Array of PCM samples
 * @returns {Uint8Array} G.711U encoded data
 */ function encodeG711U(pcm16) {
    const BIAS = 0x84;
    const CLIP = 32635;
    const out = new Uint8Array(pcm16.length);
    for(let i = 0; i < pcm16.length; i++){
        let pcm = pcm16[i];
        const sign = pcm >> 8 & 0x80;
        if (0 !== sign) pcm = -pcm;
        if (pcm > CLIP) pcm = CLIP;
        pcm += BIAS;
        let exponent = 7;
        for(let expMask = 0x4000; (pcm & expMask) === 0 && exponent > 0; expMask >>= 1)exponent--;
        const mantissa = pcm >> exponent + 3 & 0x0f;
        const ulaw = ~(sign | exponent << 4 | mantissa);
        out[i] = ulaw;
    }
    return out;
}
/**
 * Sets a value in an object at a specified path using dot notation.
 * Creates nested objects along the path if they don't exist.
 *
 * @param obj - The target object to modify
 * @param path - The path in dot notation (e.g., 'a.b.c')
 * @param value - The value to set at the specified path
 * @returns The modified object
 *
 * @example
 * // Set a value at a nested path
 * const obj = {};
 * setValueByPath(obj, 'user.profile.name', 'John');
 * // Result: { user: { profile: { name: 'John' } } }
 */ function setValueByPath(obj, path, value) {
    if (!obj || 'object' != typeof obj) throw new Error('Target must be an object');
    if (!path) throw new Error('Path cannot be empty');
    const keys = path.split('.');
    let current = obj;
    // Navigate to the last-but-one key
    for(let i = 0; i < keys.length - 1; i++){
        const key = keys[i];
        // Skip dangerous keys to prevent prototype pollution
        if ('__proto__' === key || 'constructor' === key) throw new Error(`Invalid key detected: ${key}`);
        // Create empty object if the key doesn't exist or is not an object
        if (!current[key] || 'object' != typeof current[key]) current[key] = {};
        current = current[key];
    }
    // Set the value at the final key
    const lastKey = keys[keys.length - 1];
    if ('__proto__' === lastKey || 'constructor' === lastKey) throw new Error(`Invalid key detected: ${lastKey}`);
    current[lastKey] = value;
    return obj;
}
const StreamProcessorWorklet = `
class StreamProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.hasStarted = false;
    this.hasInterrupted = false;
    this.outputBuffers = [];
    this.bufferLength = 128;
    this.write = { buffer: new Float32Array(this.bufferLength), trackId: null };
    this.writeOffset = 0;
    this.trackSampleOffsets = {};
    this.volume = 1.0;
    this.port.onmessage = (event) => {
      if (event.data) {
        const payload = event.data;
        if (payload.event === 'write') {
          const int16Array = payload.buffer;
          const float32Array = new Float32Array(int16Array.length);
          for (let i = 0; i < int16Array.length; i++) {
            float32Array[i] = int16Array[i] / 0x8000; // Convert Int16 to Float32
          }
          this.writeData(float32Array, payload.trackId);
        } else if (payload.event === 'volume') {
          this.volume = payload.volume;
        } else if (
          payload.event === 'offset' ||
          payload.event === 'interrupt'
        ) {
          const requestId = payload.requestId;
          const trackId = this.write.trackId;
          const offset = this.trackSampleOffsets[trackId] || 0;
          this.port.postMessage({
            event: 'offset',
            requestId,
            trackId,
            offset,
          });
          if (payload.event === 'interrupt') {
            this.hasInterrupted = true;
          }
        } else {
          throw new Error(\`Unhandled event "\${payload.event}"\`);
        }
      }
    };
  }

  writeData(float32Array, trackId = null) {
    let { buffer } = this.write;
    let offset = this.writeOffset;
    for (let i = 0; i < float32Array.length; i++) {
      buffer[offset++] = float32Array[i];
      if (offset >= buffer.length) {
        this.outputBuffers.push(this.write);
        this.write = { buffer: new Float32Array(this.bufferLength), trackId };
        buffer = this.write.buffer;
        offset = 0;
      }
    }
    this.writeOffset = offset;
    return true;
  }

  process(inputs, outputs, parameters) {
    const output = outputs[0];
    const outputChannelData = output[0];
    const outputBuffers = this.outputBuffers;
    if (this.hasInterrupted) {
      this.port.postMessage({ event: 'stop' });
      return false;
    } else if (outputBuffers.length) {
      if(!this.hasStarted){
        this.hasStarted = true;
        this.port.postMessage({ event: 'first_frame' });
      }
      const { buffer, trackId } = outputBuffers.shift();
      for (let i = 0; i < outputChannelData.length; i++) {
        outputChannelData[i] = (buffer[i] || 0) * this.volume;
      }
      if (trackId) {
        this.trackSampleOffsets[trackId] =
          this.trackSampleOffsets[trackId] || 0;
        this.trackSampleOffsets[trackId] += buffer.length;
      }
      return true;
    } else if (this.hasStarted) {
      this.port.postMessage({ event: 'stop' });
      return false;
    } else {
      return true;
    }
  }
}

registerProcessor('stream-processor', StreamProcessor);
`;
let src = '';
if (utils_isBrowserExtension()) src = chrome.runtime.getURL('stream-processor.js');
else {
    const script = new Blob([
        StreamProcessorWorklet
    ], {
        type: 'application/javascript'
    });
    src = URL.createObjectURL(script);
}
const StreamProcessorSrc = src;
/**
 * Local audio loopback implementation using WebRTC peer connections
 * to create a local audio communication channel.
 * 完整的音频回环生命周期管理：
 * connect() - 建立初始连接
 * start() - 开始音频回环
 * stop() - 暂停音频回环
 * cleanup() - 完全清理所有资源
 */ class LocalLoopback {
    /**
   * Establishes a connection between two RTCPeerConnection objects
   * to create a local audio loopback channel
   * @param context - The AudioContext to use for audio processing
   * @param stream - The MediaStream to use for the loopback
   */ async connect(context, stream) {
        const servers = {
            iceServers: [],
            iceCandidatePoolSize: 1
        };
        this.mediaStream = stream;
        const pc1 = new RTCPeerConnection(servers);
        pc1.onicecandidate = (e)=>this.onIceCandidate(pc1, e);
        pc1.oniceconnectionstatechange = (e)=>this.onIceStateChange(pc1, e);
        this._debug('Created local peer connection object pc1');
        const pc2 = new RTCPeerConnection(servers);
        pc2.onicecandidate = (e)=>this.onIceCandidate(pc2, e);
        pc2.oniceconnectionstatechange = (e)=>this.onIceStateChange(pc2, e);
        pc2.ontrack = this.gotRemoteStream;
        this._debug('Created remote peer connection object pc2');
        const filteredStream = this.applyFilter(context);
        if (!filteredStream) {
            pc1.close();
            pc2.close();
            return;
        }
        filteredStream.getTracks().forEach((track)=>pc1.addTrack(track, filteredStream));
        pc1.createOffer({
            iceRestart: true
        }).then(this.gotDescription1).catch((error)=>console.log(`createOffer failed: ${error}`));
        this.pc1 = pc1;
        this.pc2 = pc2;
    }
    /**
   * 检查WebRTC连接状态，确保 ICE State 处于 connected 状态
   * @returns
   */ async checkForReady() {
        var _this_pc1;
        // 检查ICE连接状态
        // WebRTC连接状态可能是: new, checking, connected, completed, failed, disconnected, closed
        const validStates = [
            'connected',
            'completed'
        ];
        var _this_pc1_iceConnectionState;
        if (validStates.includes(null !== (_this_pc1_iceConnectionState = null === (_this_pc1 = this.pc1) || void 0 === _this_pc1 ? void 0 : _this_pc1.iceConnectionState) && void 0 !== _this_pc1_iceConnectionState ? _this_pc1_iceConnectionState : '')) return true;
        var _this_pc11;
        this._debug(`WebRTC connection not ready, current state: ${null === (_this_pc11 = this.pc1) || void 0 === _this_pc11 ? void 0 : _this_pc11.iceConnectionState}`);
        await this.sleep(300);
        await new Promise((resolve, reject)=>{
            if (!this.pc1 || !this.pc2) {
                this._error('WebRTC peer connections not initialized');
                reject(new Error('WebRTC peer connections not initialized'));
                return;
            }
            let retryCount = 0;
            const handleReconnect = async ()=>{
                retryCount++;
                // 重试 5 次
                if (retryCount >= 5) {
                    this._error('WebRTC connection not ready');
                    reject(new Error('WebRTC connection not ready'));
                    return;
                }
                const result = await this.reconnect();
                if (result) {
                    this._debug('WebRTC connection reestablished');
                    resolve(true);
                } else setTimeout(()=>{
                    handleReconnect();
                }, 2000);
            };
            setTimeout(()=>{
                handleReconnect();
            }, 500);
        });
    }
    /**
   * Starts the audio loopback by connecting the provided AudioWorkletNode
   * to the peer destination
   * @param streamNode - The AudioWorkletNode to connect to the peer destination
   */ start(streamNode) {
        if (!this.context || !this.peer) {
            this._error('No audio context or peer found');
            return;
        }
        if ('running' !== this.context.state) {
            this._error('Audio context is not running');
            return;
        }
        // 检查WebRTC连接状态
        if (!this.pc1 || !this.pc2) {
            this._error('WebRTC peer connections not initialized');
            return;
        }
        this.currentStreamNode = streamNode;
        streamNode.connect(this.peer);
        this._debug('local loopback start');
    }
    /**
   * Stops the audio loopback temporarily without destroying connections
   * Can be restarted by calling start() again
   */ stop() {
        if (!this.currentStreamNode) {
            this._debug('No active stream to stop');
            return;
        }
        try {
            // Disconnect the stream node from the peer destination
            if (this.peer) this.currentStreamNode.disconnect(this.peer);
            this.currentStreamNode = void 0;
            this._debug('local loopback stopped');
        } catch (err) {
            this._error('Error stopping local loopback:', err);
        }
    }
    /**
   * Reconnects the WebRTC peer connections
   * This method closes existing connections and establishes new ones
   * while preserving the audio context and stream configuration
   */ async reconnect() {
        this._debug('Reconnecting WebRTC peer connections');
        // Save current context and stream
        const currentContext = this.context;
        const currentStream = this.mediaStream;
        const currentStreamNode = this.currentStreamNode;
        // Close existing peer connections but don't fully clean up
        if (this.pc1) {
            this.pc1.onicecandidate = null;
            this.pc1.oniceconnectionstatechange = null;
            this.pc1.close();
            this.pc1 = void 0;
        }
        if (this.pc2) {
            this.pc2.onicecandidate = null;
            this.pc2.oniceconnectionstatechange = null;
            this.pc2.ontrack = null;
            this.pc2.close();
            this.pc2 = void 0;
        }
        // Wait a short time to ensure connections are properly closed
        await this.sleep(500);
        // Reestablish connection if we have the necessary context
        if (currentContext) {
            await this.connect(currentContext, currentStream);
            // If we were previously streaming, reconnect the stream node
            if (currentStreamNode && this.peer) {
                this._debug('Reestablishing audio connection');
                // Wait for ICE connection to establish
                const maxAttempts = 10;
                let attempts = 0;
                while(attempts < maxAttempts){
                    if (this.pc1) {
                        const pc1 = this.pc1;
                        const state = pc1.iceConnectionState;
                        if ('connected' === state || 'completed' === state) break;
                    }
                    await this.sleep(500);
                    attempts++;
                    this._debug(`Waiting for ICE connection (${attempts}/${maxAttempts})`);
                }
                // Reconnect the stream node
                if (this.pc1) {
                    const pc1 = this.pc1;
                    const state = pc1.iceConnectionState;
                    if ('connected' === state || 'completed' === state) {
                        currentStreamNode.connect(this.peer);
                        this.currentStreamNode = currentStreamNode;
                        this._debug('Audio connection reestablished');
                        return true;
                    }
                }
                this._warn('Failed to establish ICE connection after multiple attempts');
                return false;
            }
            return true;
        }
        this._error('Cannot reconnect - no audio context available');
        return false;
    }
    sleep(ms) {
        return new Promise((resolve)=>setTimeout(resolve, ms));
    }
    /**
   * Creates and connects audio processing nodes for the media stream
   * @param context - The AudioContext to use for creating audio nodes
   * @returns The processed MediaStream or undefined if no stream is available
   * @private
   */ applyFilter(context) {
        if (!this.mediaStream) {
            this._error('No media stream found');
            return;
        }
        this.context = context;
        this.mic = this.context.createMediaStreamSource(this.mediaStream);
        this.peer = this.context.createMediaStreamDestination();
        this.mic.connect(this.peer);
        return this.peer.stream;
    }
    /**
   * Handles the incoming remote stream from the peer connection
   * @param e - The RTCTrackEvent containing the remote stream
   * @private
   */ gotRemoteStream(e) {
        this._debug('pc2 received remote stream', e.streams[0]);
        if (this.remoteAudio.srcObject !== e.streams[0]) {
            this.remoteAudio.srcObject = e.streams[0];
            this.remoteAudio.muted = false;
            this.remoteAudio.volume = 0.5;
            const playPromise = this.remoteAudio.play();
            if (playPromise) playPromise.catch((err)=>{
                this._error('Failed to play audio:', err);
                // If autoplay is prevented, try unlocking the audio context again
                this._unlockAudioContext();
            });
        }
    }
    /**
   * Handles the SDP offer from the first peer connection (pc1)
   * @param desc - The RTCSessionDescriptionInit containing the SDP offer
   * @private
   */ async gotDescription1(desc) {
        var _this_pc1, _this_pc2, _this_pc21;
        this._debug(`Offer from pc1\n${desc.sdp}`);
        await (null === (_this_pc1 = this.pc1) || void 0 === _this_pc1 ? void 0 : _this_pc1.setLocalDescription(desc));
        await (null === (_this_pc2 = this.pc2) || void 0 === _this_pc2 ? void 0 : _this_pc2.setRemoteDescription(desc));
        null === (_this_pc21 = this.pc2) || void 0 === _this_pc21 || _this_pc21.createAnswer().then(this.gotDescription2).catch((error)=>console.error(`createAnswer failed: ${error}`));
    }
    /**
   * Handles the SDP answer from the second peer connection (pc2)
   * @param desc - The RTCSessionDescriptionInit containing the SDP answer
   * @private
   */ async gotDescription2(desc) {
        var _this_pc2, _this_pc1;
        this._debug(`Answer from pc2\n${desc.sdp}`);
        await (null === (_this_pc2 = this.pc2) || void 0 === _this_pc2 ? void 0 : _this_pc2.setLocalDescription(desc));
        await (null === (_this_pc1 = this.pc1) || void 0 === _this_pc1 ? void 0 : _this_pc1.setRemoteDescription(desc));
    }
    /**
   * Processes ICE candidates and forwards them to the other peer connection
   * @param pc - The RTCPeerConnection that generated the candidate
   * @param event - The RTCPeerConnectionIceEvent containing the candidate
   * @private
   */ onIceCandidate(pc, event) {
        var _this_getOtherPc;
        null === (_this_getOtherPc = this.getOtherPc(pc)) || void 0 === _this_getOtherPc || _this_getOtherPc.addIceCandidate(event.candidate).then(()=>this.onAddIceCandidateSuccess(pc), (err)=>this.onAddIceCandidateError(pc, err));
        this._debug(`${this.getName(pc)} ICE candidate:\n${event.candidate ? event.candidate.candidate : '(null)'}`);
    }
    /**
   * Returns the other peer connection (pc1 or pc2) based on the input
   * @param pc - The RTCPeerConnection to find the counterpart for
   * @returns The other RTCPeerConnection
   * @private
   */ getOtherPc(pc) {
        return pc === this.pc1 ? this.pc2 : this.pc1;
    }
    /**
   * Returns the name ('pc1' or 'pc2') of the peer connection for logging
   * @param pc - The RTCPeerConnection to get the name for
   * @returns The name of the peer connection
   * @private
   */ getName(pc) {
        return pc === this.pc1 ? 'pc1' : 'pc2';
    }
    /**
   * Handles successful addition of an ICE candidate
   * @param pc - The RTCPeerConnection that successfully added the candidate
   * @private
   */ onAddIceCandidateSuccess(pc) {
        this._debug(`${this.getName(pc)} addIceCandidate success`);
    }
    /**
   * Handles errors that occur when adding an ICE candidate
   * @param pc - The RTCPeerConnection that failed to add the candidate
   * @param error - The error that occurred
   * @private
   */ onAddIceCandidateError(pc, error) {
        this._error(`${this.getName(pc)} addIceCandidate failed: ${error}`);
    }
    /**
   * Handles ICE connection state changes
   * @param pc - The RTCPeerConnection whose ICE state changed
   * @param event - The event object containing state change information
   * @private
   */ onIceStateChange(pc, event) {
        if (pc) {
            this._debug(`${this.getName(pc)} ICE state: ${pc.iceConnectionState}`);
            this._debug('ICE state change event: ', event);
        }
    }
    /**
   * Logs debug information if debug mode is enabled
   * @param args - Arguments to pass to console.log
   * @private
   */ _debug() {
        for(var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++)args[_key] = arguments[_key];
        if (this.isDebug) console.log(...args);
    }
    /**
   * Logs error messages to the console
   * @param args - Arguments to pass to console.error
   * @private
   */ _error() {
        for(var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++)args[_key] = arguments[_key];
        console.error(...args);
    }
    /**
   * Logs warning messages to the console
   * @param args - Arguments to pass to console.warn
   * @private
   */ _warn() {
        for(var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++)args[_key] = arguments[_key];
        console.warn(...args);
    }
    /**
   * Attempts to unlock the audio context for iOS devices
   * Creates a silent audio element and plays it on user interaction
   * to bypass iOS autoplay restrictions
   * @private
   */ _unlockAudioContext() {
        // Create a silent audio element
        const silentSound = document.createElement('audio');
        silentSound.setAttribute('src', 'data:audio/mp3;base64,//MkxAAHiAICWABElBeKPL/RANb2w+yiT1g/gTok//lP/W/l3h8QO/OCdCqCW2Cw//MkxAQHkAIWUAhEmAQXWUOFW2dxPu//9mr60ElY5sseQ+xxesmHKtZr7bsqqX2L//MkxAgFwAYiQAhEAC2hq22d3///9FTV6tA36JdgBJoOGgc+7qvqej5EPomQ+RMn/QmSACAv7mcADf//MkxBQHAAYi8AhEAO193vt9KGOq+6qcT7hhfN5FTInmwk8RkqKImTM55pRQHQSq//MkxBsGkgoIAABHhTACIJLf99nVI///yuW1uBqWfEu7CgNPWGpUadBmZ////4sL//MkxCMHMAH9iABEmAsKioqKigsLCwtVTEFNRTMuOTkuNVVVVVVVVVVVVVVVVVVV//MkxCkECAUYCAAAAFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV');
        silentSound.volume = 0.001; // Very low volume, essentially silent
        // Add event listeners for user interaction events
        const pageEvents = [
            'touchstart',
            'touchend',
            'mousedown',
            'keydown'
        ];
        const unlockAudio = ()=>{
            this._debug('User interaction detected, trying to unlock audio');
            const playPromise = silentSound.play();
            if (playPromise) playPromise.catch(()=>{
            // Catch error but don't handle it
            }).then(()=>{
                // Also try to play the actual remote audio
                const remotePlayPromise = this.remoteAudio.play();
                if (remotePlayPromise) remotePlayPromise.catch(()=>{
                // Catch error but don't process it
                });
                // Remove all event listeners once succeeded
                pageEvents.forEach((event)=>{
                    document.removeEventListener(event, unlockAudio);
                });
                this._debug('Audio context unlocked');
            });
        };
        // Add all event listeners and track them for later cleanup
        pageEvents.forEach((event)=>{
            document.addEventListener(event, unlockAudio);
            this.eventListeners.push({
                element: document,
                event,
                handler: unlockAudio
            });
        });
        // Also try to play immediately
        setTimeout(()=>{
            this._debug('Attempting initial audio unlock');
            unlockAudio();
        }, 100);
    }
    /**
   * Cleans up all resources used by the LocalLoopback instance
   * This should be called when the instance is no longer needed to prevent memory leaks
   */ cleanup() {
        this._debug('Cleaning up LocalLoopback resources');
        // Close peer connections
        if (this.pc1) {
            // 1. 关闭所有轨道（摄像头/麦克风）
            this.pc1.getSenders().forEach((sender)=>{
                if (sender.track) sender.track.stop(); // 停止媒体轨道
            });
            // 2. 移除所有事件监听器（避免内存泄漏）
            this.pc1.onicecandidate = null;
            this.pc1.oniceconnectionstatechange = null;
            this.pc1.close();
            this.pc1 = void 0;
        }
        if (this.pc2) {
            // 1. 关闭所有轨道（摄像头/麦克风）
            this.pc2.getSenders().forEach((sender)=>{
                if (sender.track) sender.track.stop(); // 停止媒体轨道
            });
            // 2. 移除所有事件监听器（避免内存泄漏）
            this.pc2.onicecandidate = null;
            this.pc2.oniceconnectionstatechange = null;
            this.pc2.close();
            this.pc2 = void 0;
        }
        // Cleanup media stream
        if (this.mediaStream) {
            // Stop all tracks in the media stream
            this.mediaStream.getTracks().forEach((track)=>{
                track.stop();
            });
            this.mediaStream = void 0;
        }
        // Clean up current stream node
        if (this.currentStreamNode) {
            try {
                this.currentStreamNode.disconnect();
            } catch (e) {
            // Ignore errors during disconnect
            }
            this.currentStreamNode = void 0;
        }
        // Disconnect audio nodes
        if (this.mic) {
            this.mic.disconnect();
            this.mic = void 0;
        }
        if (this.peer) {
            this.peer.disconnect();
            this.peer = void 0;
        }
        // Clean up HTML audio element
        if (this.remoteAudio) {
            this.remoteAudio.pause();
            this.remoteAudio.srcObject = null;
            if (this.remoteAudio.parentNode) this.remoteAudio.parentNode.removeChild(this.remoteAudio);
        }
        // Remove any registered event listeners
        this.eventListeners.forEach((param)=>{
            let { element, event, handler } = param;
            element.removeEventListener(event, handler);
        });
        this.eventListeners = [];
        this._debug('LocalLoopback cleanup complete');
    }
    /**
   * Initializes a new instance of LocalLoopback
   * @param isDebug - Whether to enable debug logging
   */ constructor(isDebug = false){
        this.eventListeners = [];
        this.remoteAudio = document.createElement('audio');
        this.remoteAudio.setAttribute('autoplay', 'true');
        this.remoteAudio.setAttribute('muted', 'true');
        this.remoteAudio.setAttribute('style', 'display: none');
        document.body.appendChild(this.remoteAudio);
        this.isDebug = isDebug;
        // Unlock audio context for iOS devices
        this._unlockAudioContext();
        this.gotDescription1 = this.gotDescription1.bind(this);
        this.gotDescription2 = this.gotDescription2.bind(this);
        this.gotRemoteStream = this.gotRemoteStream.bind(this);
    }
}
/* ESM default export */ const local_loopback = LocalLoopback;
/**
 * G.711 codec implementation for A-law and μ-law
 */ // A-law to linear PCM conversion table
const ALAW_TO_LINEAR_TABLE = new Int16Array(256);
// μ-law to linear PCM conversion table
const ULAW_TO_LINEAR_TABLE = new Int16Array(256);
// Initialize conversion tables
(function() {
    // A-law to linear PCM conversion
    for(let i = 0; i < 256; i++){
        const aval = 0x55 ^ i;
        let t = (0x0f & aval) << 4;
        let seg = (0x70 & aval) >> 4;
        if (seg) t = t + 0x108 << seg - 1;
        else t += 8;
        ALAW_TO_LINEAR_TABLE[i] = 0x80 & aval ? t : -t;
    }
    // μ-law to linear PCM conversion
    for(let i = 0; i < 256; i++){
        const uval = ~i;
        let t = ((0x0f & uval) << 3) + 0x84;
        let seg = (0x70 & uval) >> 4;
        t <<= seg;
        ULAW_TO_LINEAR_TABLE[i] = 0x80 & uval ? 0x84 - t : t - 0x84;
    }
})();
/**
 * Converts G.711 A-law encoded data to PCM16 format
 * @param {Uint8Array} alawData - A-law encoded data
 * @returns {Int16Array} - PCM16 data
 */ function decodeAlaw(alawData) {
    const pcmData = new Int16Array(alawData.length);
    for(let i = 0; i < alawData.length; i++)pcmData[i] = ALAW_TO_LINEAR_TABLE[alawData[i]];
    return pcmData;
}
/**
 * Converts G.711 μ-law encoded data to PCM16 format
 * @param {Uint8Array} ulawData - μ-law encoded data
 * @returns {Int16Array} - PCM16 data
 */ function decodeUlaw(ulawData) {
    const pcmData = new Int16Array(ulawData.length);
    for(let i = 0; i < ulawData.length; i++)pcmData[i] = ULAW_TO_LINEAR_TABLE[ulawData[i]];
    return pcmData;
}
/**
 * Plays audio streams received in raw PCM16, G.711a, or G.711u chunks from the browser
 * @class
 */ class WavStreamPlayer {
    /**
   * Connects the audio context and enables output to speakers
   * @returns {Promise<true>}
   */ async connect() {
        this.context = new AudioContext({
            sampleRate: this.sampleRate
        });
        if (this.enableLocalLoopback) {
            var _this_localLoopback;
            await (null === (_this_localLoopback = this.localLoopback) || void 0 === _this_localLoopback ? void 0 : _this_localLoopback.connect(this.context, this.localLoopbackStream));
        }
        if ('suspended' === this.context.state) await this.context.resume();
        try {
            await this.context.audioWorklet.addModule(this.scriptSrc);
        } catch (e) {
            console.error(e);
            throw new Error(`Could not add audioWorklet module: ${this.scriptSrc}`);
        }
        return true;
    }
    /**
   * Pauses audio playback
   */ async pause() {
        if (this.context && !this.isPaused) {
            await this.context.suspend();
            this.isPaused = true;
        }
    }
    /**
   * Resumes audio playback
   */ async resume() {
        if (this.context && this.isPaused) {
            await this.context.resume();
            this.isPaused = false;
        }
    }
    /**
   * Toggles between play and pause states
   */ async togglePlay() {
        if (this.isPaused) await this.resume();
        else await this.pause();
    }
    /**
   * Checks if audio is currently playing
   * @returns {boolean}
   */ isPlaying() {
        return Boolean(this.context && this.streamNode && !this.isPaused && 'running' === this.context.state);
    }
    /**
   * 如果使用了本地回环，需要确保音频上下文已经准备好
   * @returns {Promise<void>}
   */ async checkForReady() {
        if (this.localLoopback && !this.context) {
            await this._start();
            await this.localLoopback.checkForReady();
        }
    }
    /**
   * Starts audio streaming
   * @private
   * @returns {Promise<true>}
   */ async _start() {
        // Ensure worklet is loaded
        if (!this.context) await this.connect();
        const streamNode = new AudioWorkletNode(this.context, 'stream-processor');
        streamNode.port.onmessage = (e)=>{
            const { event } = e.data;
            if ('stop' === event) {
                if (this.localLoopback) this.localLoopback.stop();
                else streamNode.disconnect();
                this.streamNode = null;
            } else if ('offset' === event) {
                const { requestId, trackId, offset } = e.data;
                const currentTime = offset / this.sampleRate;
                this.trackSampleOffsets[requestId] = {
                    trackId,
                    offset,
                    currentTime
                };
            }
        };
        if (this.enableLocalLoopback) {
            var _this_localLoopback;
            null === (_this_localLoopback = this.localLoopback) || void 0 === _this_localLoopback || _this_localLoopback.start(streamNode);
        } else streamNode.connect(this.context.destination);
        this.streamNode = streamNode;
        return true;
    }
    /**
   * Adds audio data to the currently playing audio stream
   * You can add chunks beyond the current play point and they will be queued for play
   * @param {ArrayBuffer|Int16Array|Uint8Array} arrayBuffer
   * @param {string} [trackId]
   * @param {AudioFormat} [format] - Audio format: 'pcm', 'g711a', or 'g711u'
   */ async add16BitPCM(arrayBuffer) {
        let trackId = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : 'default', format = arguments.length > 2 ? arguments[2] : void 0;
        if ('string' != typeof trackId) throw new Error("trackId must be a string");
        this.interruptedTrackIds[trackId];
        if (!this.streamNode) await this._start();
        let buffer;
        const audioFormat = format || this.defaultFormat;
        if (arrayBuffer instanceof Int16Array) // Already in PCM format
        buffer = arrayBuffer;
        else if (arrayBuffer instanceof Uint8Array) // Handle G.711 formats
        buffer = 'g711a' === audioFormat ? decodeAlaw(arrayBuffer) : 'g711u' === audioFormat ? decodeUlaw(arrayBuffer) : new Int16Array(arrayBuffer.buffer);
        else if (arrayBuffer instanceof ArrayBuffer) // Handle different formats based on the specified format
        buffer = 'g711a' === audioFormat ? decodeAlaw(new Uint8Array(arrayBuffer)) : 'g711u' === audioFormat ? decodeUlaw(new Uint8Array(arrayBuffer)) : new Int16Array(arrayBuffer);
        else throw new Error("argument must be Int16Array, Uint8Array, or ArrayBuffer");
        // 使用 Transferable 对象传递 ArrayBuffer，避免数据复制
        // 注意：只能传递 buffer.buffer，因为 buffer 是 Int16Array
        const transferableBuffer = buffer.buffer;
        this.streamNode.port.postMessage({
            event: 'write',
            buffer,
            trackId
        }, [
            transferableBuffer
        ]);
    }
    /**
   * Gets the offset (sample count) of the currently playing stream
   * @param {boolean} [interrupt]
   * @returns {{trackId: string|null, offset: number, currentTime: number} | null}
   */ async getTrackSampleOffset() {
        let interrupt = arguments.length > 0 && void 0 !== arguments[0] && arguments[0];
        if (!this.streamNode) return null;
        const requestId = crypto.randomUUID();
        this.streamNode.port.postMessage({
            event: interrupt ? 'interrupt' : 'offset',
            requestId
        });
        let trackSampleOffset;
        while(!trackSampleOffset){
            trackSampleOffset = this.trackSampleOffsets[requestId];
            await new Promise((r)=>setTimeout(r, 1));
        }
        const { trackId } = trackSampleOffset;
        if (interrupt && trackId) this.interruptedTrackIds[trackId] = true;
        return trackSampleOffset;
    }
    /**
   * Strips the current stream and returns the sample offset of the audio
   * @returns {{trackId: string|null, offset: number, currentTime: number} | null}
   */ async interrupt() {
        return this.getTrackSampleOffset(true);
    }
    /**
   * Set media stream for local loopback
   */ setMediaStream(stream) {
        this.localLoopbackStream = stream;
    }
    /**
   * Adds G.711 A-law encoded audio data to the currently playing audio stream
   * @param {ArrayBuffer|Uint8Array} arrayBuffer - G.711 A-law encoded data
   * @param {string} [trackId]
   * @returns {Int16Array}
   */ async addG711a(arrayBuffer) {
        let trackId = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : 'default';
        await this.add16BitPCM(arrayBuffer, trackId, 'g711a');
    }
    /**
   * Adds G.711 μ-law encoded audio data to the currently playing audio stream
   * @param {ArrayBuffer|Uint8Array} arrayBuffer - G.711 μ-law encoded data
   * @param {string} [trackId]
   */ async addG711u(arrayBuffer) {
        let trackId = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : 'default';
        await this.add16BitPCM(arrayBuffer, trackId, 'g711u');
    }
    setSampleRate(sampleRate) {
        this.sampleRate = sampleRate;
    }
    setDefaultFormat(format) {
        this.defaultFormat = format;
    }
    /**
   * Sets the volume of audio playback
   * @param {number} volume - Volume level (0.0 to 1.0)
   */ setVolume(volume) {
        // Clamp volume between 0 and 1
        this.volume = Math.max(0, Math.min(1, volume));
        if (this.streamNode) this.streamNode.port.postMessage({
            event: 'volume',
            volume: this.volume
        });
    }
    /**
   * Gets the current volume level of audio playback
   * @returns {number} Current volume level (0.0 to 1.0)
   */ getVolume() {
        return this.volume;
    }
    /**
   * Destroys the player instance and releases all resources
   * Should be called when the player is no longer needed
   */ async destroy() {
        // Stop any audio that's playing
        if (this.streamNode) {
            this.streamNode.disconnect();
            this.streamNode = null;
        }
        // Clean up local loopback
        if (this.localLoopback) {
            this.localLoopback.cleanup();
            this.localLoopback = void 0;
        }
        // Close audio context
        if (this.context) {
            await this.context.close();
            this.context = null;
        }
        // Reset all state
        this.trackSampleOffsets = {};
        this.interruptedTrackIds = {};
        this.isPaused = false;
    }
    /**
   * Creates a new WavStreamPlayer instance
   * @param {{sampleRate?: number, enableLocalLoopback?: boolean, defaultFormat?: AudioFormat, volume?: number}} options
   * @returns {WavStreamPlayer}
   */ constructor({ sampleRate = 44100, enableLocalLoopback = false, defaultFormat = 'pcm', volume = 1.0 } = {}){
        this.volume = 1.0;
        this.scriptSrc = StreamProcessorSrc;
        this.sampleRate = sampleRate;
        this.context = null;
        this.streamNode = null;
        this.trackSampleOffsets = {};
        this.interruptedTrackIds = {};
        this.isPaused = false;
        this.enableLocalLoopback = enableLocalLoopback;
        this.defaultFormat = defaultFormat;
        if (this.enableLocalLoopback) this.localLoopback = new local_loopback(true);
        // Initialize volume (0 = muted, 1 = full volume)
        this.volume = volume;
    }
}
const AudioProcessorWorklet = `
class AudioProcessor extends AudioWorkletProcessor {

  constructor() {
    super();
    this.port.onmessage = this.receive.bind(this);
    this.initialize();
  }

  initialize() {
    this.foundAudio = false;
    this.recording = false;
    this.chunks = [];
  }

  /**
   * Concatenates sampled chunks into channels
   * Format is chunk[Left[], Right[]]
   */
  readChannelData(chunks, channel = -1, maxChannels = 9) {
    let channelLimit;
    if (channel !== -1) {
      if (chunks[0] && chunks[0].length - 1 < channel) {
        throw new Error(
          \`Channel \${channel} out of range: max \${chunks[0].length}\`
        );
      }
      channelLimit = channel + 1;
    } else {
      channel = 0;
      channelLimit = Math.min(chunks[0] ? chunks[0].length : 1, maxChannels);
    }
    const channels = [];
    for (let n = channel; n < channelLimit; n++) {
      const length = chunks.reduce((sum, chunk) => {
        return sum + chunk[n].length;
      }, 0);
      const buffers = chunks.map((chunk) => chunk[n]);
      const result = new Float32Array(length);
      let offset = 0;
      for (let i = 0; i < buffers.length; i++) {
        result.set(buffers[i], offset);
        offset += buffers[i].length;
      }
      channels[n] = result;
    }
    return channels;
  }

  /**
   * Combines parallel audio data into correct format,
   * channels[Left[], Right[]] to float32Array[LRLRLRLR...]
   */
  formatAudioData(channels) {
    if (channels.length === 1) {
      // Simple case is only one channel
      const float32Array = channels[0].slice();
      const meanValues = channels[0].slice();
      return { float32Array, meanValues };
    } else {
      const float32Array = new Float32Array(
        channels[0].length * channels.length
      );
      const meanValues = new Float32Array(channels[0].length);
      for (let i = 0; i < channels[0].length; i++) {
        const offset = i * channels.length;
        let meanValue = 0;
        for (let n = 0; n < channels.length; n++) {
          float32Array[offset + n] = channels[n][i];
          meanValue += channels[n][i];
        }
        meanValues[i] = meanValue / channels.length;
      }
      return { float32Array, meanValues };
    }
  }

  /**
   * Converts 32-bit float data to 16-bit integers
   */
  floatTo16BitPCM(float32Array) {
    const buffer = new ArrayBuffer(float32Array.length * 2);
    const view = new DataView(buffer);
    let offset = 0;
    for (let i = 0; i < float32Array.length; i++, offset += 2) {
      let s = Math.max(-1, Math.min(1, float32Array[i]));
      view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
    }
    return buffer;
  }

  /**
   * Retrieves the most recent amplitude values from the audio stream
   * @param {number} channel
   */
  getValues(channel = -1) {
    const channels = this.readChannelData(this.chunks, channel);
    const { meanValues } = this.formatAudioData(channels);
    return { meanValues, channels };
  }

  /**
   * Exports chunks as an audio/wav file
   */
  export() {
    const channels = this.readChannelData(this.chunks);
    const { float32Array, meanValues } = this.formatAudioData(channels);
    const audioData = this.floatTo16BitPCM(float32Array);
    return {
      meanValues: meanValues,
      audio: {
        bitsPerSample: 16,
        channels: channels,
        data: audioData,
      },
    };
  }

  receive(e) {
    const { event, id } = e.data;
    let receiptData = {};
    switch (event) {
      case 'start':
        this.recording = true;
        break;
      case 'stop':
        this.recording = false;
        break;
      case 'clear':
        this.initialize();
        break;
      case 'export':
        receiptData = this.export();
        break;
      case 'read':
        receiptData = this.getValues();
        break;
      default:
        break;
    }
    // Always send back receipt
    this.port.postMessage({ event: 'receipt', id, data: receiptData });
  }

  sendChunk(chunk) {
    const channels = this.readChannelData([chunk]);
    const { float32Array, meanValues } = this.formatAudioData(channels);
    const rawAudioData = this.floatTo16BitPCM(float32Array);
    const monoAudioData = this.floatTo16BitPCM(meanValues);
    this.port.postMessage({
      event: 'chunk',
      data: {
        mono: monoAudioData,
        raw: rawAudioData,
      },
    });
  }

  process(inputList, outputList, parameters) {
    // Copy input to output (e.g. speakers)
    // Note that this creates choppy sounds with Mac products
    const sourceLimit = Math.min(inputList.length, outputList.length);
    for (let inputNum = 0; inputNum < sourceLimit; inputNum++) {
      const input = inputList[inputNum];
      const output = outputList[inputNum];
      const channelCount = Math.min(input.length, output.length);
      for (let channelNum = 0; channelNum < channelCount; channelNum++) {
        input[channelNum].forEach((sample, i) => {
          output[channelNum][i] = sample;
        });
      }
    }
    const inputs = inputList[0];
    // There's latency at the beginning of a stream before recording starts
    // Make sure we actually receive audio data before we start storing chunks
    let sliceIndex = 0;
    if (!this.foundAudio) {
      for (const channel of inputs) {
        sliceIndex = 0; // reset for each channel
        if (this.foundAudio) {
          break;
        }
        if (channel) {
          for (const value of channel) {
            if (value !== 0) {
              // find only one non-zero entry in any channel
              this.foundAudio = true;
              break;
            } else {
              sliceIndex++;
            }
          }
        }
      }
    }
    if (inputs && inputs[0] && this.foundAudio && this.recording) {
      // We need to copy the TypedArray, because the \`process\`
      // internals will reuse the same buffer to hold each input
      const chunk = inputs.map((input) => input.slice(sliceIndex));
      this.chunks.push(chunk);
      this.sendChunk(chunk);
    }
    return true;
  }
}

registerProcessor('audio_processor', AudioProcessor);
`;
let audio_processor_src = '';
if (utils_isBrowserExtension()) chrome.runtime.getURL('audio_processor.js');
else {
    const script = new Blob([
        AudioProcessorWorklet
    ], {
        type: 'application/javascript'
    });
    URL.createObjectURL(script);
}
/**
 * Constants for help with visualization
 * Helps map frequency ranges from Fast Fourier Transform
 * to human-interpretable ranges, notably music ranges and
 * human vocal ranges.
 */ // Eighth octave frequencies
const octave8Frequencies = [
    4186.01,
    4434.92,
    4698.63,
    4978.03,
    5274.04,
    5587.65,
    5919.91,
    6271.93,
    6644.88,
    7040.0,
    7458.62,
    7902.13
];
// Labels for each of the above frequencies
const octave8FrequencyLabels = [
    'C',
    'C#',
    'D',
    'D#',
    'E',
    'F',
    'F#',
    'G',
    'G#',
    'A',
    'A#',
    'B'
];
/**
 * All note frequencies from 1st to 8th octave
 * in format "A#8" (A#, 8th octave)
 */ const constants_noteFrequencies = [];
const constants_noteFrequencyLabels = [];
for(let i = 1; i <= 8; i++)for(let f = 0; f < octave8Frequencies.length; f++){
    const freq = octave8Frequencies[f];
    constants_noteFrequencies.push(freq / Math.pow(2, 8 - i));
    constants_noteFrequencyLabels.push(octave8FrequencyLabels[f] + i);
}
/**
 * Subset of the note frequencies between 32 and 2000 Hz
 * 6 octave range: C1 to B6
 */ const voiceFrequencyRange = [
    32.0,
    2000.0
];
constants_noteFrequencies.filter((_, i)=>constants_noteFrequencies[i] > voiceFrequencyRange[0] && constants_noteFrequencies[i] < voiceFrequencyRange[1]);
constants_noteFrequencyLabels.filter((_, i)=>constants_noteFrequencies[i] > voiceFrequencyRange[0] && constants_noteFrequencies[i] < voiceFrequencyRange[1]);
class APIResource {
    constructor(client){
        this._client = client;
    }
}
/* eslint-disable @typescript-eslint/no-namespace */ class Bots extends APIResource {
    /**
   * Create a new agent. | 调用接口创建一个新的智能体。
   * @docs en:https://www.coze.com/docs/developer_guides/create_bot?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/create_bot?_lang=zh
   * @param params - Required The parameters for creating a bot. | 创建 Bot 的参数。
   * @param params.space_id - Required The Space ID of the space where the agent is located. | Bot 所在的空间的 Space ID。
   * @param params.name - Required The name of the agent. It should be 1 to 20 characters long. | Bot 的名称。
   * @param params.description - Optional The description of the agent. It can be 0 to 500 characters long. | Bot 的描述信息。
   * @param params.icon_file_id - Optional The file ID for the agent's avatar. | 作为智能体头像的文件 ID。
   * @param params.prompt_info - Optional The personality and reply logic of the agent. | Bot 的提示词配置。
   * @param params.onboarding_info - Optional The settings related to the agent's opening remarks. | Bot 的开场白配置。
   * @returns Information about the created bot. | 创建的 Bot 信息。
   */ async create(params, options) {
        const apiUrl = '/v1/bot/create';
        const result = await this._client.post(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Update the configuration of an agent. | 调用接口修改智能体的配置。
   * @docs en:https://www.coze.com/docs/developer_guides/update_bot?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/update_bot?_lang=zh
   * @param params - Required The parameters for updating a bot. | 修改 Bot 的参数。
   * @param params.bot_id - Required The ID of the agent that the API interacts with. | 待修改配置的智能体ID。
   * @param params.name - Optional The name of the agent. | Bot 的名称。
   * @param params.description - Optional The description of the agent. | Bot 的描述信息。
   * @param params.icon_file_id - Optional The file ID for the agent's avatar. | 作为智能体头像的文件 ID。
   * @param params.prompt_info - Optional The personality and reply logic of the agent. | Bot 的提示词配置。
   * @param params.onboarding_info - Optional The settings related to the agent's opening remarks. | Bot 的开场白配置。
   * @param params.knowledge - Optional Knowledge configurations of the agent. | Bot 的知识库配置。
   * @returns Undefined | 无返回值
   */ async update(params, options) {
        const apiUrl = '/v1/bot/update';
        const result = await this._client.post(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Get the agents published as API service. | 调用接口查看指定空间发布到 Agent as API 渠道的智能体列表。
   * @docs en:https://www.coze.com/docs/developer_guides/published_bots_list?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/published_bots_list?_lang=zh
   * @param params - Required The parameters for listing bots. | 列出 Bot 的参数。
   * @param params.space_id - Required The ID of the space. | Bot 所在的空间的 Space ID。
   * @param params.page_size - Optional Pagination size. | 分页大小。
   * @param params.page_index - Optional Page number for paginated queries. | 分页查询时的页码。
   * @returns List of published bots. | 已发布的 Bot 列表。
   */ async list(params, options) {
        const apiUrl = '/v1/space/published_bots_list';
        const result = await this._client.get(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Publish the specified agent as an API service. | 调用接口创建一个新的智能体。
   * @docs en:https://www.coze.com/docs/developer_guides/publish_bot?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/publish_bot?_lang=zh
   * @param params - Required The parameters for publishing a bot. | 发布 Bot 的参数。
   * @param params.bot_id - Required The ID of the agent that the API interacts with. | 要发布的智能体ID。
   * @param params.connector_ids - Required The list of publishing channel IDs for the agent. | 智能体的发布渠道 ID 列表。
   * @returns Undefined | 无返回值
   */ async publish(params, options) {
        const apiUrl = '/v1/bot/publish';
        const result = await this._client.post(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Get the configuration information of the agent. | 获取指定智能体的配置信息。
   * @docs en:https://www.coze.com/docs/developer_guides/get_metadata?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/get_metadata?_lang=zh
   * @param params - Required The parameters for retrieving a bot. | 获取 Bot 的参数。
   * @param params.bot_id - Required The ID of the agent that the API interacts with. | 要查看的智能体ID。
   * @returns Information about the bot. | Bot 的配置信息。
   */ async retrieve(params, options) {
        const apiUrl = '/v1/bot/get_online_info';
        const result = await this._client.get(apiUrl, params, false, options);
        return result.data;
    }
}
/* eslint-disable security/detect-object-injection */ /* eslint-disable @typescript-eslint/no-explicit-any */ function safeJsonParse(jsonString) {
    let defaultValue = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : '';
    try {
        return JSON.parse(jsonString);
    } catch (error) {
        return defaultValue;
    }
}
function utils_sleep(ms) {
    return new Promise((resolve)=>{
        setTimeout(resolve, ms);
    });
}
function utils_isUniApp() {
    return 'undefined' != typeof uni;
}
function utils_isBrowser() {
    return 'undefined' != typeof window;
}
function isPlainObject(obj) {
    if ('object' != typeof obj || null === obj) return false;
    const proto = Object.getPrototypeOf(obj);
    if (null === proto) return true;
    let baseProto = proto;
    while(null !== Object.getPrototypeOf(baseProto))baseProto = Object.getPrototypeOf(baseProto);
    return proto === baseProto;
}
function mergeConfig() {
    for(var _len = arguments.length, objects = new Array(_len), _key = 0; _key < _len; _key++)objects[_key] = arguments[_key];
    return objects.reduce((result, obj)=>{
        if (void 0 === obj) return result || {};
        for(const key in obj)if (Object.prototype.hasOwnProperty.call(obj, key)) {
            if (isPlainObject(obj[key]) && !Array.isArray(obj[key])) result[key] = mergeConfig(result[key] || {}, obj[key]);
            else result[key] = obj[key];
        }
        return result;
    }, {});
}
function isPersonalAccessToken(token) {
    return !!(null == token ? void 0 : token.startsWith('pat_'));
}
function buildWebsocketUrl(path, params) {
    const queryString = Object.entries(params || {}).filter((param)=>{
        let [_, value] = param;
        return null != value && '' !== value;
    }).map((param)=>{
        let [key, value] = param;
        return `${key}=${value}`;
    }).join('&');
    return `${path}${queryString ? `?${queryString}` : ''}`;
}
const src_utils_isBrowserExtension = ()=>'undefined' != typeof chrome && !!chrome.runtime && !!chrome.runtime.id;
/* eslint-disable max-params */ class CozeError extends Error {
}
class error_APIError extends CozeError {
    static makeMessage(status, errorBody, message, headers) {
        if (!errorBody && message) return message;
        if (errorBody) {
            const list = [];
            const { code, msg, error } = errorBody;
            if (code) list.push(`code: ${code}`);
            if (msg) list.push(`msg: ${msg}`);
            if ((null == error ? void 0 : error.detail) && msg !== error.detail) list.push(`detail: ${error.detail}`);
            const logId = (null == error ? void 0 : error.logid) || (null == headers ? void 0 : headers['x-tt-logid']);
            if (logId) list.push(`logid: ${logId}`);
            return list.join(', ');
        }
        if (status) return `http status code: ${status} (no body)`;
        return '(no status code or body)';
    }
    static generate(status, errorResponse, message, headers) {
        if (!status) return new APIConnectionError({
            cause: castToError(errorResponse)
        });
        const error = errorResponse;
        // https://www.coze.cn/docs/developer_guides/coze_error_codes
        if (400 === status || (null == error ? void 0 : error.code) === 4000) return new BadRequestError(status, error, message, headers);
        if (401 === status || (null == error ? void 0 : error.code) === 4100) return new AuthenticationError(status, error, message, headers);
        if (403 === status || (null == error ? void 0 : error.code) === 4101) return new PermissionDeniedError(status, error, message, headers);
        if (404 === status || (null == error ? void 0 : error.code) === 4200) return new NotFoundError(status, error, message, headers);
        if (429 === status || (null == error ? void 0 : error.code) === 4013) return new RateLimitError(status, error, message, headers);
        if (408 === status) return new TimeoutError(status, error, message, headers);
        if (502 === status) return new GatewayError(status, error, message, headers);
        if (status >= 500) return new InternalServerError(status, error, message, headers);
        return new error_APIError(status, error, message, headers);
    }
    constructor(status, error, message, headers){
        var _error_detail, _error_error;
        super(`${error_APIError.makeMessage(status, error, message, headers)}`);
        this.status = status;
        this.headers = headers;
        this.logid = (null == error ? void 0 : null === (_error_detail = error.detail) || void 0 === _error_detail ? void 0 : _error_detail.logid) || (null == headers ? void 0 : headers['x-tt-logid']);
        // this.error = error;
        this.code = null == error ? void 0 : error.code;
        this.msg = null == error ? void 0 : error.msg;
        this.detail = null == error ? void 0 : null === (_error_error = error.error) || void 0 === _error_error ? void 0 : _error_error.detail;
        this.rawError = error;
    }
}
class APIConnectionError extends error_APIError {
    constructor({ message }){
        super(void 0, void 0, message || 'Connection error.', void 0), this.status = void 0;
    // if (cause) {
    //   this.cause = cause;
    // }
    }
}
class APIUserAbortError extends error_APIError {
    constructor(message){
        super(void 0, void 0, message || 'Request was aborted.', void 0), this.name = 'UserAbortError', this.status = void 0;
    }
}
class BadRequestError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'BadRequestError', this.status = 400;
    }
}
class AuthenticationError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'AuthenticationError', this.status = 401;
    }
}
class PermissionDeniedError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'PermissionDeniedError', this.status = 403;
    }
}
class NotFoundError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'NotFoundError', this.status = 404;
    }
}
class TimeoutError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'TimeoutError', this.status = 408;
    }
}
class RateLimitError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'RateLimitError', this.status = 429;
    }
}
class InternalServerError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'InternalServerError', this.status = 500;
    }
}
class GatewayError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'GatewayError', this.status = 502;
    }
}
const castToError = (err)=>{
    if (err instanceof Error) return err;
    return new Error(err);
};
class Messages extends APIResource {
    /**
   * Get the list of messages in a chat. | 获取对话中的消息列表。
   * @docs en:https://www.coze.com/docs/developer_guides/chat_message_list?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_message_list?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param chat_id - Required The ID of the chat. | 对话 ID。
   * @returns An array of chat messages. | 对话消息数组。
   */ async list(conversation_id, chat_id, options) {
        const apiUrl = `/v3/chat/message/list?conversation_id=${conversation_id}&chat_id=${chat_id}`;
        const result = await this._client.get(apiUrl, void 0, false, options);
        return result.data;
    }
}
const uuid = ()=>(Math.random() * new Date().getTime()).toString();
const handleAdditionalMessages = (additional_messages)=>null == additional_messages ? void 0 : additional_messages.map((i)=>({
            ...i,
            content: 'object' == typeof i.content ? JSON.stringify(i.content) : i.content
        }));
const handleParameters = (parameters)=>{
    if (parameters) {
        for (const [key, value] of Object.entries(parameters))if ('object' == typeof value) parameters[key] = JSON.stringify(value);
    }
    return parameters;
};
class Chat extends APIResource {
    /**
   * Call the Chat API to send messages to a published Coze agent. | 调用此接口发起一次对话，支持添加上下文
   * @docs en:https://www.coze.com/docs/developer_guides/chat_v3?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_v3?_lang=zh
   * @param params - Required The parameters for creating a chat session. | 创建会话的参数。
   * @param params.bot_id - Required The ID of the agent. | 要进行会话聊天的 Bot ID。
   * @param params.user_id - Optional The ID of the user interacting with the Bot. | 标识当前与 Bot 交互的用户。
   * @param params.additional_messages - Optional Additional messages for the conversation. | 对话的附加信息。
   * @param params.custom_variables - Optional Variables defined in the Bot. | Bot 中定义变量。
   * @param params.auto_save_history - Optional Whether to automatically save the conversation history. | 是否自动保存历史对话记录。
   * @param params.meta_data - Optional Additional metadata for the message. | 创建消息时的附加消息。
   * @param params.conversation_id - Optional The ID of the conversation. | 标识对话发生在哪一次会话中。
   * @param params.extra_params - Optional Extra parameters for the conversation. | 附加参数。
   * @param params.shortcut_command - Optional The shortcut command information. | 快捷指令信息。
   * @param params.parameters - Optional custom parameters. | 自定义参数。
   * @returns The data of the created chat. | 创建的对话数据。
   */ async create(params, options) {
        if (!params.user_id) params.user_id = uuid();
        const { conversation_id, ...rest } = params;
        const apiUrl = `/v3/chat${conversation_id ? `?conversation_id=${conversation_id}` : ''}`;
        const payload = {
            ...rest,
            additional_messages: handleAdditionalMessages(params.additional_messages),
            shortcut_command: params.shortcut_command ? {
                ...params.shortcut_command,
                parameters: handleParameters(params.shortcut_command.parameters)
            } : void 0,
            stream: false
        };
        const result = await this._client.post(apiUrl, payload, false, options);
        return result.data;
    }
    /**
   * Call the Chat API to send messages to a published Coze agent. | 调用此接口发起一次对话，支持添加上下文
   * @docs en:https://www.coze.com/docs/developer_guides/chat_v3?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_v3?_lang=zh
   * @param params - Required The parameters for creating a chat session. | 创建会话的参数。
   * @param params.bot_id - Required The ID of the agent. | 要进行会话聊天的 Bot ID。
   * @param params.user_id - Optional The ID of the user interacting with the Bot. | 标识当前与 Bot 交互的用户。
   * @param params.additional_messages - Optional Additional messages for the conversation. | 对话的附加信息。
   * @param params.custom_variables - Optional Variables defined in the Bot. | Bot 中定义的变量。
   * @param params.auto_save_history - Optional Whether to automatically save the conversation history. | 是否自动保存历史对话记录。
   * @param params.meta_data - Optional Additional metadata for the message. | 创建消息时的附加消息。
   * @param params.conversation_id - Optional The ID of the conversation. | 标识对话发生在哪一次会话中。
   * @param params.extra_params - Optional Extra parameters for the conversation. | 附加参数。
   * @param params.shortcut_command - Optional The shortcut command information. | 快捷指令信息。
   * @param params.parameters - Optional custom parameters. | 自定义参数。
   * @returns
   */ async createAndPoll(params, options) {
        if (!params.user_id) params.user_id = uuid();
        const { conversation_id, ...rest } = params;
        const apiUrl = `/v3/chat${conversation_id ? `?conversation_id=${conversation_id}` : ''}`;
        const payload = {
            ...rest,
            additional_messages: handleAdditionalMessages(params.additional_messages),
            shortcut_command: params.shortcut_command ? {
                ...params.shortcut_command,
                parameters: handleParameters(params.shortcut_command.parameters)
            } : void 0,
            stream: false
        };
        const result = await this._client.post(apiUrl, payload, false, options);
        const chatId = result.data.id;
        const conversationId = result.data.conversation_id;
        let chat;
        while(true){
            await utils_sleep(100);
            chat = await this.retrieve(conversationId, chatId);
            if ('completed' === chat.status || 'failed' === chat.status || 'requires_action' === chat.status) break;
        }
        const messageList = await this.messages.list(conversationId, chatId);
        return {
            chat,
            messages: messageList
        };
    }
    /**
   * Call the Chat API to send messages to a published Coze agent with streaming response. | 调用此接口发起一次对话，支持流式响应。
   * @docs en:https://www.coze.com/docs/developer_guides/chat_v3?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_v3?_lang=zh
   * @param params - Required The parameters for streaming a chat session. | 流式会话的参数。
   * @param params.bot_id - Required The ID of the agent. | 要进行会话聊天的 Bot ID。
   * @param params.user_id - Optional The ID of the user interacting with the Bot. | 标识当前与 Bot 交互的用户。
   * @param params.additional_messages - Optional Additional messages for the conversation. | 对话的附加信息。
   * @param params.custom_variables - Optional Variables defined in the Bot. | Bot 中定义的变量。
   * @param params.auto_save_history - Optional Whether to automatically save the conversation history. | 是否自动保存历史对话记录。
   * @param params.meta_data - Optional Additional metadata for the message. | 创建消息时的附加消息。
   * @param params.conversation_id - Optional The ID of the conversation. | 标识对话发生在哪一次会话中。
   * @param params.extra_params - Optional Extra parameters for the conversation. | 附加参数。
   * @param params.shortcut_command - Optional The shortcut command information. | 快捷指令信息。
   * @param params.parameters - Optional custom parameters. | 自定义参数。
   * @returns A stream of chat data. | 对话数据流。
   */ async *stream(params, options) {
        if (!params.user_id) params.user_id = uuid();
        const { conversation_id, ...rest } = params;
        const apiUrl = `/v3/chat${conversation_id ? `?conversation_id=${conversation_id}` : ''}`;
        const payload = {
            ...rest,
            additional_messages: handleAdditionalMessages(params.additional_messages),
            shortcut_command: params.shortcut_command ? {
                ...params.shortcut_command,
                parameters: handleParameters(params.shortcut_command.parameters)
            } : void 0,
            stream: true
        };
        const result = await this._client.post(apiUrl, payload, true, options);
        for await (const message of result)if ("done" === message.event) {
            const ret = {
                event: message.event,
                data: '[DONE]'
            };
            yield ret;
        } else try {
            const ret = {
                event: message.event,
                data: JSON.parse(message.data)
            };
            yield ret;
        } catch (error) {
            throw new CozeError(`Could not parse message into JSON:${message.data}`);
        }
    }
    /**
   * Get the detailed information of the chat. | 查看对话的详细信息。
   * @docs en:https://www.coze.com/docs/developer_guides/retrieve_chat?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/retrieve_chat?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param chat_id - Required The ID of the chat. | 对话 ID。
   * @returns The data of the retrieved chat. | 检索到的对话数据。
   */ async retrieve(conversation_id, chat_id, options) {
        const apiUrl = `/v3/chat/retrieve?conversation_id=${conversation_id}&chat_id=${chat_id}`;
        const result = await this._client.post(apiUrl, void 0, false, options);
        return result.data;
    }
    /**
   * Cancel a chat session. | 取消对话会话。
   * @docs en:https://www.coze.com/docs/developer_guides/cancel_chat?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/cancel_chat?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param chat_id - Required The ID of the chat. | 对话 ID。
   * @returns The data of the canceled chat. | 取消的对话数据。
   */ async cancel(conversation_id, chat_id, options) {
        const apiUrl = '/v3/chat/cancel';
        const payload = {
            conversation_id,
            chat_id
        };
        const result = await this._client.post(apiUrl, payload, false, options);
        return result.data;
    }
    /**
   * Submit tool outputs for a chat session. | 提交对话会话的工具输出。
   * @docs en:https://www.coze.com/docs/developer_guides/chat_submit_tool_outputs?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_submit_tool_outputs?_lang=zh
   * @param params - Required Parameters for submitting tool outputs. | 提交工具输出的参数。
   * @param params.conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param params.chat_id - Required The ID of the chat. | 对话 ID。
   * @param params.tool_outputs - Required The outputs of the tool. | 工具的输出。
   * @param params.stream - Optional Whether to use streaming response. | 是否使用流式响应。
   * @returns The data of the submitted tool outputs or a stream of chat data. | 提交的工具输出数据或对话数据流。
   */ async *submitToolOutputs(params, options) {
        const { conversation_id, chat_id, ...rest } = params;
        const apiUrl = `/v3/chat/submit_tool_outputs?conversation_id=${params.conversation_id}&chat_id=${params.chat_id}`;
        const payload = {
            ...rest
        };
        if (false === params.stream) {
            const response = await this._client.post(apiUrl, payload, false, options);
            return response.data;
        }
        {
            const result = await this._client.post(apiUrl, payload, true, options);
            for await (const message of result)if ("done" === message.event) {
                const ret = {
                    event: message.event,
                    data: '[DONE]'
                };
                yield ret;
            } else try {
                const ret = {
                    event: message.event,
                    data: JSON.parse(message.data)
                };
                yield ret;
            } catch (error) {
                throw new CozeError(`Could not parse message into JSON:${message.data}`);
            }
        }
    }
    constructor(...args){
        super(...args), this.messages = new Messages(this._client);
    }
}
var chat_ChatEventType = /*#__PURE__*/ function(ChatEventType) {
    ChatEventType["CONVERSATION_CHAT_CREATED"] = "conversation.chat.created";
    ChatEventType["CONVERSATION_CHAT_IN_PROGRESS"] = "conversation.chat.in_progress";
    ChatEventType["CONVERSATION_CHAT_COMPLETED"] = "conversation.chat.completed";
    ChatEventType["CONVERSATION_CHAT_FAILED"] = "conversation.chat.failed";
    ChatEventType["CONVERSATION_CHAT_REQUIRES_ACTION"] = "conversation.chat.requires_action";
    ChatEventType["CONVERSATION_MESSAGE_DELTA"] = "conversation.message.delta";
    ChatEventType["CONVERSATION_MESSAGE_COMPLETED"] = "conversation.message.completed";
    ChatEventType["CONVERSATION_AUDIO_DELTA"] = "conversation.audio.delta";
    ChatEventType["DONE"] = "done";
    ChatEventType["ERROR"] = "error";
    return ChatEventType;
}({});
var chat_RoleType = /*#__PURE__*/ function(RoleType) {
    RoleType["User"] = "user";
    RoleType["Assistant"] = "assistant";
    return RoleType;
}({});
class messages_Messages extends APIResource {
    /**
   * Create a message and add it to the specified conversation. | 创建一条消息，并将其添加到指定的会话中。
   * @docs en: https://www.coze.com/docs/developer_guides/create_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param params - Required The parameters for creating a message | 创建消息所需的参数
   * @param params.role - Required The entity that sent this message. Possible values: user, assistant. | 发送这条消息的实体。取值：user, assistant。
   * @param params.content - Required The content of the message. | 消息的内容。
   * @param params.content_type - Required The type of the message content. | 消息内容的类型。
   * @param params.meta_data - Optional Additional information when creating a message. | 创建消息时的附加消息。
   * @returns Information about the new message. | 消息详情。
   */ async create(conversation_id, params, options) {
        const apiUrl = `/v1/conversation/message/create?conversation_id=${conversation_id}`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Modify a message, supporting the modification of message content, additional content, and message type. | 修改一条消息，支持修改消息内容、附加内容和消息类型。
   * @docs en: https://www.coze.com/docs/developer_guides/modify_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/modify_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param message_id - Required The ID of the message. | Message ID，即消息的唯一标识。
   * @param params - Required The parameters for modifying a message | 修改消息所需的参数
   * @param params.meta_data - Optional Additional information when modifying a message. | 修改消息时的附加消息。
   * @param params.content - Optional The content of the message. | 消息的内容。
   * @param params.content_type - Optional The type of the message content. | 消息内容的类型。
   * @returns Information about the modified message. | 消息详情。
   */ // eslint-disable-next-line max-params
    async update(conversation_id, message_id, params, options) {
        const apiUrl = `/v1/conversation/message/modify?conversation_id=${conversation_id}&message_id=${message_id}`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.message;
    }
    /**
   * Get the detailed information of specified message. | 查看指定消息的详细信息。
   * @docs en: https://www.coze.com/docs/developer_guides/retrieve_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/retrieve_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param message_id - Required The ID of the message. | Message ID，即消息的唯一标识。
   * @returns Information about the message. | 消息详情。
   */ async retrieve(conversation_id, message_id, options) {
        const apiUrl = `/v1/conversation/message/retrieve?conversation_id=${conversation_id}&message_id=${message_id}`;
        const response = await this._client.get(apiUrl, null, false, options);
        return response.data;
    }
    /**
   * List messages in a conversation. | 列出会话中的消息。
   * @docs en: https://www.coze.com/docs/developer_guides/message_list?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/message_list?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param params - Optional The parameters for listing messages | 列出消息所需的参数
   * @param params.order - Optional The order of the messages. | 消息的顺序。
   * @param params.chat_id - Optional The ID of the chat. | 聊天 ID。
   * @param params.before_id - Optional The ID of the message before which to list. | 列出此消息之前的消息 ID。
   * @param params.after_id - Optional The ID of the message after which to list. | 列出此消息之后的消息 ID。
   * @param params.limit - Optional The maximum number of messages to return. | 返回的最大消息数。
   * @returns A list of messages. | 消息列表。
   */ async list(conversation_id, params, options) {
        const apiUrl = `/v1/conversation/message/list?conversation_id=${conversation_id}`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response;
    }
    /**
   * Call the API to delete a message within a specified conversation. | 调用接口在指定会话中删除消息。
   * @docs en: https://www.coze.com/docs/developer_guides/delete_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param message_id - Required The ID of the message. | Message ID，即消息的唯一标识。
   * @returns Details of the deleted message. | 已删除的消息详情。
   */ async delete(conversation_id, message_id, options) {
        const apiUrl = `/v1/conversation/message/delete?conversation_id=${conversation_id}&message_id=${message_id}`;
        const response = await this._client.post(apiUrl, void 0, false, options);
        return response.data;
    }
}
class Conversations extends APIResource {
    /**
   * Create a conversation. Conversation is an interaction between an agent and a user, including one or more messages. | 调用接口创建一个会话。
   * @docs en: https://www.coze.com/docs/developer_guides/create_conversation?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_conversation?_lang=zh
   * @param params - Required The parameters for creating a conversation | 创建会话所需的参数
   * @param params.messages - Optional Messages in the conversation. | 会话中的消息内容。
   * @param params.meta_data - Optional Additional information when creating a message. | 创建消息时的附加消息。
   * @param params.bot_id - Optional Bind and isolate conversation on different bots. | 绑定和隔离不同Bot的会话。
   * @returns Information about the created conversation. | 会话的基础信息。
   */ async create(params, options) {
        const apiUrl = '/v1/conversation/create';
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Get the information of specific conversation. | 通过会话 ID 查看会话信息。
   * @docs en: https://www.coze.com/docs/developer_guides/retrieve_conversation?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/retrieve_conversation?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @returns Information about the conversation. | 会话的基础信息。
   */ async retrieve(conversation_id, options) {
        const apiUrl = `/v1/conversation/retrieve?conversation_id=${conversation_id}`;
        const response = await this._client.get(apiUrl, null, false, options);
        return response.data;
    }
    /**
   * List all conversations. | 列出 Bot 下所有会话。
   * @param params
   * @param params.bot_id - Required Bot ID. | Bot ID。
   * @param params.page_num - Optional The page number. | 页码，默认值为 1。
   * @param params.page_size - Optional The number of conversations per page. | 每页的会话数量，默认值为 50。
   * @returns Information about the conversations. | 会话的信息。
   */ async list(params, options) {
        const apiUrl = '/v1/conversations';
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Clear a conversation. | 清空会话。
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @returns Information about the conversation session. | 会话的会话 ID。
   */ async clear(conversation_id, options) {
        const apiUrl = `/v1/conversations/${conversation_id}/clear`;
        const response = await this._client.post(apiUrl, null, false, options);
        return response.data;
    }
    constructor(...args){
        super(...args), this.messages = new messages_Messages(this._client);
    }
}
class Files extends APIResource {
    /**
   * Upload files to Coze platform. | 调用接口上传文件到扣子。
   * @docs en: https://www.coze.com/docs/developer_guides/upload_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/upload_files?_lang=zh
   * @param params - Required The parameters for file upload | 上传文件所需的参数
   * @param params.file - Required The file to be uploaded. | 需要上传的文件。
   * @returns Information about the new file. | 已上传的文件信息。
   */ async upload(params, options) {
        const apiUrl = '/v1/files/upload';
        const response = await this._client.post(apiUrl, (0, __WEBPACK_EXTERNAL_MODULE_axios__.toFormData)(params), false, options);
        return response.data;
    }
    /**
   * Get the information of the specific file uploaded to Coze platform. | 查看已上传的文件详情。
   * @docs en: https://www.coze.com/docs/developer_guides/retrieve_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/retrieve_files?_lang=zh
   * @param file_id - Required The ID of the uploaded file. | 已上传的文件 ID。
   * @returns Information about the uploaded file. | 已上传的文件信息。
   */ async retrieve(file_id, options) {
        const apiUrl = `/v1/files/retrieve?file_id=${file_id}`;
        const response = await this._client.get(apiUrl, null, false, options);
        return response.data;
    }
}
class Runs extends APIResource {
    /**
   * Initiates a workflow run. | 启动工作流运行。
   * @docs en: https://www.coze.com/docs/developer_guides/workflow_run?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_run?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to run. | 必选 要运行的工作流 ID。
   * @param params.bot_id - Optional The ID of the bot associated with the workflow. | 可选 与工作流关联的机器人 ID。
   * @param params.parameters - Optional Parameters for the workflow execution. | 可选 工作流执行的参数。
   * @param params.ext - Optional Additional information for the workflow execution. | 可选 工作流执行的附加信息。
   * @param params.execute_mode - Optional The mode in which to execute the workflow. | 可选 工作流执行的模式。
   * @param params.connector_id - Optional The ID of the connector to use for the workflow. | 可选 用于工作流的连接器 ID。
   * @param params.app_id - Optional The ID of the app.  | 可选 要进行会话聊天的 App ID
   * @returns RunWorkflowData | 工作流运行数据
   */ async create(params, options) {
        const apiUrl = '/v1/workflow/run';
        const response = await this._client.post(apiUrl, params, false, options);
        return response;
    }
    /**
   * Streams the workflow run events. | 流式传输工作流运行事件。
   * @docs en: https://www.coze.com/docs/developer_guides/workflow_stream_run?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_stream_run?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to run. | 必选 要运行的工作流 ID。
   * @param params.bot_id - Optional The ID of the bot associated with the workflow. | 可选 与工作流关联的机器人 ID。
   * @param params.parameters - Optional Parameters for the workflow execution. | 可选 工作流执行的参数。
   * @param params.ext - Optional Additional information for the workflow execution. | 可选 工作流执行的附加信息。
   * @param params.execute_mode - Optional The mode in which to execute the workflow. | 可选 工作流执行的模式。
   * @param params.connector_id - Optional The ID of the connector to use for the workflow. | 可选 用于工作流的连接器 ID。
   * @param params.app_id - Optional The ID of the app.  | 可选 要进行会话聊天的 App ID
   * @returns Stream<WorkflowEvent, { id: string; event: string; data: string }> | 工作流事件流
   */ async *stream(params, options) {
        const apiUrl = '/v1/workflow/stream_run';
        const result = await this._client.post(apiUrl, params, true, options);
        for await (const message of result)try {
            if ("Done" === message.event) yield new WorkflowEvent(Number(message.id), "Done");
            else yield new WorkflowEvent(Number(message.id), message.event, JSON.parse(message.data));
        } catch (error) {
            throw new CozeError(`Could not parse message into JSON:${message.data}`);
        }
    }
    /**
   * Resumes a paused workflow run. | 恢复暂停的工作流运行。
   * @docs en: https://www.coze.com/docs/developer_guides/workflow_resume?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_resume?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to resume. | 必选 要恢复的工作流 ID。
   * @param params.event_id - Required The ID of the event to resume from. | 必选 要从中恢复的事件 ID。
   * @param params.resume_data - Required Data needed to resume the workflow. | 必选 恢复工作流所需的数据。
   * @param params.interrupt_type - Required The type of interruption to resume from. | 必选 要恢复的中断类型。
   * @returns AsyncGenerator<WorkflowEvent, { id: string; event: string; data: string }> | 工作流事件流
   */ async *resume(params, options) {
        const apiUrl = '/v1/workflow/stream_resume';
        const result = await this._client.post(apiUrl, params, true, options);
        for await (const message of result)try {
            if ("Done" === message.event) yield new WorkflowEvent(Number(message.id), "Done");
            else yield new WorkflowEvent(Number(message.id), message.event, JSON.parse(message.data));
        } catch (error) {
            throw new CozeError(`Could not parse message into JSON:${message.data}`);
        }
    }
    /**
   * Get the workflow run history | 工作流异步运行后，查看执行结果
   * @docs zh: https://www.coze.cn/open/docs/developer_guides/workflow_history
   * @param workflowId - Required The ID of the workflow. | 必选 工作流 ID。
   * @param executeId - Required The ID of the workflow execution. | 必选 工作流执行 ID。
   * @returns WorkflowExecuteHistory[] | 工作流执行历史
   */ async history(workflowId, executeId, options) {
        const apiUrl = `/v1/workflows/${workflowId}/run_histories/${executeId}`;
        const response = await this._client.get(apiUrl, void 0, false, options);
        return response.data;
    }
}
class WorkflowEvent {
    constructor(id, event, data){
        this.id = id;
        this.event = event;
        this.data = data;
    }
}
class WorkflowChat extends APIResource {
    /**
   * Execute a chat workflow. | 执行对话流
   * @docs en: https://www.coze.cn/docs/developer_guides/workflow_chat?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_chat?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to chat with. | 必选 要对话的工作流 ID。
   * @param params.additional_messages - Required Array of messages for the chat. | 必选 对话的消息数组。
   * @param params.parameters - Optional  Parameters for the workflow execution. | 必选 工作流执行的参数。
   * @param params.app_id - Optional The ID of the app. | 可选 应用 ID。
   * @param params.bot_id - Optional The ID of the bot. | 可选 Bot ID。
   * @param params.conversation_id - Optional The ID of the conversation. | 可选 会话 ID。
   * @param params.ext - Optional Additional information for the chat. | 可选 对话的附加信息。
   * @returns AsyncGenerator<StreamChatData> | 对话数据流
   */ async *stream(params, options) {
        const apiUrl = '/v1/workflows/chat';
        const payload = {
            ...params,
            additional_messages: handleAdditionalMessages(params.additional_messages)
        };
        const result = await this._client.post(apiUrl, payload, true, options);
        for await (const message of result)if (message.event === chat_ChatEventType.DONE) {
            const ret = {
                event: message.event,
                data: '[DONE]'
            };
            yield ret;
        } else try {
            const ret = {
                event: message.event,
                data: JSON.parse(message.data)
            };
            yield ret;
        } catch (error) {
            throw new CozeError(`Could not parse message into JSON:${message.data}`);
        }
    }
}
class Workflows extends APIResource {
    constructor(...args){
        super(...args), this.runs = new Runs(this._client), this.chat = new WorkflowChat(this._client);
    }
}
class WorkSpaces extends APIResource {
    /**
   * View the list of workspaces that the current Coze user has joined. | 查看当前扣子用户加入的空间列表。
   * @docs en: https://www.coze.com/docs/developer_guides/list_workspace?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_workspace?_lang=zh
   * @param params.page_num - Optional The page number for paginated queries. Default is 1.
   * | 可选 分页查询时的页码。默认为 1，即从第一页数据开始返回。
   * @param params.page_size - Optional The size of pagination. Default is 10. Maximum is 50. | 可选 分页大小。默认为 10，最大为 50。
   * @returns OpenSpaceData | 工作空间列表
   */ async list(params, options) {
        const apiUrl = '/v1/workspaces';
        const response = await this._client.get(apiUrl, params, false, options);
        return safeJsonParse(response, response).data;
    }
}
// Required header for knowledge APIs
const documents_headers = {
    'agw-js-conv': 'str'
};
class Documents extends APIResource {
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.list' instead.
   *
   * View the file list of a specified knowledge base, which includes lists of documents, spreadsheets, or images.
   * | 调用接口查看指定知识库的内容列表，即文件、表格或图像列表。
   * @docs en: https://www.coze.com/docs/developer_guides/list_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge base. | 必选 待查看文件的知识库 ID。
   * @param params.page - Optional The page number for paginated queries. Default is 1. | 可选 分页查询时的页码。默认为 1。
   * @param params.page_size - Optional The size of pagination. Default is 10. | 可选 分页大小。默认为 10。
   * @returns ListDocumentData | 知识库文件列表
   */ list(params, options) {
        const apiUrl = '/open_api/knowledge/document/list';
        const response = this._client.get(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
        return response;
    }
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.create' instead.
   *
   * Upload files to the specific knowledge. | 调用此接口向指定知识库中上传文件。
   * @docs en: https://www.coze.com/docs/developer_guides/create_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge. | 必选 知识库 ID。
   * @param params.document_bases - Required The metadata information of the files awaiting upload. | 必选 待上传文件的元数据信息。
   * @param params.chunk_strategy - Required when uploading files to a new knowledge for the first time. Chunk strategy.
   * | 向新知识库首次上传文件时必选 分段规则。
   * @returns DocumentInfo[] | 已上传文件的基本信息
   */ async create(params, options) {
        const apiUrl = '/open_api/knowledge/document/create';
        const response = await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
        return response.document_infos;
    }
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.delete' instead.
   *
   * Delete text, images, sheets, and other files in the knowledge base, supporting batch deletion.
   * | 删除知识库中的文本、图像、表格等文件，支持批量删除。
   * @docs en: https://www.coze.com/docs/developer_guides/delete_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_knowledge_files?_lang=zh
   * @param params.document_ids - Required The list of knowledge base files to be deleted. | 必选 待删除的文件 ID。
   * @returns void | 无返回
   */ async delete(params, options) {
        const apiUrl = '/open_api/knowledge/document/delete';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
    }
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.update' instead.
   *
   * Modify the knowledge base file name and update strategy. | 调用接口修改知识库文件名称和更新策略。
   * @docs en: https://www.coze.com/docs/developer_guides/modify_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/modify_knowledge_files?_lang=zh
   * @param params.document_id - Required The ID of the knowledge base file. | 必选 待修改的知识库文件 ID。
   * @param params.document_name - Optional The new name of the knowledge base file. | 可选 知识库文件的新名称。
   * @param params.update_rule - Optional The update strategy for online web pages. | 可选 在线网页更新策略。
   * @returns void | 无返回
   */ async update(params, options) {
        const apiUrl = '/open_api/knowledge/document/update';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
    }
}
class Knowledge extends APIResource {
    constructor(...args){
        super(...args), /**
   * @deprecated
   */ this.documents = new Documents(this._client);
    }
}
// Required header for knowledge APIs
const documents_documents_headers = {
    'agw-js-conv': 'str'
};
class documents_Documents extends APIResource {
    /**
   * View the file list of a specified knowledge base, which includes lists of documents, spreadsheets, or images.
   * | 调用接口查看指定知识库的内容列表，即文件、表格或图像列表。
   * @docs en: https://www.coze.com/docs/developer_guides/list_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge base. | 必选 待查看文件的知识库 ID。
   * @param params.page - Optional The page number for paginated queries. Default is 1. | 可选 分页查询时的页码。默认为 1。
   * @param params.page_size - Optional The size of pagination. Default is 10. | 可选 分页大小。默认为 10。
   * @returns ListDocumentData | 知识库文件列表
   */ async list(params, options) {
        const apiUrl = '/open_api/knowledge/document/list';
        const response = await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
        return response;
    }
    /**
   * Upload files to the specific knowledge. | 调用此接口向指定知识库中上传文件。
   * @docs en: https://www.coze.com/docs/developer_guides/create_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge. | 必选 知识库 ID。
   * @param params.document_bases - Required The metadata information of the files awaiting upload. | 必选 待上传文件的元数据信息。
   * @param params.chunk_strategy - Required when uploading files to a new knowledge for the first time. Chunk strategy.
   * | 向新知识库首次上传文件时必选 分段规则。
   * @returns DocumentInfo[] | 已上传文件的基本信息
   */ async create(params, options) {
        const apiUrl = '/open_api/knowledge/document/create';
        const response = await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
        return response.document_infos;
    }
    /**
   * Delete text, images, sheets, and other files in the knowledge base, supporting batch deletion.
   * | 删除知识库中的文本、图像、表格等文件，支持批量删除。
   * @docs en: https://www.coze.com/docs/developer_guides/delete_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_knowledge_files?_lang=zh
   * @param params.document_ids - Required The list of knowledge base files to be deleted. | 必选 待删除的文件 ID。
   * @returns void | 无返回
   */ async delete(params, options) {
        const apiUrl = '/open_api/knowledge/document/delete';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
    }
    /**
   * Modify the knowledge base file name and update strategy. | 调用接口修改知识库文件名称和更新策略。
   * @docs en: https://www.coze.com/docs/developer_guides/modify_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/modify_knowledge_files?_lang=zh
   * @param params.document_id - Required The ID of the knowledge base file. | 必选 待修改的知识库文件 ID。
   * @param params.document_name - Optional The new name of the knowledge base file. | 可选 知识库文件的新名称。
   * @param params.update_rule - Optional The update strategy for online web pages. | 可选 在线网页更新策略。
   * @returns void | 无返回
   */ async update(params, options) {
        const apiUrl = '/open_api/knowledge/document/update';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
    }
}
class Images extends APIResource {
    /**
   * Update the description of an image in the knowledge base | 更新知识库中的图片描述
   * @docs en: https://www.coze.com/docs/developer_guides/developer_guides/update_image_caption?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/developer_guides/update_image_caption?_lang=zh
   * @param datasetId - The ID of the dataset | 必选 知识库 ID
   * @param documentId - The ID of the document | 必选 知识库文件 ID
   * @param params - The parameters for updating the image
   * @param params.caption - Required. The description of the image | 必选 图片的描述信息
   * @returns undefined
   */ // eslint-disable-next-line max-params
    async update(datasetId, documentId, params, options) {
        const apiUrl = `/v1/datasets/${datasetId}/images/${documentId}`;
        await this._client.put(apiUrl, params, false, options);
    }
    /**
   * List images in the knowledge base | 列出知识库中的图片
   * @docs en: https://www.coze.com/docs/developer_guides/developer_guides/get_images?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/developer_guides/get_images?_lang=zh
   * @param datasetId - The ID of the dataset | 必选 知识库 ID
   * @param params - The parameters for listing images
   * @param params.page_num - Optional. Page number for pagination, minimum value is 1, defaults to 1 | 可选 分页查询时的页码。默认为 1。
   * @param params.page_size - Optional. Number of items per page, range 1-299, defaults to 10 | 可选 分页大小。默认为 10。
   * @param params.keyword - Optional. Search keyword for image descriptions | 可选 图片描述的搜索关键词。
   * @param params.has_caption - Optional. Filter for images with/without captions | 可选 是否过滤有/无描述的图片。
   */ async list(datasetId, params, options) {
        const apiUrl = `/v1/datasets/${datasetId}/images`;
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
}
class Datasets extends APIResource {
    /**
   * Creates a new dataset | 创建数据集
   * @docs en: https://www.coze.com/docs/developer_guides/create_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_dataset?_lang=zh
   * @param params - The parameters for creating a dataset
   * @param {string} params.name - Required. Dataset name, maximum length of 100 characters | 必选 数据集名称，最大长度为 100 个字符
   * @param {string} params.space_id - Required. Space ID where the dataset belongs | 必选 数据集所属的空间 ID
   * @param {number} params.format_type - Required. Dataset type (0: Text type, 2: Image type) | 必选 数据集类型 (0: 文本类型, 2: 图片类型)
   * @param {string} [params.description] - Optional. Dataset description | 可选 数据集描述
   * @param {string} [params.file_id] - Optional. Dataset icon file ID from file upload
   */ async create(params, options) {
        const apiUrl = '/v1/datasets';
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Lists all datasets in a space | 列出空间中的所有数据集
   * @docs en: https://www.coze.com/docs/developer_guides/list_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_dataset?_lang=zh
   * @param params - The parameters for listing datasets | 列出数据集的参数
   * @param {string} params.space_id - Required. Space ID where the datasets belong | 必选 数据集所属的空间 ID
   * @param {string} [params.name] - Optional. Dataset name for fuzzy search | 可选 数据集名称用于模糊搜索
   * @param {number} [params.format_type] - Optional. Dataset type (0: Text type, 2: Image type) | 可选 数据集类型 (0: 文本类型, 2: 图片类型)
   * @param {number} [params.page_num] - Optional. Page number for pagination (default: 1) | 可选 分页查询时的页码。默认为 1。
   * @param {number} [params.page_size] - Optional. Number of items per page (default: 10) | 可选 分页大小。默认为 10。
   */ async list(params, options) {
        const apiUrl = '/v1/datasets';
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Updates a dataset | 更新数据集
   * @docs en: https://www.coze.com/docs/developer_guides/update_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/update_dataset?_lang=zh
   * @param dataset_id - Required. The ID of the dataset to update | 必选 数据集 ID
   * @param params - Required. The parameters for updating the dataset | 必选 更新数据集的参数
   * @param params.name - Required. Dataset name, maximum length of 100 characters. | 必选 数据集名称，最大长度为 100 个字符。
   * @param params.file_id - Optional. Dataset icon, should pass the file_id obtained from the file upload interface. | 可选 数据集图标，应传递从文件上传接口获取的 file_id。
   * @param params.description - Optional. Dataset description. | 可选 数据集描述。
   */ async update(dataset_id, params, options) {
        const apiUrl = `/v1/datasets/${dataset_id}`;
        await this._client.put(apiUrl, params, false, options);
    }
    /**
   * Deletes a dataset | 删除数据集
   * @docs en: https://www.coze.com/docs/developer_guides/delete_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_dataset?_lang=zh
   * @param dataset_id - Required. The ID of the dataset to delete | 必选 数据集 ID
   */ async delete(dataset_id, options) {
        const apiUrl = `/v1/datasets/${dataset_id}`;
        await this._client.delete(apiUrl, false, options);
    }
    /**
   * Views the progress of dataset upload | 查看数据集上传进度
   * @docs en: https://www.coze.com/docs/developer_guides/get_dataset_progress?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/get_dataset_progress?_lang=zh
   * @param dataset_id - Required. The ID of the dataset to process | 必选 数据集 ID
   * @param params - Required. The parameters for processing the dataset | 必选 处理数据集的参数
   * @param params.dataset_ids - Required. List of dataset IDs | 必选 数据集 ID 列表
   */ async process(dataset_id, params, options) {
        const apiUrl = `/v1/datasets/${dataset_id}/process`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    constructor(...args){
        super(...args), this.documents = new documents_Documents(this._client), this.images = new Images(this._client);
    }
}
class Voices extends APIResource {
    /**
   * @description Clone a voice | 音色克隆
   * @param params
   * @param params.voice_name - Required. Voice name, cannot be empty and must be longer than 6 characters
   * | 复刻的音色名称，不能为空，长度大于 6
   * @param params.file - Required. Audio file | 音频文件
   * @param params.audio_format - Required. Only supports "wav", "mp3", "ogg", "m4a", "aac", "pcm" formats
   * | 只支持 "wav", "mp3", "ogg", "m4a", "aac", "pcm" 格式
   * @param params.language - Optional. Only supports "zh", "en" "ja" "es" "id" "pt" languages
   * | 只支持 "zh", "en" "ja" "es" "id" "pt" 语种
   * @param params.voice_id - Optional. If provided, will train on existing voice and override previous training
   * | 传入的话就会在原有的音色上去训练，覆盖前面训练好的音色
   * @param params.preview_text - Optional. If provided, will generate preview audio based on this text, otherwise uses default text
   * | 如果传入会基于该文本生成预览音频，否则使用默认的文本
   * @param params.text - Optional. Users can read this text, service will compare audio with text. Returns error if difference is too large
   * | 可以让用户按照该文本念诵，服务会对比音频与该文本的差异。若差异过大会返回错误
   * @param params.space_id - Optional.  The space id of the voice. | 空间ID
   * @param params.description- Optional. The description of the voice. | 音色描述
   * @param options - Request options
   * @returns Clone voice data
   */ async clone(params, options) {
        const apiUrl = '/v1/audio/voices/clone';
        const response = await this._client.post(apiUrl, (0, __WEBPACK_EXTERNAL_MODULE_axios__.toFormData)(params), false, options);
        return response.data;
    }
    /**
   * @description List voices | 获取音色列表
   * @param params
   * @param params.filter_system_voice - Optional. Whether to filter system voices, default is false
   * | 是否过滤系统音色, 默认不过滤
   * @param params.page_num - Optional. Starts from 1 by default, value must be > 0
   * | 不传默认从 1 开始，传值需要 > 0
   * @param params.page_size - Optional. Default is 100, value must be (0, 100]
   * | 不传默认 100，传值需要 (0, 100]
   * @param params.model_type - Optional. The type of the voice model, if not filled, all will be returned.
   * @param options - Request options
   * @returns List voices data
   */ async list(params, options) {
        const apiUrl = '/v1/audio/voices';
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
}
class Transcriptions extends APIResource {
    /**
   * ASR voice to text | ASR 语音转文本
   * @param params - Required The parameters for file upload | 上传文件所需的参数
   * @param params.file - Required The audio file to be uploaded. | 需要上传的音频文件。
   */ async create(params, options) {
        const apiUrl = '/v1/audio/transcriptions';
        const response = await this._client.post(apiUrl, (0, __WEBPACK_EXTERNAL_MODULE_axios__.toFormData)(params), false, options);
        return response.data;
    }
}
class Speech extends APIResource {
    /**
   * @description Speech synthesis | 语音合成
   * @param params
   * @param params.input - Required. Text to generate audio | 要为其生成音频的文本
   * @param params.voice_id - Required. Voice ID | 生成音频的音色 ID
   * @param params.response_format - Optional. Audio encoding format,
   * supports "wav", "pcm", "ogg", "opus", "mp3", default is "mp3"
   * | 音频编码格式，支持 "wav", "pcm", "ogg", "opus", "mp3"，默认是 "mp3"
   * @param options - Request options
   * @returns Speech synthesis data
   */ async create(params, options) {
        const apiUrl = '/v1/audio/speech';
        const response = await this._client.post(apiUrl, {
            ...params,
            sample_rate: params.sample_rate || 24000
        }, false, mergeConfig(options, {
            responseType: 'arraybuffer'
        }));
        return response;
    }
}
class Rooms extends APIResource {
    async create(params, options) {
        const apiUrl = '/v1/audio/rooms';
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
}
class VoiceprintFeature extends APIResource {
    /**
   * Create voiceprint feature
   */ async create(groupId, params, options) {
        const apiUrl = `/v1/audio/voiceprint_groups/${groupId}/features`;
        const response = await this._client.post(apiUrl, (0, __WEBPACK_EXTERNAL_MODULE_axios__.toFormData)(params), false, options);
        return response.data;
    }
    /**
   * Update voiceprint feature
   */ // eslint-disable-next-line max-params
    async update(groupId, featureId, params, options) {
        const apiUrl = `/v1/audio/voiceprint_groups/${groupId}/features/${featureId}`;
        const response = await this._client.put(apiUrl, (0, __WEBPACK_EXTERNAL_MODULE_axios__.toFormData)(params), false, options);
        return response.data;
    }
    /**
   * Delete voiceprint feature
   */ async delete(groupId, featureId, options) {
        const apiUrl = `/v1/audio/voiceprint_groups/${groupId}/features/${featureId}`;
        const response = await this._client.delete(apiUrl, false, options);
        return response.data;
    }
    /**
   * Get voiceprint feature list
   */ async list(groupId, params, options) {
        const apiUrl = `/v1/audio/voiceprint_groups/${groupId}/features`;
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Speaker identification
   */ async speakerIdentify(groupId, params, options) {
        const apiUrl = `/v1/audio/voiceprint_groups/${groupId}/speaker_identify`;
        const response = await this._client.post(apiUrl, (0, __WEBPACK_EXTERNAL_MODULE_axios__.toFormData)(params), false, options);
        return response.data;
    }
}
class VoiceprintGroups extends APIResource {
    /**
   * Create voiceprint group
   */ async create(params, options) {
        const apiUrl = '/v1/audio/voiceprint_groups';
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Get voiceprint group list
   */ async list(params, options) {
        const apiUrl = '/v1/audio/voiceprint_groups';
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Update voiceprint group
   */ async update(groupId, params, options) {
        const apiUrl = `/v1/audio/voiceprint_groups/${groupId}`;
        const response = await this._client.put(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Delete voiceprint group
   */ async delete(groupId, options) {
        const apiUrl = `/v1/audio/voiceprint_groups/${groupId}`;
        const response = await this._client.delete(apiUrl, false, options);
        return response.data;
    }
    constructor(...args){
        super(...args), this.features = new VoiceprintFeature(this._client);
    }
}
class Audio extends APIResource {
    constructor(...args){
        super(...args), this.rooms = new Rooms(this._client), this.voices = new Voices(this._client), this.speech = new Speech(this._client), this.transcriptions = new Transcriptions(this._client), this.voiceprintGroups = new VoiceprintGroups(this._client);
    }
}
class Templates extends APIResource {
    /**
   * Duplicate a template. | 复制一个模板。
   * @param templateId - Required. The ID of the template to duplicate. | 要复制的模板的 ID。
   * @param params - Optional. The parameters for the duplicate operation. | 可选参数，用于复制操作。
   * @param params.workspace_id - Required. The ID of the workspace to duplicate the template into. | 要复制到的目标工作空间的 ID。
   * @param params.name - Optional. The name of the new template. | 新模板的名称。
   * @returns TemplateDuplicateRes | 复制模板结果
   */ async duplicate(templateId, params, options) {
        const apiUrl = `/v1/templates/${templateId}/duplicate`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
}
class chat_Chat extends APIResource {
    async create(req, options) {
        const apiUrl = buildWebsocketUrl('/v1/chat', req);
        return await this._client.makeWebsocket(apiUrl, options);
    }
}
class transcriptions_Transcriptions extends APIResource {
    async create(req, options) {
        const apiUrl = buildWebsocketUrl('/v1/audio/transcriptions', req);
        return await this._client.makeWebsocket(apiUrl, options);
    }
}
class speech_Speech extends APIResource {
    async create(req, options) {
        const apiUrl = buildWebsocketUrl('/v1/audio/speech', req);
        return await this._client.makeWebsocket(apiUrl, options);
    }
}
class SimultInterpretation extends APIResource {
    async create(options) {
        const apiUrl = '/v1/audio/simult_interpretation';
        return await this._client.makeWebsocket(apiUrl, options);
    }
}
class audio_Audio extends APIResource {
    constructor(...args){
        super(...args), this.speech = new speech_Speech(this._client), this.transcriptions = new transcriptions_Transcriptions(this._client), this.simultInterpretation = new SimultInterpretation(this._client);
    }
}
// Common types (not exported)
// Keep all existing exports but use the base types where applicable
var types_WebsocketsEventType = /*#__PURE__*/ function(WebsocketsEventType) {
    // Common
    /** SDK error */ WebsocketsEventType["CLIENT_ERROR"] = "client_error";
    /** Connection closed */ WebsocketsEventType["CLOSED"] = "closed";
    /** All events */ WebsocketsEventType["ALL"] = "all";
    // Error
    /** Received error event */ WebsocketsEventType["ERROR"] = "error";
    // v1/audio/speech
    /** Send text to server */ WebsocketsEventType["INPUT_TEXT_BUFFER_APPEND"] = "input_text_buffer.append";
    /** No text to send, after audio all received, can close connection */ WebsocketsEventType["INPUT_TEXT_BUFFER_COMPLETE"] = "input_text_buffer.complete";
    /** Send speech config to server */ WebsocketsEventType["SPEECH_UPDATE"] = "speech.update";
    /** Received `speech.updated` event */ WebsocketsEventType["SPEECH_UPDATED"] = "speech.updated";
    /** After speech created */ WebsocketsEventType["SPEECH_CREATED"] = "speech.created";
    /** Received `input_text_buffer.complete` event */ WebsocketsEventType["INPUT_TEXT_BUFFER_COMPLETED"] = "input_text_buffer.completed";
    /** Received `speech.update` event */ WebsocketsEventType["SPEECH_AUDIO_UPDATE"] = "speech.audio.update";
    /** All audio received, can close connection */ WebsocketsEventType["SPEECH_AUDIO_COMPLETED"] = "speech.audio.completed";
    // v1/audio/transcriptions
    /** Send audio to server */ WebsocketsEventType["INPUT_AUDIO_BUFFER_APPEND"] = "input_audio_buffer.append";
    /** No audio to send, after text all received, can close connection */ WebsocketsEventType["INPUT_AUDIO_BUFFER_COMPLETE"] = "input_audio_buffer.complete";
    /** Send transcriptions config to server */ WebsocketsEventType["TRANSCRIPTIONS_UPDATE"] = "transcriptions.update";
    /** Send `input_audio_buffer.clear` event */ WebsocketsEventType["INPUT_AUDIO_BUFFER_CLEAR"] = "input_audio_buffer.clear";
    /** After transcriptions created */ WebsocketsEventType["TRANSCRIPTIONS_CREATED"] = "transcriptions.created";
    /** Received `input_audio_buffer.complete` event */ WebsocketsEventType["INPUT_AUDIO_BUFFER_COMPLETED"] = "input_audio_buffer.completed";
    /** Received `transcriptions.update` event */ WebsocketsEventType["TRANSCRIPTIONS_MESSAGE_UPDATE"] = "transcriptions.message.update";
    /** All audio received, can close connection */ WebsocketsEventType["TRANSCRIPTIONS_MESSAGE_COMPLETED"] = "transcriptions.message.completed";
    /** Received `input_audio_buffer.cleared` event */ WebsocketsEventType["INPUT_AUDIO_BUFFER_CLEARED"] = "input_audio_buffer.cleared";
    /** Received `transcriptions.updated` event */ WebsocketsEventType["TRANSCRIPTIONS_UPDATED"] = "transcriptions.updated";
    // v1/chat
    /** Send chat config to server */ WebsocketsEventType["CHAT_UPDATE"] = "chat.update";
    /** Send tool outputs to server */ WebsocketsEventType["CONVERSATION_CHAT_SUBMIT_TOOL_OUTPUTS"] = "conversation.chat.submit_tool_outputs";
    /** After chat created */ WebsocketsEventType["CHAT_CREATED"] = "chat.created";
    /** After chat updated */ WebsocketsEventType["CHAT_UPDATED"] = "chat.updated";
    /** Audio AST completed, chat started */ WebsocketsEventType["CONVERSATION_CHAT_CREATED"] = "conversation.chat.created";
    /** Message created */ WebsocketsEventType["CONVERSATION_MESSAGE_CREATE"] = "conversation.message.create";
    /** Clear conversation */ WebsocketsEventType["CONVERSATION_CLEAR"] = "conversation.clear";
    /** Chat in progress */ WebsocketsEventType["CONVERSATION_CHAT_IN_PROGRESS"] = "conversation.chat.in_progress";
    /** Get agent text message update */ WebsocketsEventType["CONVERSATION_MESSAGE_DELTA"] = "conversation.message.delta";
    /** Need plugin submit */ WebsocketsEventType["CONVERSATION_CHAT_REQUIRES_ACTION"] = "conversation.chat.requires_action";
    /** Message completed */ WebsocketsEventType["CONVERSATION_MESSAGE_COMPLETED"] = "conversation.message.completed";
    /** Get agent audio message update */ WebsocketsEventType["CONVERSATION_AUDIO_DELTA"] = "conversation.audio.delta";
    /** Audio message completed */ WebsocketsEventType["CONVERSATION_AUDIO_COMPLETED"] = "conversation.audio.completed";
    /** All message received, can close connection */ WebsocketsEventType["CONVERSATION_CHAT_COMPLETED"] = "conversation.chat.completed";
    /** Chat failed */ WebsocketsEventType["CONVERSATION_CHAT_FAILED"] = "conversation.chat.failed";
    /** Received `conversation.cleared` event */ WebsocketsEventType["CONVERSATION_CLEARED"] = "conversation.cleared";
    /** Speech started */ WebsocketsEventType["INPUT_AUDIO_BUFFER_SPEECH_STARTED"] = "input_audio_buffer.speech_started";
    /** Speech stopped */ WebsocketsEventType["INPUT_AUDIO_BUFFER_SPEECH_STOPPED"] = "input_audio_buffer.speech_stopped";
    /** Chat interrupted by client */ WebsocketsEventType["CONVERSATION_CHAT_CANCEL"] = "conversation.chat.cancel";
    /** Chat canceled */ WebsocketsEventType["CONVERSATION_CHAT_CANCELED"] = "conversation.chat.canceled";
    /** Audio transcript update */ WebsocketsEventType["CONVERSATION_AUDIO_TRANSCRIPT_UPDATE"] = "conversation.audio_transcript.update";
    /** Audio transcript completed */ WebsocketsEventType["CONVERSATION_AUDIO_TRANSCRIPT_COMPLETED"] = "conversation.audio_transcript.completed";
    /** Audio sentence start */ WebsocketsEventType["CONVERSATION_AUDIO_SENTENCE_START"] = "conversation.audio.sentence_start";
    /** Audio dump */ WebsocketsEventType["DUMP_AUDIO"] = "dump.audio";
    // v1/audio/simult_interpretation
    WebsocketsEventType["SIMULT_INTERPRETATION_UPDATE"] = "simult_interpretation.update";
    WebsocketsEventType["SIMULT_INTERPRETATION_CREATED"] = "simult_interpretation.created";
    WebsocketsEventType["SIMULT_INTERPRETATION_UPDATED"] = "simult_interpretation.updated";
    WebsocketsEventType["SIMULT_INTERPRETATION_AUDIO_DELTA"] = "simult_interpretation.audio.delta";
    WebsocketsEventType["SIMULT_INTERPRETATION_TRANSCRIPTION_DELTA"] = "simult_interpretation.transcription.delta";
    WebsocketsEventType["SIMULT_INTERPRETATION_TRANSLATION_DELTA"] = "simult_interpretation.translation.delta";
    WebsocketsEventType["SIMULT_INTERPRETATION_MESSAGE_COMPLETED"] = "simult_interpretation.message.completed";
    return WebsocketsEventType;
}({});
class Websockets extends APIResource {
    constructor(...args){
        super(...args), this.audio = new audio_Audio(this._client), this.chat = new chat_Chat(this._client);
    }
}
class Variables extends APIResource {
    /**
   * Set values for user variables
   * @docs en: https://www.coze.com/open/docs/developer_guides/update_variable
   * @docs zh: https://www.coze.cn/open/docs/developer_guides/update_variable
   * @param params - The parameters for the variable update
   * @param options - Optional request options
   */ async update(params, options) {
        const apiUrl = '/v1/variables';
        await this._client.put(apiUrl, params, false, options);
    }
    /**
   * Get the values of user variables
   * @docs en: https://www.coze.com/open/docs/developer_guides/read_variable
   * @docs zh: https://www.coze.cn/open/docs/developer_guides/read_variable
   * @param params - The parameters for the variable retrieval
   * @param options - Optional request options
   */ async retrieve(params, options) {
        const apiUrl = '/v1/variables';
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
}
/**
 * Users client for interacting with user-related APIs
 */ class Users extends APIResource {
    /**
   * Get information about the authenticated user.
   * @param options - Optional request configuration options.
   * @returns Information about the authenticated user.
   */ async me(options) {
        const apiUrl = '/v1/users/me';
        const result = await this._client.get(apiUrl, void 0, false, options);
        return result.data;
    }
}
class WebSocketAPI {
    // Standard WebSocket properties
    get readyState() {
        return this.rws.readyState;
    }
    // Standard WebSocket methods
    send(data) {
        return this.rws.send(JSON.stringify(data));
    }
    close(code, reason) {
        return this.rws.close(code, reason);
    }
    reconnect(code, reason) {
        return this.rws.reconnect(code, reason);
    }
    // Event listener methods
    addEventListener(type, listener) {
        this.rws.addEventListener(type, listener);
    }
    removeEventListener(type, listener) {
        this.rws.removeEventListener(type, listener);
    }
    constructor(url, options = {}){
        // Event handler methods
        this.onmessage = null;
        this.onopen = null;
        this.onclose = null;
        this.onerror = null;
        const separator = url.includes('?') ? '&' : '?';
        const { authorization } = options.headers || {};
        // Get the appropriate WebSocket implementation
        let WebSocketImpl;
        if (utils_isUniApp()) {
            // Use MiniApp WebSocket implementation
            const factory = 'function' == typeof getMiniAppWebSocketFactory ? getMiniAppWebSocketFactory() : null;
            WebSocketImpl = factory ? factory.getWebSocketImplementation() : window.WebSocket;
        } else // Use browser's native WebSocket
        WebSocketImpl = utils_isBrowser() ? window.WebSocket : class extends __WEBPACK_EXTERNAL_MODULE_ws__["default"] {
            constructor(url2, protocols){
                super(url2, protocols, {
                    headers: options.headers
                });
            }
        };
        this.rws = new __WEBPACK_EXTERNAL_MODULE_reconnecting_websocket__["default"](`${url}${separator}authorization=${authorization}`, [], {
            WebSocket: WebSocketImpl,
            ...options
        });
        this.rws.addEventListener('message', (event)=>{
            try {
                var _this_onmessage, _this;
                const data = JSON.parse(event.data);
                null === (_this_onmessage = (_this = this).onmessage) || void 0 === _this_onmessage || _this_onmessage.call(_this, data, event);
            } catch (error) {
                console.error('WebSocketAPI onmessage error', error);
            }
        });
        this.rws.addEventListener('open', (event)=>{
            var _this_onopen, _this;
            null === (_this_onopen = (_this = this).onopen) || void 0 === _this_onopen || _this_onopen.call(_this, event);
        });
        this.rws.addEventListener('close', (event)=>{
            var _this_onclose, _this;
            null === (_this_onclose = (_this = this).onclose) || void 0 === _this_onclose || _this_onclose.call(_this, event);
        });
        this.rws.addEventListener('error', (event)=>{
            var _event_target__req_res, _event_target__req, _event_target, _event_target__req_res1, _event_target__req1, _event_target1, _this_onerror, _this;
            const { readyState } = this.rws;
            if (3 === readyState) return;
            const statusCode = null === (_event_target = event.target) || void 0 === _event_target ? void 0 : null === (_event_target__req = _event_target._req) || void 0 === _event_target__req ? void 0 : null === (_event_target__req_res = _event_target__req.res) || void 0 === _event_target__req_res ? void 0 : _event_target__req_res.statusCode;
            const rawHeaders = (null === (_event_target1 = event.target) || void 0 === _event_target1 ? void 0 : null === (_event_target__req1 = _event_target1._req) || void 0 === _event_target__req1 ? void 0 : null === (_event_target__req_res1 = _event_target__req1.res) || void 0 === _event_target__req_res1 ? void 0 : _event_target__req_res1.rawHeaders) || [];
            const logidIndex = rawHeaders.findIndex((header)=>'X-Tt-Logid' === header);
            const logid = -1 !== logidIndex ? rawHeaders[logidIndex + 1] : void 0;
            const error = {
                id: '0',
                event_type: types_WebsocketsEventType.ERROR,
                data: {
                    code: -1,
                    msg: 'WebSocket error'
                },
                detail: {
                    logid
                }
            };
            if (401 === statusCode) {
                error.data.code = 401;
                error.data.msg = 'Unauthorized';
            } else if (403 === statusCode) {
                error.data.code = 403;
                error.data.msg = 'Forbidden';
            } else {
                error.data.code = 500;
                var _event_error;
                error.data.msg = String(null !== (_event_error = null == event ? void 0 : event.error) && void 0 !== _event_error ? _event_error : '') || 'WebSocket error';
            }
            null === (_this_onerror = (_this = this).onerror) || void 0 === _this_onerror || _this_onerror.call(_this, error, event);
        });
    }
}
var package_namespaceObject = JSON.parse('{"name":"@coze/api","version":"1.3.2","description":"Official Coze Node.js SDK for seamless AI integration into your applications | 扣子官方 Node.js SDK，助您轻松集成 AI 能力到应用中","keywords":["coze","ai","nodejs","sdk","chatbot","typescript"],"homepage":"https://github.com/coze-dev/coze-js/tree/main/packages/coze-js","bugs":{"url":"https://github.com/coze-dev/coze-js/issues"},"repository":{"type":"git","url":"https://github.com/coze-dev/coze-js.git","directory":"packages/coze-js"},"license":"MIT","author":"Leeight <leeight@gmail.com>","exports":{".":"./src/index.ts","./ws-tools":"./src/ws-tools/index.ts"},"main":"src/index.ts","unpkg":"dist/umd/index.js","module":"src/index.ts","browser":{"crypto":false,"os":false,"jsonwebtoken":false,"node-fetch":false,"ws":false},"typesVersions":{"*":{".":["dist/types/index.d.ts"],"ws-tools":["dist/types/ws-tools/ws-tools/index.d.ts"]}},"files":["dist","LICENSE","README.md","README.zh-CN.md"],"scripts":{"build":"rslib build","format":"prettier --write .","lint":"eslint ./ --cache --quiet","start":"rslib build -w","test":"vitest","test:cov":"vitest --coverage --run"},"dependencies":{"agora-extension-ai-denoiser":"^1.0.0","agora-rtc-sdk-ng":"4.23.2-1","agora-rte-extension":"^1.2.4","jsonwebtoken":"^9.0.2","node-fetch":"^2.x","opus-encdec":"^0.1.1","reconnecting-websocket":"^4.4.0","uuid":"^10.0.0","ws":"^8.11.0"},"devDependencies":{"@coze-infra/eslint-config":"workspace:*","@coze-infra/ts-config":"workspace:*","@coze-infra/vitest-config":"workspace:*","@rslib/core":"0.0.18","@swc/core":"^1.3.14","@types/jsonwebtoken":"^9.0.0","@types/node":"^20","@types/node-fetch":"^2.x","@types/uuid":"^9.0.1","@types/whatwg-fetch":"^0.0.33","@types/ws":"^8.5.1","@vitest/coverage-v8":"~2.1.9","axios":"^1.7.7","typescript":"^5.5.3","vitest":"~2.1.9"},"peerDependencies":{"axios":"^1.7.1"},"publishConfig":{"access":"public","registry":"https://registry.npmjs.org"},"cozePublishConfig":{"exports":{".":{"require":"./dist/cjs/index.js","import":"./dist/esm/index.mjs","types":"./dist/types/index.d.ts"},"./ws-tools":{"require":"./dist/cjs/ws-tools/index.js","import":"./dist/esm/ws-tools/index.mjs","types":"./dist/types/ws-tools/ws-tools/index.d.ts"}},"main":"dist/cjs/index.js","module":"dist/esm/index.mjs","types":"dist/types/index.d.ts"},"overrides":{"agora-extension-ai-denoiser":{"agora-rtc-sdk-ng":"$agora-rtc-sdk-ng"}}}'); // CONCATENATED MODULE: ./src/version.ts
const { version: version_version } = package_namespaceObject;
const getEnv = ()=>{
    const nodeVersion = process.version.slice(1); // Remove 'v' prefix
    const { platform } = process;
    let osName = platform.toLowerCase();
    let osVersion = __WEBPACK_EXTERNAL_MODULE_os__["default"].release();
    if ('darwin' === platform) {
        osName = 'macos';
        // Try to parse the macOS version
        try {
            const darwinVersion = __WEBPACK_EXTERNAL_MODULE_os__["default"].release().split('.');
            if (darwinVersion.length >= 2) {
                const majorVersion = parseInt(darwinVersion[0], 10);
                if (!isNaN(majorVersion) && majorVersion >= 9) {
                    const macVersion = majorVersion - 9;
                    osVersion = `10.${macVersion}.${darwinVersion[1]}`;
                }
            }
        } catch (error) {
        // Keep the default os.release() value if parsing fails
        }
    } else if ('win32' === platform) {
        osName = 'windows';
        osVersion = __WEBPACK_EXTERNAL_MODULE_os__["default"].release();
    } else if ('linux' === platform) {
        osName = 'linux';
        osVersion = __WEBPACK_EXTERNAL_MODULE_os__["default"].release();
    }
    return {
        osName,
        osVersion,
        nodeVersion
    };
};
const getUserAgent = ()=>{
    const { nodeVersion, osName, osVersion } = getEnv();
    return `coze-js/${version_version} node/${nodeVersion} ${osName}/${osVersion}`.toLowerCase();
};
const getNodeClientUserAgent = ()=>{
    const { osVersion, nodeVersion, osName } = getEnv();
    const ua = {
        version: version_version,
        lang: 'node',
        lang_version: nodeVersion,
        os_name: osName,
        os_version: osVersion
    };
    return JSON.stringify(ua);
};
const getBrowserClientUserAgent = ()=>{
    const browserInfo = {
        name: 'unknown',
        version: 'unknown'
    };
    const osInfo = {
        name: 'unknown',
        version: 'unknown'
    };
    const { userAgent } = navigator;
    if (userAgent) {
        // 检测操作系统及版本
        if (userAgent.indexOf('Windows') > -1) {
            var _userAgent_match;
            osInfo.name = 'windows';
            const windowsVersion = (null === (_userAgent_match = userAgent.match(/Windows NT ([0-9.]+)/)) || void 0 === _userAgent_match ? void 0 : _userAgent_match[1]) || 'unknown';
            osInfo.version = windowsVersion;
        } else if (userAgent.indexOf('Mac OS X') > -1) {
            var _userAgent_match1;
            osInfo.name = 'macos';
            // 将 10_15_7 格式转换为 10.15.7
            osInfo.version = ((null === (_userAgent_match1 = userAgent.match(/Mac OS X ([0-9_]+)/)) || void 0 === _userAgent_match1 ? void 0 : _userAgent_match1[1]) || 'unknown').replace(/_/g, '.');
        } else if (userAgent.indexOf('Linux') > -1) {
            var _userAgent_match2;
            osInfo.name = 'linux';
            osInfo.version = (null === (_userAgent_match2 = userAgent.match(/Linux ([0-9.]+)/)) || void 0 === _userAgent_match2 ? void 0 : _userAgent_match2[1]) || 'unknown';
        }
        // 检测浏览器及版本
        if (userAgent.indexOf('Chrome') > -1) {
            var _userAgent_match3;
            browserInfo.name = 'chrome';
            browserInfo.version = (null === (_userAgent_match3 = userAgent.match(/Chrome\/([0-9.]+)/)) || void 0 === _userAgent_match3 ? void 0 : _userAgent_match3[1]) || 'unknown';
        } else if (userAgent.indexOf('Firefox') > -1) {
            var _userAgent_match4;
            browserInfo.name = 'firefox';
            browserInfo.version = (null === (_userAgent_match4 = userAgent.match(/Firefox\/([0-9.]+)/)) || void 0 === _userAgent_match4 ? void 0 : _userAgent_match4[1]) || 'unknown';
        } else if (userAgent.indexOf('Safari') > -1) {
            var _userAgent_match5;
            browserInfo.name = 'safari';
            browserInfo.version = (null === (_userAgent_match5 = userAgent.match(/Version\/([0-9.]+)/)) || void 0 === _userAgent_match5 ? void 0 : _userAgent_match5[1]) || 'unknown';
        }
    }
    const ua = {
        version: version_version,
        browser: browserInfo.name,
        browser_version: browserInfo.version,
        os_name: osInfo.name,
        os_version: osInfo.version
    };
    return JSON.stringify(ua);
};
// Get UniApp client user agent
const getUniAppClientUserAgent = ()=>{
    // Get system info
    if (!(null == uni ? void 0 : uni.getSystemInfoSync)) return JSON.stringify({});
    const systemInfo = uni.getSystemInfoSync();
    const platformInfo = {
        name: 'unknown',
        version: 'unknown'
    };
    const osInfo = {
        name: 'unknown',
        version: 'unknown'
    };
    // Handle operating system info
    if ('android' === systemInfo.platform) {
        osInfo.name = 'android';
        osInfo.version = systemInfo.system || 'unknown';
    } else if ('ios' === systemInfo.platform) {
        osInfo.name = 'ios';
        osInfo.version = systemInfo.system || 'unknown';
    } else if ('windows' === systemInfo.platform) {
        osInfo.name = 'windows';
        osInfo.version = systemInfo.system || 'unknown';
    } else if ('mac' === systemInfo.platform) {
        osInfo.name = 'macos';
        osInfo.version = systemInfo.system || 'unknown';
    } else {
        // Other platforms use platform name directly
        osInfo.name = systemInfo.platform;
        osInfo.version = systemInfo.system || 'unknown';
    }
    // Handle app/platform info
    if (systemInfo.AppPlatform) {
        // App environment
        platformInfo.name = systemInfo.AppPlatform.toLowerCase();
        platformInfo.version = systemInfo.appVersion || 'unknown';
    } else if (systemInfo.uniPlatform) {
        // UniApp recognized platform
        platformInfo.name = systemInfo.uniPlatform;
        platformInfo.version = systemInfo.SDKVersion || 'unknown';
    } else {
        // Try to determine platform type from environment
        const { appName, appVersion } = systemInfo;
        if (appName) {
            platformInfo.name = appName.toLowerCase();
            platformInfo.version = appVersion || 'unknown';
        }
    }
    const ua = {
        version: version_version,
        framework: 'uniapp',
        platform: platformInfo.name,
        platform_version: platformInfo.version,
        os_name: osInfo.name,
        os_version: osInfo.version,
        screen_width: systemInfo.screenWidth,
        screen_height: systemInfo.screenHeight,
        device_model: systemInfo.model,
        device_brand: systemInfo.brand
    };
    return JSON.stringify(ua);
};
/**
 * default coze  base URL is api.coze.com
 */ const constant_COZE_COM_BASE_URL = 'https://api.coze.com';
/**
 * change to api.coze.cn if you use https://coze.cn
 */ const COZE_CN_BASE_URL = 'https://api.coze.cn';
/**
 * change to wss://ws.coze.cn if you use https://coze.cn
 */ const COZE_CN_BASE_WS_URL = 'wss://ws.coze.cn';
/* eslint-disable @typescript-eslint/no-explicit-any */ const handleError = (error)=>{
    if (!error.isAxiosError && (!error.code || !error.message)) return error_APIError.generate(500, void 0, `Unexpected error: ${error.message}`, void 0);
    if ('ECONNABORTED' === error.code && error.message.includes('timeout') || 'ETIMEDOUT' === error.code) {
        var _error_response;
        return new TimeoutError(408, void 0, `Request timed out: ${error.message}`, null === (_error_response = error.response) || void 0 === _error_response ? void 0 : _error_response.headers);
    }
    if ('ERR_CANCELED' === error.code) return new APIUserAbortError(error.message);
    else {
        var _error_response1, _error_response2, _error_response3;
        return error_APIError.generate((null === (_error_response1 = error.response) || void 0 === _error_response1 ? void 0 : _error_response1.status) || 500, null === (_error_response2 = error.response) || void 0 === _error_response2 ? void 0 : _error_response2.data, error.message, null === (_error_response3 = error.response) || void 0 === _error_response3 ? void 0 : _error_response3.headers);
    }
};
// node-fetch is used for streaming requests
const adapterFetch = async (options)=>{
    const response = await (0, __WEBPACK_EXTERNAL_MODULE_node_fetch__["default"])(options.url, {
        body: options.data,
        ...options
    });
    return {
        data: response.body,
        ...response
    };
};
const isSupportNativeFetch = ()=>{
    if (utils_isBrowser() || src_utils_isBrowserExtension() || utils_isUniApp()) return true;
    // native fetch is supported in node 18.0.0 or higher
    const version = process.version.slice(1);
    return compareVersions(version, '18.0.0') >= 0;
};
async function fetchAPI(url) {
    let options = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {};
    const axiosInstance = options.axiosInstance || __WEBPACK_EXTERNAL_MODULE_axios__["default"];
    // Add version check for streaming requests
    if (options.isStreaming && isAxiosStatic(axiosInstance)) {
        const axiosVersion = axiosInstance.VERSION || __WEBPACK_EXTERNAL_MODULE_axios__["default"].VERSION;
        if (!axiosVersion || compareVersions(axiosVersion, '1.7.1') < 0) throw new CozeError('Streaming requests require axios version 1.7.1 or higher. Please upgrade your axios version.');
    }
    // Check for 4101 authentication error
    // If BaseURL is set to overseas address, provide a warning to try setting it to the domestic address
    const checkError = ()=>{
        if (url.startsWith(constant_COZE_COM_BASE_URL)) console.warn(`
鉴权失败，如果您是国内用户，请将 baseURL 设置为 ${COZE_CN_BASE_URL} 示例：
new CozeAPI({
  // ...
  baseURL: COZE_CN_BASE_URL
})`);
    };
    const response = await axiosInstance({
        url,
        responseType: options.isStreaming ? 'stream' : 'json',
        adapter: options.isStreaming ? isSupportNativeFetch() ? 'fetch' : adapterFetch : void 0,
        ...options
    }).catch((error)=>{
        if ((null == error ? void 0 : error.status) === 401) checkError();
        throw handleError(error);
    });
    return {
        async *stream () {
            try {
                const stream = response.data;
                const reader = stream[Symbol.asyncIterator] ? stream[Symbol.asyncIterator]() : stream.getReader();
                const decoder = new TextDecoder();
                const fieldValues = {};
                let buffer = '';
                while(true){
                    const { done, value } = await (reader.next ? reader.next() : reader.read());
                    if (done) {
                        if (buffer) {
                            // If the stream ends without a newline, it means an error occurred
                            fieldValues.event = 'error';
                            fieldValues.data = buffer;
                            try {
                                const error = JSON.parse(buffer);
                                if ((null == error ? void 0 : error.code) === 4101) checkError();
                            // eslint-disable-next-line no-empty
                            } catch (e) {}
                            yield fieldValues;
                        }
                        break;
                    }
                    buffer += decoder.decode(value, {
                        stream: true
                    });
                    const lines = buffer.split('\n');
                    for(let i = 0; i < lines.length - 1; i++){
                        const line = lines[i];
                        const index = line.indexOf(':');
                        if (-1 !== index) {
                            const field = line.substring(0, index).trim();
                            const content = line.substring(index + 1).trim();
                            fieldValues[field] = content;
                            if ('data' === field) yield fieldValues;
                        }
                    }
                    buffer = lines[lines.length - 1]; // Keep the last incomplete line in the buffer
                }
            } catch (error) {
                handleError(error);
            }
        },
        json: ()=>response.data,
        response
    };
}
// Add version comparison utility
function compareVersions(v1, v2) {
    const v1Parts = v1.split('.').map(Number);
    const v2Parts = v2.split('.').map(Number);
    for(let i = 0; i < 3; i++){
        const part1 = v1Parts[i] || 0;
        const part2 = v2Parts[i] || 0;
        if (part1 > part2) return 1;
        if (part1 < part2) return -1;
    }
    return 0;
}
function isAxiosStatic(instance) {
    return !!(null == instance ? void 0 : instance.Axios);
}
/* eslint-disable max-params */ class core_APIClient {
    async getToken() {
        if ('function' == typeof this.token) return await this.token();
        return this.token;
    }
    async buildOptions(method, body, options) {
        const token = await this.getToken();
        const headers = {
            authorization: `Bearer ${token}`
        };
        if (utils_isUniApp()) headers['X-Coze-Client-User-Agent'] = getUniAppClientUserAgent();
        else if (utils_isBrowser() || src_utils_isBrowserExtension()) headers['X-Coze-Client-User-Agent'] = getBrowserClientUserAgent();
        else {
            headers['User-Agent'] = getUserAgent();
            headers['X-Coze-Client-User-Agent'] = getNodeClientUserAgent();
        }
        const config = mergeConfig(this.axiosOptions, options, {
            headers
        }, {
            headers: this.headers || {}
        });
        config.method = method;
        config.data = body;
        return config;
    }
    async buildWebsocketOptions(options) {
        const token = await this.getToken();
        const headers = {
            authorization: `Bearer ${token}`
        };
        if (utils_isUniApp()) headers['X-Coze-Client-User-Agent'] = getUniAppClientUserAgent();
        else if (utils_isBrowser()) headers['X-Coze-Client-User-Agent'] = getBrowserClientUserAgent();
        else {
            headers['User-Agent'] = getUserAgent();
            headers['X-Coze-Client-User-Agent'] = getNodeClientUserAgent();
        }
        var _this__config_debug;
        const config = mergeConfig({
            debug: null !== (_this__config_debug = this._config.debug) && void 0 !== _this__config_debug && _this__config_debug
        }, this._config.websocketOptions, options, {
            headers
        }, {
            headers: this.headers || {}
        });
        return config;
    }
    async makeRequest(apiUrl, method, body, isStream, options) {
        const fullUrl = `${this.baseURL}${apiUrl}`;
        try {
            const fetchOptions = await this.buildOptions(method, body, options);
            fetchOptions.isStreaming = isStream;
            fetchOptions.axiosInstance = this.axiosInstance;
            this.debugLog(null == options ? void 0 : options.debug, `--- request url: ${fullUrl}`);
            this.debugLog(null == options ? void 0 : options.debug, '--- request options:', fetchOptions);
            const { response, stream, json } = await fetchAPI(fullUrl, fetchOptions);
            this.debugLog(null == options ? void 0 : options.debug, `--- response status: ${response.status}`);
            this.debugLog(null == options ? void 0 : options.debug, '--- response headers: ', response.headers);
            var _response_headers;
            // Taro use `header`
            const contentType = (null !== (_response_headers = response.headers) && void 0 !== _response_headers ? _response_headers : response.header)['content-type'];
            if (isStream) {
                if (contentType && contentType.includes('application/json')) {
                    const result = await json();
                    const { code, msg } = result;
                    if (0 !== code && void 0 !== code) throw error_APIError.generate(response.status, result, msg, response.headers);
                }
                return stream();
            }
            if (!(contentType && contentType.includes('application/json'))) return await response.data;
            {
                const result = await json();
                const { code, msg } = result;
                if (0 !== code && void 0 !== code) throw error_APIError.generate(response.status, result, msg, response.headers);
                return result;
            }
        } catch (error) {
            var _this__config;
            // Call the onApiError callback if provided
            // This handles network errors and other exceptions not caught above
            const onApiError = (null == options ? void 0 : options.onApiError) || (null === (_this__config = this._config) || void 0 === _this__config ? void 0 : _this__config.onApiError);
            if (onApiError) onApiError(error);
            // Re-throw the error after calling the callback
            throw error;
        }
    }
    async post(apiUrl, body) {
        let isStream = arguments.length > 2 && void 0 !== arguments[2] && arguments[2], options = arguments.length > 3 ? arguments[3] : void 0;
        return this.makeRequest(apiUrl, 'POST', body, isStream, options);
    }
    async get(apiUrl, param, isStream, options) {
        // 拼接参数
        const queryString = Object.entries(param || {}).filter((param)=>{
            let [_, value] = param;
            return null != value;
        }).map((param)=>{
            let [key, value] = param;
            return `${key}=${value}`;
        }).join('&');
        return this.makeRequest(queryString ? `${apiUrl}${apiUrl.includes('?') ? '&' : '?'}${queryString}` : apiUrl, 'GET', void 0, isStream, options);
    }
    async put(apiUrl, body, isStream, options) {
        return this.makeRequest(apiUrl, 'PUT', body, isStream, options);
    }
    async delete(apiUrl, isStream, options) {
        return this.makeRequest(apiUrl, 'DELETE', void 0, isStream, options);
    }
    async makeWebsocket(apiUrl, options) {
        const fullUrl = `${this.baseWsURL}${apiUrl}`;
        const websocketOptions = await this.buildWebsocketOptions(options);
        this.debugLog(null == options ? void 0 : options.debug, `--- websocket url: ${fullUrl}`);
        this.debugLog(null == options ? void 0 : options.debug, '--- websocket options:', websocketOptions);
        const ws = new WebSocketAPI(fullUrl, websocketOptions);
        return ws;
    }
    getConfig() {
        return this._config;
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    debugLog() {
        let forceDebug = arguments.length > 0 && void 0 !== arguments[0] && arguments[0];
        for(var _len = arguments.length, msgs = new Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++)msgs[_key - 1] = arguments[_key];
        if (this.debug || forceDebug) console.debug(...msgs);
    }
    constructor(config){
        this._config = config;
        this.baseURL = config.baseURL || constant_COZE_COM_BASE_URL;
        this.baseWsURL = config.baseWsURL || COZE_CN_BASE_WS_URL;
        this.token = config.token;
        this.axiosOptions = config.axiosOptions || {};
        this.axiosInstance = config.axiosInstance;
        this.debug = config.debug || false;
        this.allowPersonalAccessTokenInBrowser = config.allowPersonalAccessTokenInBrowser || false;
        this.headers = config.headers;
        if (utils_isBrowser() && 'function' != typeof this.token && isPersonalAccessToken(this.token) && !this.allowPersonalAccessTokenInBrowser) throw new CozeError('Browser environments do not support authentication using Personal Access Token (PAT) by default.\nas it may expose secret API keys. \n\nPlease use OAuth2.0 authentication mechanism. see:\nhttps://www.coze.com/docs/developer_guides/oauth_apps?_lang=en \n\nIf you need to force use, please set the `allowPersonalAccessTokenInBrowser` option to `true`. \n\ne.g new CozeAPI({ token, allowPersonalAccessTokenInBrowser: true });\n\n');
    }
}
core_APIClient.APIError = error_APIError;
core_APIClient.BadRequestError = BadRequestError;
core_APIClient.AuthenticationError = AuthenticationError;
core_APIClient.PermissionDeniedError = PermissionDeniedError;
core_APIClient.NotFoundError = NotFoundError;
core_APIClient.RateLimitError = RateLimitError;
core_APIClient.InternalServerError = InternalServerError;
core_APIClient.GatewayError = GatewayError;
core_APIClient.TimeoutError = TimeoutError;
core_APIClient.UserAbortError = APIUserAbortError;
class CozeAPI extends core_APIClient {
    constructor(...args){
        super(...args), this.bots = new Bots(this), this.chat = new Chat(this), this.conversations = new Conversations(this), this.files = new Files(this), /**
   * @deprecated
   */ this.knowledge = new Knowledge(this), this.datasets = new Datasets(this), this.workflows = new Workflows(this), this.workspaces = new WorkSpaces(this), this.audio = new Audio(this), this.templates = new Templates(this), this.websockets = new Websockets(this), this.variables = new Variables(this), this.users = new Users(this);
    }
}
class WsSpeechClient {
    async init() {
        if (this.ws) return this.ws;
        const ws = await this.api.websockets.audio.speech.create({
            entity_type: this.config.entityType,
            entity_id: this.config.entityId
        }, this.config.websocketOptions);
        this.ws = ws;
        let isResolved = false;
        this.trackId = `my-track-id-${(0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)()}`;
        this.totalDuration = 0;
        if (this.playbackTimeout) {
            clearTimeout(this.playbackTimeout);
            this.playbackTimeout = null;
        }
        this.playbackStartTime = null;
        return new Promise((resolve, reject)=>{
            ws.onopen = ()=>{
                console.debug('[speech] ws open');
            };
            ws.onmessage = (data)=>{
                // Trigger all registered event listeners
                this.emit('data', data);
                this.emit(data.event_type, data);
                if (data.event_type === types_WebsocketsEventType.ERROR) {
                    this.closeWs();
                    if (isResolved) return;
                    isResolved = true;
                    reject(new error_APIError(data.data.code, {
                        code: data.data.code,
                        msg: data.data.msg,
                        detail: data.detail
                    }, data.data.msg, void 0));
                    return;
                }
                if (data.event_type === types_WebsocketsEventType.SPEECH_CREATED) {
                    resolve(ws);
                    isResolved = true;
                } else if (data.event_type === types_WebsocketsEventType.SPEECH_AUDIO_UPDATE) {
                    this.audioDeltaList.push(data.data.delta);
                    if (1 === this.audioDeltaList.length) this.handleAudioMessage();
                } else if (data.event_type === types_WebsocketsEventType.SPEECH_AUDIO_COMPLETED) {
                    console.debug('[speech] totalDuration', this.totalDuration);
                    if (this.playbackStartTime) {
                        // 剩余时间 = 总时间 - 已播放时间 - 已暂停时间
                        const now = new Date().getTime();
                        const remaining = this.totalDuration - (now - this.playbackStartTime) / 1000 - this.elapsedBeforePause;
                        this.playbackTimeout = setTimeout(()=>{
                            this.emit('completed', void 0);
                            this.playbackStartTime = null;
                            this.elapsedBeforePause = 0;
                        }, 1000 * remaining);
                    }
                    this.closeWs();
                }
            };
            ws.onerror = (error, event)=>{
                console.error('[speech] WebSocket error', error, event);
                this.emit('data', error);
                this.emit(types_WebsocketsEventType.ERROR, error);
                this.closeWs();
                if (isResolved) return;
                isResolved = true;
                reject(new error_APIError(error.data.code, error, error.data.msg, void 0));
            };
            ws.onclose = ()=>{
                console.debug('[speech] ws close');
            };
        });
    }
    async connect() {
        let { voiceId, speechRate } = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
        var _this_ws;
        await this.init();
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.SPEECH_UPDATE,
            data: {
                output_audio: {
                    codec: 'pcm',
                    voice_id: voiceId || void 0,
                    speech_rate: speechRate || void 0
                }
            }
        });
    }
    async disconnect() {
        if (this.playbackTimeout) clearTimeout(this.playbackTimeout);
        this.audioDeltaList.length = 0;
        await this.wavStreamPlayer.interrupt();
        this.closeWs();
    }
    append(message) {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.INPUT_TEXT_BUFFER_APPEND,
            data: {
                delta: message
            }
        });
    }
    complete() {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.INPUT_TEXT_BUFFER_COMPLETE
        });
    }
    appendAndComplete(message) {
        this.append(message);
        this.complete();
    }
    async interrupt() {
        await this.disconnect();
        this.emit('completed', void 0);
        console.debug('[speech] playback completed', this.totalDuration);
    }
    async pause() {
        if (this.playbackTimeout) {
            clearTimeout(this.playbackTimeout);
            this.playbackTimeout = null;
        }
        if (this.playbackStartTime && !this.playbackPauseTime) {
            this.playbackPauseTime = Date.now();
            this.elapsedBeforePause += (this.playbackPauseTime - this.playbackStartTime) / 1000;
        }
        await this.wavStreamPlayer.pause();
    }
    async resume() {
        if (this.playbackPauseTime) {
            this.playbackStartTime = Date.now();
            this.playbackPauseTime = null;
            // Update the timeout with remaining duration
            if (this.playbackTimeout) clearTimeout(this.playbackTimeout);
            const remaining = this.totalDuration - this.elapsedBeforePause;
            this.playbackTimeout = setTimeout(()=>{
                this.emit('completed', void 0);
                console.debug('[speech] playback completed', this.totalDuration);
                this.playbackStartTime = null;
                this.elapsedBeforePause = 0;
            }, 1000 * remaining);
        }
        await this.wavStreamPlayer.resume();
    }
    async togglePlay() {
        if (this.isPlaying()) await this.pause();
        else await this.resume();
    }
    isPlaying() {
        return this.wavStreamPlayer.isPlaying();
    }
    on(event, callback) {
        var _this_listeners_get;
        if (!this.listeners.has(event)) this.listeners.set(event, new Set());
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.add(callback);
    }
    off(event, callback) {
        var _this_listeners_get;
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.delete(callback);
    }
    closeWs() {
        var _this_ws;
        if ((null === (_this_ws = this.ws) || void 0 === _this_ws ? void 0 : _this_ws.readyState) === 1) {
            var _this_ws1;
            null === (_this_ws1 = this.ws) || void 0 === _this_ws1 || _this_ws1.close();
        }
        this.ws = null;
    }
    emit(event, data) {
        var _this_listeners_get;
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.forEach((callback)=>callback(data));
    }
    constructor(config){
        this.ws = null;
        this.listeners = new Map();
        this.trackId = 'default';
        this.totalDuration = 0;
        this.playbackStartTime = null;
        this.playbackPauseTime = null;
        this.playbackTimeout = null;
        this.elapsedBeforePause = 0;
        this.audioDeltaList = [];
        this.handleAudioMessage = async ()=>{
            const message = this.audioDeltaList[0];
            const decodedContent = atob(message);
            const arrayBuffer = new ArrayBuffer(decodedContent.length);
            const view = new Uint8Array(arrayBuffer);
            for(let i = 0; i < decodedContent.length; i++)view[i] = decodedContent.charCodeAt(i);
            // Calculate duration in seconds
            const bytesPerSecond = 48000; // sampleRate * channels * (bitDepth/8)
            const duration = arrayBuffer.byteLength / bytesPerSecond;
            this.totalDuration += duration;
            try {
                await this.wavStreamPlayer.add16BitPCM(arrayBuffer, this.trackId);
                // Start or update the playback timer
                if (!this.playbackStartTime && !this.playbackPauseTime) {
                    this.playbackStartTime = Date.now();
                    this.elapsedBeforePause = 0;
                }
                // Remove the processed message and process the next one if available
                this.audioDeltaList.shift();
                if (this.audioDeltaList.length > 0) this.handleAudioMessage();
            } catch (error) {
                console.warn(`[speech] wavStreamPlayer error ${null == error ? void 0 : error.message}`, error);
            }
        };
        this.api = new CozeAPI({
            baseWsURL: COZE_CN_BASE_WS_URL,
            ...config
        });
        this.wavStreamPlayer = new WavStreamPlayer({
            sampleRate: 24000
        });
        this.config = config;
    }
}
/* ESM default export */ const speech = WsSpeechClient;
const WavProcessorWorklet = `
class WavProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.port.onmessage = this.handleMessage.bind(this);
    this.initialize();
  }

  initialize() {
    this.chunks = [];
    this.isRecording = false;
  }

  handleMessage(event) {
    const { type } = event.data;

    switch (type) {
      case 'start':
        this.isRecording = true;
        break;
      case 'stop':
        if (this.isRecording) {
          this.isRecording = false;
          const audioData = this.processChunks();
          this.port.postMessage({
            type: 'audio',
            audioData,
            sampleRate: sampleRate,
            numChannels: this.chunks[0]?.length || 1,
          });
          this.initialize();
        }
        break;
    }
  }

  processChunks() {
    // Combine all channels
    const channels = [];
    const firstChunk = this.chunks[0] || [];
    const numChannels = firstChunk.length || 1;

    for (let channel = 0; channel < numChannels; channel++) {
      const length = this.chunks.reduce((sum, chunk) => sum + chunk[channel].length, 0);
      const channelData = new Float32Array(length);
      let offset = 0;

      for (const chunk of this.chunks) {
        channelData.set(chunk[channel], offset);
        offset += chunk[channel].length;
      }

      channels.push(channelData);
    }

    // Interleave channels
    const interleaved = new Float32Array(channels[0].length * channels.length);
    for (let i = 0; i < channels[0].length; i++) {
      for (let channel = 0; channel < channels.length; channel++) {
        interleaved[i * channels.length + channel] = channels[channel][i];
      }
    }

    return interleaved;
  }

  process(inputs) {
    const input = inputs[0];
    if (input && input[0] && this.isRecording) {
      // Clone the input data
      const chunk = input.map(channel => channel.slice());
      this.chunks.push(chunk);
    }
    return true;
  }
}

registerProcessor('wav-processor', WavProcessor);
`;
let wav_worklet_processor_src = '';
if (utils_isBrowserExtension()) wav_worklet_processor_src = chrome.runtime.getURL('wav-worklet-processor.js');
else {
    const script = new Blob([
        WavProcessorWorklet
    ], {
        type: 'application/javascript'
    });
    wav_worklet_processor_src = URL.createObjectURL(script);
}
const WavProcessorSrc = wav_worklet_processor_src;
class WavAudioProcessor extends __WEBPACK_EXTERNAL_MODULE_agora_rte_extension__.AudioProcessor {
    async onNode(node, context) {
        const audioContext = context.getAudioContext();
        await audioContext.audioWorklet.addModule(WavProcessorSrc);
        this.workletNode = new window.AudioWorkletNode(audioContext, 'wav-processor');
        null == node || node.connect(this.workletNode);
        this.workletNode.port.onmessage = (event)=>{
            if ('audio' === event.data.type) {
                var _this_onAudioData, _this;
                const { audioData, sampleRate, numChannels } = event.data;
                const wavBlob = this.createWavFile(audioData, sampleRate, numChannels);
                console.log('[wav-audio-processor] onAudioData', event.data);
                null === (_this_onAudioData = (_this = this).onAudioData) || void 0 === _this_onAudioData || _this_onAudioData.call(_this, {
                    wav: wavBlob
                });
            }
        };
        this.startRecording();
        this.output(node, context);
    }
    startRecording() {
        var _this_workletNode;
        null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode || _this_workletNode.port.postMessage({
            type: 'start'
        });
    }
    stopRecording() {
        var _this_workletNode;
        null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode || _this_workletNode.port.postMessage({
            type: 'stop'
        });
    }
    createWavFile(audioData, sampleRate, numChannels) {
        const buffer = floatTo16BitPCM(audioData);
        // const dataView = new DataView(buffer);
        const wavBuffer = new ArrayBuffer(44 + buffer.byteLength);
        const view = new DataView(wavBuffer);
        // Write WAV header
        const writeString = (view2, offset, string)=>{
            for(let i = 0; i < string.length; i++)view2.setUint8(offset + i, string.charCodeAt(i));
        };
        // RIFF identifier
        writeString(view, 0, 'RIFF');
        // File length
        view.setUint32(4, 36 + buffer.byteLength, true);
        // RIFF type
        writeString(view, 8, 'WAVE');
        // Format chunk identifier
        writeString(view, 12, 'fmt ');
        // Format chunk length
        view.setUint32(16, 16, true);
        // Sample format (raw)
        view.setUint16(20, 1, true);
        // Channel count
        view.setUint16(22, numChannels, true);
        // Sample rate
        view.setUint32(24, sampleRate, true);
        // Byte rate (sample rate * block align)
        view.setUint32(28, sampleRate * numChannels * 2, true);
        // Block align (channel count * bytes per sample)
        view.setUint16(32, 2 * numChannels, true);
        // Bits per sample
        view.setUint16(34, 16, true);
        // Data chunk identifier
        writeString(view, 36, 'data');
        // Data chunk length
        view.setUint32(40, buffer.byteLength, true);
        // Write audio data
        const uint8Array = new Uint8Array(buffer);
        const wavUint8Array = new Uint8Array(wavBuffer);
        wavUint8Array.set(uint8Array, 44);
        return new Blob([
            wavBuffer
        ], {
            type: 'audio/wav'
        });
    }
    /**
   * en: Destroy and cleanup resources
   * zh: 销毁并清理资源
   */ destroy() {
        var _this_workletNode;
        // 1. 停止录音
        this.stopRecording();
        // 2. 移除 workletNode 的消息监听器
        if (null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode ? void 0 : _this_workletNode.port) this.workletNode.port.onmessage = null;
        // 3. 断开 workletNode 的连接
        if (this.workletNode) {
            this.workletNode.disconnect();
            this.workletNode = void 0;
        }
        // 4. 释放 Blob URL
        if (WavProcessorSrc) URL.revokeObjectURL(WavProcessorSrc);
        // 5. 清理回调函数
        this.onAudioData = void 0;
    }
    constructor(onAudioData){
        super();
        this.name = 'WavAudioProcessor';
        this.onAudioData = onAudioData;
    }
}
/* ESM default export */ const wav_audio_processor = WavAudioProcessor;
/* eslint-disable @typescript-eslint/no-explicit-any */ const pcm_worklet_processor_AudioProcessorWorklet = `
class PCMProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.port.onmessage = this.handleMessage.bind(this);
    this.initialize();
  }

  initialize() {
    this.buffer = [];
    this.bufferSize = 1024;
    this.isRecording = false;
  }

  handleMessage(event) {
    const { type } = event.data;

    switch (type) {
      case 'start':
        this.isRecording = true;
        break;
      case 'stop':
        this.isRecording = false;
        break;
    }
  }

  process(inputs) {
    const input = inputs[0];
    if (input.length > 0 && this.isRecording) {
      // 将当前输入添加到缓冲区
      this.buffer = this.buffer.concat(Array.from(input[0]));

      // 当缓冲区达到或超过目标大小时发送数据
      if (this.buffer.length >= this.bufferSize) {
        // 发送1024个字节的数据
        this.port.postMessage({
          audioData: new Float32Array(this.buffer),
        });

        // 清空缓冲区
        this.buffer = [];
      }
    }
    return true;
  }
}

registerProcessor('pcm-processor', PCMProcessor);
`;
let pcm_worklet_processor_src = '';
if (utils_isBrowserExtension()) pcm_worklet_processor_src = chrome.runtime.getURL('pcm-worklet-processor.js');
else {
    const script = new Blob([
        pcm_worklet_processor_AudioProcessorWorklet
    ], {
        type: 'application/javascript'
    });
    pcm_worklet_processor_src = URL.createObjectURL(script);
}
const pcm_worklet_processor_AudioProcessorSrc = pcm_worklet_processor_src;
/**
 * BaseAudioProcessor
 */ class BaseAudioProcessor extends __WEBPACK_EXTERNAL_MODULE_agora_rte_extension__.AudioProcessor {
}
/* ESM default export */ const base_audio_processor = BaseAudioProcessor;
class PcmAudioProcessor extends base_audio_processor {
    async onNode(node, context) {
        const audioContext = context.getAudioContext();
        await audioContext.audioWorklet.addModule(pcm_worklet_processor_AudioProcessorSrc);
        this.workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');
        null == node || node.connect(this.workletNode);
        // workletNode.connect(node);
        this.workletNode.port.onmessage = (event)=>{
            var _this_chunkProcessor, _this;
            const float32 = event.data.audioData;
            let buffer;
            switch(this.encoding){
                case 'g711a':
                    {
                        const float32_8000 = downsampleTo8000(float32);
                        const pcm16_8000 = float32ToInt16Array(float32_8000);
                        buffer = encodeG711A(pcm16_8000).buffer;
                        break;
                    }
                case 'g711u':
                    {
                        const float32_8000 = downsampleTo8000(float32);
                        const pcm16_8000 = float32ToInt16Array(float32_8000);
                        buffer = encodeG711U(pcm16_8000).buffer;
                        break;
                    }
                case 'pcm':
                default:
                    buffer = floatTo16BitPCM(float32);
            }
            null === (_this_chunkProcessor = (_this = this).chunkProcessor) || void 0 === _this_chunkProcessor || _this_chunkProcessor.call(_this, buffer);
        };
        this.startRecording();
        this.output(node, context);
    }
    startRecording() {
        var _this_workletNode;
        null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode || _this_workletNode.port.postMessage({
            type: 'start'
        });
    }
    stopRecording() {
        var _this_workletNode;
        null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode || _this_workletNode.port.postMessage({
            type: 'stop'
        });
    }
    /**
   * en: Destroy and cleanup resources
   * zh: 销毁并清理资源
   */ destroy() {
        var _this_workletNode;
        // 1. 停止录音
        this.stopRecording();
        // 2. 移除 workletNode 的消息监听器
        if (null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode ? void 0 : _this_workletNode.port) this.workletNode.port.onmessage = null;
        // 3. 断开 workletNode 的连接
        if (this.workletNode) {
            this.workletNode.disconnect();
            this.workletNode = void 0;
        }
        // 4. 释放 Blob URL
        if (pcm_worklet_processor_AudioProcessorSrc) URL.revokeObjectURL(pcm_worklet_processor_AudioProcessorSrc);
        // 5. 清理回调函数
        this.chunkProcessor = void 0;
    }
    constructor(chunkProcessor, encoding = 'pcm'){
        super();
        this.name = 'PcmAudioProcessor';
        this.chunkProcessor = chunkProcessor;
        this.encoding = encoding;
    }
}
/* ESM default export */ const pcm_audio_processor = PcmAudioProcessor;
// @ts-expect-error no types
// @ts-expect-error no types
/**
 * OpusAudioProcessor
 */ class OpusAudioProcessor extends base_audio_processor {
    async onNode(node, context) {
        const audioContext = context.getAudioContext();
        await audioContext.audioWorklet.addModule(pcm_worklet_processor_AudioProcessorSrc);
        this.workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');
        node.connect(this.workletNode);
        // 初始化裸 Opus 编码器
        const encoderConfig = {
            encoderApplication: 2049,
            encoderFrameSize: 20,
            encoderSampleRate: audioContext.sampleRate,
            numberOfChannels: 1,
            rawOpus: true,
            originalSampleRate: audioContext.sampleRate
        };
        this.encoder = new __WEBPACK_EXTERNAL_MODULE_opus_encdec_src_oggOpusEncoder_js__.OggOpusEncoder(encoderConfig, __WEBPACK_EXTERNAL_MODULE_opus_encdec_dist_libopus_encoder_js__["default"]);
        if (false === this.encoder.isReady && this.encoder.onready) await new Promise((resolve)=>{
            this.encoder.onready = resolve;
        });
        this.encoderReady = true;
        this.workletNode.port.onmessage = (event)=>{
            if (!this.encoderReady) return;
            const float32 = event.data.audioData;
            // Opus-encdec 直接支持 Float32Array
            this.encoder.encode([
                float32
            ]);
            const packets = this.encoder.encodedData || [];
            this.encoder.encodedData = [];
            if (packets && packets.length) for (const packet of packets){
                var _this_chunkProcessor, _this;
                null === (_this_chunkProcessor = (_this = this).chunkProcessor) || void 0 === _this_chunkProcessor || _this_chunkProcessor.call(_this, packet.buffer);
            }
        };
        this.startRecording();
        this.output(node, context);
    }
    startRecording() {
        var _this_workletNode;
        null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode || _this_workletNode.port.postMessage({
            type: 'start'
        });
    }
    stopRecording() {
        var _this_workletNode;
        null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode || _this_workletNode.port.postMessage({
            type: 'stop'
        });
    }
    /**
   * en: Destroy and cleanup resources
   * zh: 销毁并清理资源
   */ destroy() {
        var _this_workletNode;
        this.stopRecording();
        if (null === (_this_workletNode = this.workletNode) || void 0 === _this_workletNode ? void 0 : _this_workletNode.port) this.workletNode.port.onmessage = null;
        if (this.workletNode) {
            this.workletNode.disconnect();
            this.workletNode = void 0;
        }
        if (pcm_worklet_processor_AudioProcessorSrc) URL.revokeObjectURL(pcm_worklet_processor_AudioProcessorSrc);
        this.chunkProcessor = void 0;
        if (this.encoder) {
            this.encoder.destroy();
            this.encoder = void 0;
        }
        this.encoderReady = false;
    }
    constructor(chunkProcessor){
        super(), this.encoderReady = false;
        this.name = 'OpusAudioProcessor';
        this.chunkProcessor = chunkProcessor;
    }
}
/* ESM default export */ const opus_audio_processor = OpusAudioProcessor;
var pcm_recorder_AIDenoiserProcessorMode = /*#__PURE__*/ function(AIDenoiserProcessorMode) {
    AIDenoiserProcessorMode["NSNG"] = "NSNG";
    AIDenoiserProcessorMode["STATIONARY_NS"] = "STATIONARY_NS";
    return AIDenoiserProcessorMode;
}({});
var pcm_recorder_AIDenoiserProcessorLevel = /*#__PURE__*/ function(AIDenoiserProcessorLevel) {
    AIDenoiserProcessorLevel["SOFT"] = "SOFT";
    AIDenoiserProcessorLevel["AGGRESSIVE"] = "AGGRESSIVE";
    return AIDenoiserProcessorLevel;
}({});
class PcmRecorder {
    async start() {
        let inputAudioCodec = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : 'pcm';
        const { deviceId, mediaStreamTrack, audioCaptureConfig, wavRecordConfig, debug } = this.config;
        if (mediaStreamTrack) {
            if ('function' == typeof mediaStreamTrack) this.audioTrack = await (0, __WEBPACK_EXTERNAL_MODULE_agora_rtc_sdk_ng_esm__.createCustomAudioTrack)({
                mediaStreamTrack: await mediaStreamTrack()
            });
            else this.audioTrack = await (0, __WEBPACK_EXTERNAL_MODULE_agora_rtc_sdk_ng_esm__.createCustomAudioTrack)({
                mediaStreamTrack
            });
        } else {
            var _audioCaptureConfig_echoCancellation, _audioCaptureConfig_noiseSuppression, _audioCaptureConfig_autoGainControl;
            // Get microphone audio track
            // See:https://api-ref.agora.io/en/video-sdk/web/4.x/interfaces/microphoneaudiotrackinitconfig.html
            this.audioTrack = await (0, __WEBPACK_EXTERNAL_MODULE_agora_rtc_sdk_ng_esm__.createMicrophoneAudioTrack)({
                AEC: null === (_audioCaptureConfig_echoCancellation = null == audioCaptureConfig ? void 0 : audioCaptureConfig.echoCancellation) || void 0 === _audioCaptureConfig_echoCancellation || _audioCaptureConfig_echoCancellation,
                ANS: null === (_audioCaptureConfig_noiseSuppression = null == audioCaptureConfig ? void 0 : audioCaptureConfig.noiseSuppression) || void 0 === _audioCaptureConfig_noiseSuppression || _audioCaptureConfig_noiseSuppression,
                AGC: null === (_audioCaptureConfig_autoGainControl = null == audioCaptureConfig ? void 0 : audioCaptureConfig.autoGainControl) || void 0 === _audioCaptureConfig_autoGainControl || _audioCaptureConfig_autoGainControl,
                microphoneId: deviceId,
                encoderConfig: {
                    sampleRate: this.getSampleRate()
                }
            });
        }
        // 降噪前音频
        if (debug && (null == wavRecordConfig ? void 0 : wavRecordConfig.enableSourceRecord)) this.wavAudioProcessor = new wav_audio_processor((audioData)=>{
            var _this_wavAudioCallback, _this;
            null === (_this_wavAudioCallback = (_this = this).wavAudioCallback) || void 0 === _this_wavAudioCallback || _this_wavAudioCallback.call(_this, audioData.wav, 'source');
        });
        // 降噪后音频
        if (debug && (null == wavRecordConfig ? void 0 : wavRecordConfig.enableDenoiseRecord)) this.wavAudioProcessor2 = new wav_audio_processor((audioData)=>{
            var _this_wavAudioCallback, _this;
            null === (_this_wavAudioCallback = (_this = this).wavAudioCallback) || void 0 === _this_wavAudioCallback || _this_wavAudioCallback.call(_this, audioData.wav, 'denoise');
        });
        // 实时音频处理
        if ('opus' === inputAudioCodec) this.audioProcessor = new opus_audio_processor((data)=>{
            var _this_pcmAudioCallback, _this;
            null === (_this_pcmAudioCallback = (_this = this).pcmAudioCallback) || void 0 === _this_pcmAudioCallback || _this_pcmAudioCallback.call(_this, {
                raw: data
            });
        });
        else // pcm 音频处理
        this.audioProcessor = new pcm_audio_processor((data)=>{
            var _this_pcmAudioCallback, _this;
            null === (_this_pcmAudioCallback = (_this = this).pcmAudioCallback) || void 0 === _this_pcmAudioCallback || _this_pcmAudioCallback.call(_this, {
                raw: data
            });
        }, inputAudioCodec);
        let audioProcessor;
        if (this.isSupportAIDenoiser()) {
            if (!PcmRecorder.denoiser) return;
            this.log('support ai denoiser');
            this.processor = PcmRecorder.denoiser.createProcessor();
            audioProcessor = this.wavAudioProcessor ? this.audioTrack.pipe(this.wavAudioProcessor).pipe(this.processor) : this.audioTrack.pipe(this.processor);
            audioProcessor = audioProcessor.pipe(this.audioProcessor);
            if (this.wavAudioProcessor2) audioProcessor = audioProcessor.pipe(this.wavAudioProcessor2);
            audioProcessor.pipe(this.audioTrack.processorDestination);
            this.handleProcessor();
        } else {
            audioProcessor = this.audioTrack.pipe(this.audioProcessor);
            if (this.wavAudioProcessor) audioProcessor = audioProcessor.pipe(this.wavAudioProcessor);
            audioProcessor.pipe(this.audioTrack.processorDestination);
        }
    }
    record() {
        let { pcmAudioCallback, wavAudioCallback, dumpAudioCallback } = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
        if (!this.audioTrack) throw new Error('audioTrack is not initialized');
        if (this.isSupportAIDenoiser() && !this.processor) throw new Error('processor is not initialized');
        this.pcmAudioCallback = pcmAudioCallback;
        this.wavAudioCallback = wavAudioCallback;
        this.dumpAudioCallback = dumpAudioCallback;
        this.recording = true;
    }
    async handleProcessor() {
        if (!this.processor) return;
        await this.processor.enable();
        this.processor.on('overload', async ()=>{
            var _this_processor;
            console.warn('denoiser processor overload');
            // 调整为稳态降噪模式，临时关闭 AI 降噪
            await (null === (_this_processor = this.processor) || void 0 === _this_processor ? void 0 : _this_processor.setMode("STATIONARY_NS"));
        // 完全关闭 AI 降噪，使用浏览器自带的降噪
        // await processor.disable()
        });
        this.processor.on('dump', (blob, name)=>{
            var _this_dumpAudioCallback, _this;
            null === (_this_dumpAudioCallback = (_this = this).dumpAudioCallback) || void 0 === _this_dumpAudioCallback || _this_dumpAudioCallback.call(_this, blob, name);
        // const objectURL = URL.createObjectURL(blob);
        // const tag = document.createElement('a');
        // tag.download = name;
        // tag.href = objectURL;
        // tag.click();
        // setTimeout(() => {
        //   URL.revokeObjectURL(objectURL);
        // }, 0);
        });
        this.processor.on('dumpend', ()=>{
            this.log('dump ended!!');
        });
    }
    /**
   * en: Pause audio recording temporarily
   * zh: 暂时暂停音频录制
   */ pause() {
        if (this.recording) {
            var _this_wavAudioProcessor, _this_wavAudioProcessor2, _this_audioProcessor;
            // 1. 暂停音频轨道，而不是关闭它
            if (this.audioTrack) {
                const mediaStreamTrack = this.audioTrack.getMediaStreamTrack();
                mediaStreamTrack.enabled = false; // 暂停音频采集
            }
            null === (_this_wavAudioProcessor = this.wavAudioProcessor) || void 0 === _this_wavAudioProcessor || _this_wavAudioProcessor.stopRecording();
            null === (_this_wavAudioProcessor2 = this.wavAudioProcessor2) || void 0 === _this_wavAudioProcessor2 || _this_wavAudioProcessor2.stopRecording();
            null === (_this_audioProcessor = this.audioProcessor) || void 0 === _this_audioProcessor || _this_audioProcessor.stopRecording();
            // 3. 更新录制状态
            this.recording = false;
        } else this.warn('error: recorder is not recording');
    }
    /**
   * en: Resume audio recording
   * zh: 恢复音频录制
   */ resume() {
        if (!this.recording && this.audioTrack) {
            var _this_wavAudioProcessor, _this_wavAudioProcessor2, _this_audioProcessor;
            // 1. 重新启用音频轨道
            const mediaStreamTrack = this.audioTrack.getMediaStreamTrack();
            mediaStreamTrack.enabled = true; // 恢复音频采集
            null === (_this_wavAudioProcessor = this.wavAudioProcessor) || void 0 === _this_wavAudioProcessor || _this_wavAudioProcessor.startRecording();
            null === (_this_wavAudioProcessor2 = this.wavAudioProcessor2) || void 0 === _this_wavAudioProcessor2 || _this_wavAudioProcessor2.startRecording();
            null === (_this_audioProcessor = this.audioProcessor) || void 0 === _this_audioProcessor || _this_audioProcessor.startRecording();
            // 3. 更新录制状态
            this.recording = true;
        } else this.warn('recorder is recording');
    }
    /**
   * en: Destroy and cleanup all resources
   * zh: 销毁并清理所有资源
   */ destroy() {
        var _this_wavAudioProcessor, _this_wavAudioProcessor2, _this_audioProcessor;
        null === (_this_wavAudioProcessor = this.wavAudioProcessor) || void 0 === _this_wavAudioProcessor || _this_wavAudioProcessor.destroy();
        null === (_this_wavAudioProcessor2 = this.wavAudioProcessor2) || void 0 === _this_wavAudioProcessor2 || _this_wavAudioProcessor2.destroy();
        null === (_this_audioProcessor = this.audioProcessor) || void 0 === _this_audioProcessor || _this_audioProcessor.destroy();
        // 2. 关闭并清理音频轨道
        if (this.audioTrack) {
            this.audioTrack.close();
            this.audioTrack = void 0;
        }
        // 3. 清理 AI 降噪处理器
        if (this.processor) {
            // 移除事件监听器
            this.processor.removeAllListeners();
            // 禁用处理器
            this.processor.disable();
            this.processor = void 0;
        }
        this.pcmAudioCallback = void 0;
        this.wavAudioCallback = void 0;
        this.dumpAudioCallback = void 0;
        // 4. 重置录音状态
        this.recording = false;
    }
    getStatus() {
        if (this.recording) return 'recording';
        return 'ended';
    }
    getDenoiserEnabled() {
        var _this_processor;
        return null === (_this_processor = this.processor) || void 0 === _this_processor ? void 0 : _this_processor.enabled;
    }
    async setDenoiserEnabled(enabled) {
        if (this.checkProcessor()) {
            if (enabled) {
                var _this_processor;
                await (null === (_this_processor = this.processor) || void 0 === _this_processor ? void 0 : _this_processor.enable());
            } else {
                var _this_processor1;
                await (null === (_this_processor1 = this.processor) || void 0 === _this_processor1 ? void 0 : _this_processor1.disable());
            }
        }
    }
    async setDenoiserMode(mode) {
        if (this.checkProcessor()) {
            var _this_processor;
            await (null === (_this_processor = this.processor) || void 0 === _this_processor ? void 0 : _this_processor.setMode(mode));
        }
    }
    async setDenoiserLevel(level) {
        if (this.checkProcessor()) {
            var _this_processor;
            await (null === (_this_processor = this.processor) || void 0 === _this_processor ? void 0 : _this_processor.setLevel(level));
        }
    }
    /**
   * 导出降噪处理过程中的音频数据文件
   */ dump() {
        if (this.checkProcessor()) {
            var _this_processor;
            null === (_this_processor = this.processor) || void 0 === _this_processor || _this_processor.dump();
        }
    }
    /**
   * 获取音频采样率
   */ getSampleRate() {
        return 48000;
    // return this.audioTrack?.getMediaStreamTrack().getSettings().sampleRate;
    }
    /**
   * 获取原始麦克风输入的MediaStream（总是返回未处理的原始输入）
   */ getRawMediaStream() {
        if (!this.audioTrack) return;
        // 直接从audioTrack获取原始麦克风轨道并创建新的MediaStream
        const rawTrack = this.audioTrack.getMediaStreamTrack();
        return new MediaStream([
            rawTrack
        ]);
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    log() {
        for(var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++)args[_key] = arguments[_key];
        if (this.config.debug) console.log(...args);
        return true;
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    warn() {
        for(var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++)args[_key] = arguments[_key];
        if (this.config.debug) console.warn(...args);
        return true;
    }
    checkProcessor() {
        if (!this.processor) {
            // throw new Error('processor is not initialized');
            this.log('processor is not initialized');
            return false;
        }
        return true;
    }
    isSupportAIDenoiser() {
        var _this_config_aiDenoisingConfig;
        return (null === (_this_config_aiDenoisingConfig = this.config.aiDenoisingConfig) || void 0 === _this_config_aiDenoisingConfig ? void 0 : _this_config_aiDenoisingConfig.mode) && void 0 !== PcmRecorder.denoiser;
    }
    constructor(config){
        var _config_aiDenoisingConfig;
        this.recording = false;
        var _config_audioCaptureConfig;
        config.audioCaptureConfig = null !== (_config_audioCaptureConfig = config.audioCaptureConfig) && void 0 !== _config_audioCaptureConfig ? _config_audioCaptureConfig : {};
        var _config_aiDenoisingConfig1;
        config.aiDenoisingConfig = null !== (_config_aiDenoisingConfig1 = config.aiDenoisingConfig) && void 0 !== _config_aiDenoisingConfig1 ? _config_aiDenoisingConfig1 : {};
        this.config = config;
        const { audioCaptureConfig, aiDenoisingConfig } = config;
        if (checkDenoiserSupport(null === (_config_aiDenoisingConfig = config.aiDenoisingConfig) || void 0 === _config_aiDenoisingConfig ? void 0 : _config_aiDenoisingConfig.assetsPath)) {
            PcmRecorder.aiDenoiserSupport = true;
            PcmRecorder.denoiser = window.__denoiser;
        }
        if ((null == aiDenoisingConfig ? void 0 : aiDenoisingConfig.mode) && PcmRecorder.aiDenoiserSupport) {
            // 同时使用两种降噪方案时，则强制开启音频增益控制和禁用自动噪声抑制
            audioCaptureConfig.autoGainControl = true;
            audioCaptureConfig.noiseSuppression = false;
        }
    }
}
PcmRecorder.aiDenoiserSupport = false;
/* ESM default export */ const pcm_recorder = PcmRecorder;
class BaseWsTranscriptionClient {
    async init() {
        if (this.ws) return this.ws;
        const ws = await this.api.websockets.audio.transcriptions.create({
            entity_type: this.config.entityType,
            entity_id: this.config.entityId
        }, this.config.websocketOptions);
        let isResolved = false;
        return new Promise((resolve, reject)=>{
            ws.onopen = ()=>{
                console.debug('[transcription] ws open');
            };
            ws.onmessage = (data)=>{
                // Trigger all registered event listeners
                this.emit(types_WebsocketsEventType.ALL, data);
                this.emit(data.event_type, data);
                if (data.event_type === types_WebsocketsEventType.ERROR) {
                    this.closeWs();
                    if (isResolved) return;
                    isResolved = true;
                    reject(new error_APIError(data.data.code, {
                        code: data.data.code,
                        msg: data.data.msg,
                        detail: data.detail
                    }, data.data.msg, void 0));
                    return;
                }
                if (data.event_type === types_WebsocketsEventType.TRANSCRIPTIONS_CREATED) {
                    resolve(ws);
                    isResolved = true;
                } else if (data.event_type === types_WebsocketsEventType.TRANSCRIPTIONS_MESSAGE_COMPLETED) this.closeWs();
            };
            ws.onerror = (error, event)=>{
                console.error('[transcription] WebSocket error', error, event);
                this.emit('data', error);
                this.emit(types_WebsocketsEventType.ERROR, error);
                this.closeWs();
                if (isResolved) return;
                isResolved = true;
                reject(new error_APIError(error.data.code, error, error.data.msg, void 0));
            };
            ws.onclose = ()=>{
                console.debug('[transcription] ws close');
            };
            this.ws = ws;
        });
    }
    /**
   * 监听一个或多个事件
   * @param event 事件名称或事件名称数组
   * @param callback 回调函数
   */ on(event, callback) {
        const events = Array.isArray(event) ? event : [
            event
        ];
        events.forEach((eventName)=>{
            var _this_listeners_get;
            if (!this.listeners.has(eventName)) this.listeners.set(eventName, new Set());
            null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.add(callback);
        });
    }
    /**
   * 移除一个或多个事件的监听
   * @param event 事件名称或事件名称数组
   * @param callback 回调函数
   */ off(event, callback) {
        const events = Array.isArray(event) ? event : [
            event
        ];
        events.forEach((eventName)=>{
            var _this_listeners_get;
            null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.delete(callback);
        });
    }
    closeWs() {
        var _this_ws;
        if ((null === (_this_ws = this.ws) || void 0 === _this_ws ? void 0 : _this_ws.readyState) === 1) {
            var _this_ws1;
            null === (_this_ws1 = this.ws) || void 0 === _this_ws1 || _this_ws1.close();
        }
        this.ws = null;
    }
    emit(event, data) {
        var _this_listeners_get;
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.forEach((callback)=>callback(data));
    }
    constructor(config){
        this.ws = null;
        this.listeners = new Map();
        this.api = new CozeAPI({
            baseWsURL: COZE_CN_BASE_WS_URL,
            ...config,
            debug: false
        });
        this.recorder = new pcm_recorder({
            audioCaptureConfig: config.audioCaptureConfig,
            aiDenoisingConfig: config.aiDenoisingConfig,
            mediaStreamTrack: config.mediaStreamTrack,
            wavRecordConfig: config.wavRecordConfig,
            deviceId: config.deviceId || void 0,
            debug: config.debug
        });
        this.config = config;
    }
}
/* ESM default export */ const base = BaseWsTranscriptionClient;
class WsTranscriptionClient extends base {
    async connect() {
        var _this_ws;
        await this.init();
        await this.recorder.start();
        const sampleRate = this.recorder.getSampleRate();
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.TRANSCRIPTIONS_UPDATE,
            data: {
                input_audio: {
                    format: 'pcm',
                    codec: 'pcm',
                    sample_rate: sampleRate,
                    channel: 1,
                    bit_depth: 16
                }
            }
        });
    }
    destroy() {
        this.recorder.destroy();
        this.listeners.clear();
        this.closeWs();
    }
    getStatus() {
        if (this.isRecording) {
            if ('ended' === this.recorder.getStatus()) return 'paused';
            return 'recording';
        }
        return 'ended';
    }
    async start() {
        if ('recording' === this.getStatus()) {
            console.warn('Recording is already started');
            return;
        }
        await this.connect();
        await this.recorder.record({
            pcmAudioCallback: (data)=>{
                var _this_ws;
                const { raw } = data;
                // Convert ArrayBuffer to base64 string
                const base64String = btoa(Array.from(new Uint8Array(raw)).map((byte)=>String.fromCharCode(byte)).join(''));
                null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
                    id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
                    event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_APPEND,
                    data: {
                        delta: base64String
                    }
                });
            }
        });
        this.isRecording = true;
    }
    /**
   * 停止录音，提交结果
   */ stop() {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_COMPLETE
        });
        this.recorder.destroy();
        this.closeWs();
        this.isRecording = false;
    }
    /**
   * 暂停录音（保留上下文）
   */ pause() {
        if ('recording' !== this.getStatus()) throw new Error('Recording is not started');
        return this.recorder.pause();
    }
    /**
   * 恢复录音
   */ resume() {
        if ('paused' !== this.getStatus()) throw new Error('Recording is not paused');
        return this.recorder.resume();
    }
    getDenoiserEnabled() {
        return this.recorder.getDenoiserEnabled();
    }
    setDenoiserEnabled(enabled) {
        return this.recorder.setDenoiserEnabled(enabled);
    }
    setDenoiserMode(mode) {
        return this.recorder.setDenoiserMode(mode);
    }
    setDenoiserLevel(level) {
        return this.recorder.setDenoiserLevel(level);
    }
    constructor(...args){
        super(...args), this.isRecording = false;
    }
}
/* ESM default export */ const transcription = WsTranscriptionClient;
var types_ClientEventType = /*#__PURE__*/ function(ClientEventType) {
    /**
   * en: Audio input dump
   * zh: 音频输入 dump
   */ ClientEventType["AUDIO_INPUT_DUMP"] = "audio.input.dump";
    /**
   * en: Audio sentence playback start
   * zh: 句子开始播放
   */ ClientEventType["AUDIO_SENTENCE_PLAYBACK_START"] = "audio.sentence.playback_start";
    /**
   * en: Audio sentence playback ended
   * zh: 句子播放结束
   */ ClientEventType["AUDIO_SENTENCE_PLAYBACK_ENDED"] = "audio.sentence.playback_ended";
    return ClientEventType;
}({});
var types_WsChatEventNames = /*#__PURE__*/ function(WsChatEventNames) {
    /**
   * en: All events
   * zh: 所有事件
   */ WsChatEventNames["ALL"] = "realtime.event";
    /**
   * en: Client connected
   * zh: 客户端连接
   */ WsChatEventNames["CONNECTED"] = "client.connected";
    /**
   * en: Client connecting
   * zh: 客户端连接中
   */ WsChatEventNames["CONNECTING"] = "client.connecting";
    /**
   * en: Client interrupted
   * zh: 客户端中断
   */ WsChatEventNames["INTERRUPTED"] = "client.interrupted";
    /**
   * en: Client disconnected
   * zh: 客户端断开
   */ WsChatEventNames["DISCONNECTED"] = "client.disconnected";
    /**
   * en: Client audio unmuted
   * zh: 客户端音频未静音
   */ WsChatEventNames["AUDIO_UNMUTED"] = "client.audio.unmuted";
    /**
   * en: Client audio muted
   * zh: 客户端音频静音
   */ WsChatEventNames["AUDIO_MUTED"] = "client.audio.muted";
    /**
   * en: Client error
   * zh: 客户端错误
   */ WsChatEventNames["ERROR"] = "client.error";
    /**
   * en: Denoiser enabled
   * zh: 降噪开启
   */ WsChatEventNames["DENOISER_ENABLED"] = "client.denoiser.enabled";
    /**
   * en: Denoiser disabled
   * zh: 降噪关闭
   */ WsChatEventNames["DENOISER_DISABLED"] = "client.denoiser.disabled";
    /**
   * en: Audio input device changed
   * zh: 音频输入设备改变
   */ WsChatEventNames["AUDIO_INPUT_DEVICE_CHANGED"] = "client.input.device.changed";
    /**
   * en: Audio output device changed
   * zh: 音频输出设备改变
   */ WsChatEventNames["AUDIO_OUTPUT_DEVICE_CHANGED"] = "client.output.device.changed";
    /**
   * en: Audio record dump
   * zh: 音频 dump
   */ WsChatEventNames["AUDIO_INPUT_DUMP"] = "client.audio.input.dump";
    /**
   * en: Audio sentence playback start
   * zh: 句子开始播放
   */ WsChatEventNames["AUDIO_SENTENCE_PLAYBACK_START"] = "client.audio.sentence.playback_start";
    /**
   * en: Audio sentence playback ended
   * zh: 句子播放结束
   */ WsChatEventNames["AUDIO_SENTENCE_PLAYBACK_ENDED"] = "client.audio.sentence.playback_ended";
    /**
   * en: Chat created
   * zh: 对话创建成功
   */ WsChatEventNames["CHAT_CREATED"] = "server.chat.created";
    /**
   * en: Chat updated
   * zh: 对话更新
   */ WsChatEventNames["CHAT_UPDATED"] = "server.chat.updated";
    /**
   * en: Conversation chat created
   * zh: 会话对话创建
   */ WsChatEventNames["CONVERSATION_CHAT_CREATED"] = "server.conversation.chat.created";
    /**
   * en: Conversation chat in progress
   * zh: 对话正在处理中
   */ WsChatEventNames["CONVERSATION_CHAT_IN_PROGRESS"] = "server.conversation.chat.in.progress";
    /**
   * en: Conversation message delta received
   * zh: 文本消息增量返回
   */ WsChatEventNames["CONVERSATION_MESSAGE_DELTA"] = "server.conversation.message.delta";
    /**
   * en: Conversation audio delta received
   * zh: 语音消息增量返回
   */ WsChatEventNames["CONVERSATION_AUDIO_DELTA"] = "server.conversation.audio.delta";
    /**
   * en: Conversation message completed
   * zh: 文本消息完成
   */ WsChatEventNames["CONVERSATION_MESSAGE_COMPLETED"] = "server.conversation.message.completed";
    /**
   * en: Conversation audio completed
   * zh: 语音回复完成
   */ WsChatEventNames["CONVERSATION_AUDIO_COMPLETED"] = "server.conversation.audio.completed";
    /**
   * en: Conversation chat completed
   * zh: 对话完成
   */ WsChatEventNames["CONVERSATION_CHAT_COMPLETED"] = "server.conversation.chat.completed";
    /**
   * en: Conversation chat failed
   * zh: 对话失败
   */ WsChatEventNames["CONVERSATION_CHAT_FAILED"] = "server.conversation.chat.failed";
    /**
   * en: Server error occurred
   * zh: 服务端错误
   */ WsChatEventNames["SERVER_ERROR"] = "server.error";
    /**
   * en: Input audio buffer completed
   * zh: 语音输入缓冲区提交完成
   */ WsChatEventNames["INPUT_AUDIO_BUFFER_COMPLETED"] = "server.input_audio_buffer.completed";
    /**
   * en: Input audio buffer cleared
   * zh: 语音输入缓冲区已清除
   */ WsChatEventNames["INPUT_AUDIO_BUFFER_CLEARED"] = "server.input_audio_buffer.cleared";
    /**
   * en: Conversation chat cancelled
   * zh: 对话被取消
   */ WsChatEventNames["CONVERSATION_CHAT_CANCELLED"] = "server.conversation.chat.cancelled";
    /**
   * en: Conversation context cleared
   * zh: 对话上下文已清除
   */ WsChatEventNames["CONVERSATION_CLEARED"] = "server.conversation.cleared";
    /**
   * en: Conversation audio transcript updated
   * zh: 用户语音识别实时字幕更新
   */ WsChatEventNames["CONVERSATION_AUDIO_TRANSCRIPT_UPDATE"] = "server.conversation.audio_transcript.update";
    /**
   * en: Conversation audio transcript completed
   * zh: 用户语音识别完成
   */ WsChatEventNames["CONVERSATION_AUDIO_TRANSCRIPT_COMPLETED"] = "server.conversation.audio_transcript.completed";
    /**
   * en: Conversation chat requires action
   * zh: 对话需要端插件响应
   */ WsChatEventNames["CONVERSATION_CHAT_REQUIRES_ACTION"] = "server.conversation.chat.requires_action";
    /**
   * en: User speech detected - started
   * zh: 检测到用户开始说话
   */ WsChatEventNames["INPUT_AUDIO_BUFFER_SPEECH_STARTED"] = "server.input_audio_buffer.speech_started";
    /**
   * en: User speech detected - stopped
   * zh: 检测到用户停止说话
   */ WsChatEventNames["INPUT_AUDIO_BUFFER_SPEECH_STOPPED"] = "server.input_audio_buffer.speech_stopped";
    /**
   * en: Audio dump
   * zh: 音频 dump
   */ WsChatEventNames["DUMP_AUDIO"] = "server.dump.audio";
    /**
   * en: Audio sentence start
   * zh: 音频句子开始
   */ WsChatEventNames["CONVERSATION_AUDIO_SENTENCE_START"] = "server.conversation.audio.sentence_start";
    return WsChatEventNames;
}({});
/**
 * 音字同步器 - 负责管理音频播放与文本显示的同步
 */ class SentenceSynchronizer {
    /**
   * 设置首个句子首个音频 Delta 时间
   */ setFirstAudioDeltaTime() {
        if (0 === this.currentSentenceIndex && 0 === this.sentenceList[0].audioDuration) this.firstAudioDeltaTime = performance.now();
    }
    /**
   * 处理音频完成事件，标记最后一个句子
   */ handleAudioCompleted() {
        if (this.sentenceList.length > 0) {
            this.sentenceList[this.sentenceList.length - 1].isLastSentence = true;
            this.sentenceList[this.sentenceList.length - 1].isDurationFinish = true;
        }
    }
    /**
   * 处理句子开始事件
   * @param event 句子开始事件
   */ handleSentenceStart(event) {
        // 将句子加入队列，存储文本和初始音频累计时长
        const sentenceItem = {
            id: event.id,
            content: event.data.text,
            audioDuration: 0,
            isLastSentence: false,
            isDurationFinish: false
        };
        this.sentenceList.push(sentenceItem);
        // 如果是首个句子，立即触发客户端句子开始事件
        if (1 === this.sentenceList.length && -1 === this.currentSentenceIndex) {
            this.currentSentenceIndex = 0;
            this.emitSentenceStart(sentenceItem);
            this.scheduleSentenceSwitch();
        } else // 后续句子，更新上一个句子的 isDurationFinish 为 true
        this.sentenceList[this.sentenceList.length - 2].isDurationFinish = true;
    }
    /**
   * 更新指定句子的音频时长
   * @param sentenceId 句子ID
   * @param duration 音频时长增量
   */ updateAudioDuration(sentenceId, duration) {
        const index = this.sentenceList.findIndex((item)=>item.id === sentenceId);
        if (index >= 0) this.sentenceList[index].audioDuration += duration;
    }
    /**
   * 更新最后一个句子的音频时长
   * @param duration 音频时长增量
   * @returns 是否更新成功
   */ updateLatestSentenceAudioDuration(contentLength) {
        if (0 === this.sentenceList.length) return false;
        // 计算音频时长
        // 例如：PCM 16bit 采样率为24000的计算公式: (字节数 / 2) / 24000 * 1000 毫秒
        const bitDepth = 'pcm' === this.outputAudioCodec ? 16 : 8;
        const audioDurationMs = contentLength / (bitDepth / 8) / this.outputAudioSampleRate * 1000;
        const lastSentence = this.sentenceList[this.sentenceList.length - 1];
        lastSentence.audioDuration += audioDurationMs;
        return true;
    }
    /**
   * 安排句子切换
   */ scheduleSentenceSwitch() {
        if (this.sentenceSwitchTimer) clearTimeout(this.sentenceSwitchTimer);
        const { isDurationFinish, isLastSentence, audioDuration } = this.sentenceList[this.currentSentenceIndex];
        let delay = 0;
        if (isDurationFinish) // 第一个句子，更新剩余播放时长
        delay = 0 === this.currentSentenceIndex ? audioDuration - (performance.now() - (this.firstAudioDeltaTime || performance.now())) : audioDuration;
        else {
            delay = 100;
            this.sentenceSwitchTimer = setTimeout(()=>{
                this.scheduleSentenceSwitch();
            }, delay);
            return;
        }
        this.sentenceSwitchTimer = setTimeout(()=>{
            // 判断是否还有后续句子
            const hasNextSentence = this.currentSentenceIndex < this.sentenceList.length - 1;
            if (hasNextSentence) {
                this.currentSentenceIndex++;
                const nextSentence = this.sentenceList[this.currentSentenceIndex];
                this.emitSentenceStart(nextSentence);
            } else {
                if (isLastSentence) this.emitSentenceEnd();
                return;
            }
            this.scheduleSentenceSwitch();
        }, delay);
    }
    /**
   * 发送客户端句子开始事件
   * @param sentenceItem 句子开始事件
   */ emitSentenceStart(sentenceItem) {
        this.eventEmitter(types_WsChatEventNames.AUDIO_SENTENCE_PLAYBACK_START, {
            event_type: types_ClientEventType.AUDIO_SENTENCE_PLAYBACK_START,
            data: {
                content: sentenceItem.content,
                id: sentenceItem.id
            }
        });
    }
    /**
   * 发送客户端句子结束事件
   */ emitSentenceEnd() {
        this.eventEmitter(types_WsChatEventNames.AUDIO_SENTENCE_PLAYBACK_ENDED, {
            event_type: types_ClientEventType.AUDIO_SENTENCE_PLAYBACK_ENDED
        });
    }
    /**
   * 重置句子同步状态
   */ resetSentenceSyncState() {
        if (this.sentenceList.length > 0) // 发送句子结束事件
        this.emitSentenceEnd();
        this.currentSentenceIndex = -1;
        this.sentenceList.length = 0;
        this.firstAudioDeltaTime = null;
        if (this.sentenceSwitchTimer) clearTimeout(this.sentenceSwitchTimer);
        this.sentenceSwitchTimer = null;
    }
    setOutputAudioConfig(outputAudioSampleRate, outputAudioCodec) {
        this.outputAudioSampleRate = outputAudioSampleRate;
        this.outputAudioCodec = outputAudioCodec;
    }
    /**
   * 构造函数
   * @param options 同步器配置选项
   */ constructor(options){
        /** 句子列表队列 */ this.sentenceList = [];
        /** 首个音频delta的时间戳（用于计算实际经过的时间）*/ this.firstAudioDeltaTime = null;
        // 当前播放的句子索引
        this.currentSentenceIndex = -1;
        // 句子切换定时器
        this.sentenceSwitchTimer = null;
        this.eventEmitter = options.eventEmitter;
        this.scheduleSentenceSwitch = this.scheduleSentenceSwitch.bind(this);
        this.emitSentenceStart = this.emitSentenceStart.bind(this);
        this.emitSentenceEnd = this.emitSentenceEnd.bind(this);
        this.outputAudioSampleRate = 24000;
        this.outputAudioCodec = 'pcm';
    }
}
/* ESM default export */ const sentence_synchronizer = SentenceSynchronizer;
// @ts-expect-error no types
// @ts-expect-error no types
class OpusDecoder {
    /**
   * Decode Opus data to PCM audio
   * @param data Opus encoded data as Uint8Array
   * @returns Decoded PCM audio as Int16Array or null if error
   */ decode(inputBuffer) {
        try {
            let decodedBuffer;
            this.decoder.decodeRaw(inputBuffer, (outputBuffer)=>{
                decodedBuffer = outputBuffer;
            });
            if (!decodedBuffer) return null;
            // 转成 Int16Array
            const decodedBufferInt16 = new Int16Array(decodedBuffer.length);
            for(let i = 0; i < decodedBuffer.length; i++)decodedBufferInt16[i] = 0x8000 * decodedBuffer[i];
            return decodedBufferInt16;
        } catch (error) {
            console.error('Error decoding Opus data:', error);
            return null;
        }
    }
    /**
   * Check if the decoder is ready
   */ isReady() {
        return this.decoderReady;
    }
    /**
   * Wait for the decoder to be ready
   * @returns Promise that resolves when decoder is ready
   */ async waitForReady() {
        if (this.decoderReady) return Promise.resolve();
        return new Promise((resolve)=>{
            const checkInterval = setInterval(()=>{
                if (this.decoderReady) {
                    clearInterval(checkInterval);
                    resolve();
                }
            }, 10);
        });
    }
    /**
   * Release resources used by the decoder
   */ destroy() {
        if (this.decoder) this.decoder.destroy();
    }
    constructor(config){
        this.decoderReady = false;
        this.config = config;
        this.decoder = new __WEBPACK_EXTERNAL_MODULE_opus_encdec_src_oggOpusDecoder_js__.OggOpusDecoder({
            rawOpus: true,
            numberOfChannels: 1,
            decoderSampleRate: this.config.inputSampleRate || 24000,
            outputBufferSampleRate: this.config.outputSampleRate || 24000
        }, __WEBPACK_EXTERNAL_MODULE_opus_encdec_dist_libopus_decoder_js__["default"]);
        // Initialize the decoder
        if (false === this.decoder.isReady && this.decoder.onready) this.decoder.onready = ()=>{
            this.decoderReady = true;
        };
        else this.decoderReady = true;
    }
}
/* ESM default export */ const opus_decoder = OpusDecoder;
class BaseWsChatClient {
    async init() {
        if (this.ws) return this.ws;
        const ws = await this.api.websockets.chat.create({
            bot_id: this.config.botId,
            workflow_id: this.config.workflowId
        }, this.config.websocketOptions);
        this.ws = ws;
        // 标记 websocket 是否已 resolve or reject
        let isResolved = false;
        this.trackId = `my-track-id-${(0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)()}`;
        return new Promise((resolve, reject)=>{
            ws.onopen = ()=>{
                this.log('ws open');
            };
            ws.onmessage = (data)=>{
                // Trigger all registered event listeners
                this.emit(`server.${data.event_type}`, data);
                switch(data.event_type){
                    case types_WebsocketsEventType.ERROR:
                        this.closeWs();
                        if (isResolved) return;
                        isResolved = true;
                        reject(new error_APIError(data.data.code, {
                            code: data.data.code,
                            msg: data.data.msg,
                            detail: data.detail
                        }, void 0, void 0));
                        return;
                    case types_WebsocketsEventType.CHAT_CREATED:
                        resolve(ws);
                        isResolved = true;
                        break;
                    case types_WebsocketsEventType.CONVERSATION_AUDIO_DELTA:
                        this.audioDeltaList.push(data.data.content);
                        if (1 === this.audioDeltaList.length) this.handleAudioMessage();
                        break;
                    case types_WebsocketsEventType.CONVERSATION_AUDIO_SENTENCE_START:
                        this.sentenceSynchronizer.handleSentenceStart(data);
                        break;
                    case types_WebsocketsEventType.INPUT_AUDIO_BUFFER_SPEECH_STARTED:
                        // 打断当前播放
                        this.clear();
                        break;
                    case types_WebsocketsEventType.CONVERSATION_AUDIO_COMPLETED:
                        this.sentenceSynchronizer.handleAudioCompleted();
                        break;
                    case types_WebsocketsEventType.CONVERSATION_CHAT_CANCELED:
                        this.clear();
                        break;
                    default:
                        break;
                }
            };
            ws.onerror = (error, event)=>{
                this.warn('ws error', error, event);
                this.emit(`server.${types_WebsocketsEventType.ERROR}`, error);
                this.closeWs();
                if (isResolved) return;
                isResolved = true;
                reject(new error_APIError(error.data.code, error, error.data.msg, void 0));
            };
            ws.onclose = ()=>{
                this.log('ws close');
            };
        });
    }
    sendMessage(data) {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send(data);
        this.log('sendMessage', data);
    }
    sendTextMessage(text) {
        this.sendMessage({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.CONVERSATION_MESSAGE_CREATE,
            data: {
                role: chat_RoleType.User,
                content_type: 'text',
                content: text
            }
        });
    }
    /**
   * en: Add event listener(s)
   * zh: 添加事件监听器
   * @param event - string | string[] Event name or array of event names
   * @param callback - Event callback function
   */ on(event, callback) {
        const events = Array.isArray(event) ? event : [
            event
        ];
        events.forEach((eventName)=>{
            var _this_listeners_get;
            if (!this.listeners.has(eventName)) this.listeners.set(eventName, new Set());
            null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.add(callback);
            this.log('on', eventName);
        });
    }
    /**
   * en: Remove event listener(s)
   * zh: 移除事件监听器
   * @param event - string | string[] Event name or array of event names
   * @param callback - Event callback function to remove
   */ off(event, callback) {
        const events = Array.isArray(event) ? event : [
            event
        ];
        events.forEach((eventName)=>{
            var _this_listeners_get;
            null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.delete(callback);
        });
    }
    async initOpusDecoder() {
        this.opusDecoder = new opus_decoder({
            inputSampleRate: this.outputAudioSampleRate,
            outputSampleRate: this.outputAudioSampleRate
        });
        await this.opusDecoder.waitForReady();
    }
    closeWs() {
        var _this_ws;
        if ((null === (_this_ws = this.ws) || void 0 === _this_ws ? void 0 : _this_ws.readyState) === 1) {
            var _this_ws1;
            null === (_this_ws1 = this.ws) || void 0 === _this_ws1 || _this_ws1.close();
        }
        this.ws = null;
    }
    async clear() {
        var _this_wavStreamPlayer;
        this.audioDeltaList = [];
        // 打断当前播放
        this.trackId = `my-track-id-${(0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)()}`;
        await (null === (_this_wavStreamPlayer = this.wavStreamPlayer) || void 0 === _this_wavStreamPlayer ? void 0 : _this_wavStreamPlayer.interrupt());
        // 重置音字同步状态
        this.sentenceSynchronizer.resetSentenceSyncState();
    }
    emit(eventName, event) {
        var _this_listeners_get, _this_listeners_get1;
        null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.forEach((callback)=>callback(eventName, event));
        null === (_this_listeners_get1 = this.listeners.get(types_WsChatEventNames.ALL)) || void 0 === _this_listeners_get1 || _this_listeners_get1.forEach((callback)=>callback(eventName, event));
        this.log('dispatch', eventName, event);
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    log() {
        for(var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++)args[_key] = arguments[_key];
        if (this.config.debug) console.log('[WsChatClient]', ...args);
        return true;
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    warn() {
        for(var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++)args[_key] = arguments[_key];
        if (this.config.debug) console.warn('[WsChatClient]', ...args);
        return true;
    }
    constructor(config){
        this.ws = null;
        this.listeners = new Map();
        this.trackId = 'default';
        this.audioDeltaList = [];
        this.outputAudioCodec = 'pcm';
        this.outputAudioSampleRate = 24000;
        this.isConnected = false;
        this.handleAudioMessage = async ()=>{
            if (!this.isConnected) {
                // 确保初始化已完成
                setTimeout(()=>{
                    this.warn('WsChatClient is not connected');
                    this.handleAudioMessage();
                }, 100);
                return;
            }
            const message = this.audioDeltaList[0];
            const decodedContent = atob(message);
            const arrayBuffer = new ArrayBuffer(decodedContent.length);
            const view = new Uint8Array(arrayBuffer);
            for(let i = 0; i < decodedContent.length; i++)view[i] = decodedContent.charCodeAt(i);
            try {
                var _this_wavStreamPlayer;
                let int16Array;
                if (this.opusDecoder) {
                    // Decode the Opus data to PCM audio
                    int16Array = this.opusDecoder.decode(view);
                    if (!int16Array) throw new Error('Failed to decode Opus data');
                }
                // 设置首个音频 Delta 时间
                this.sentenceSynchronizer.setFirstAudioDeltaTime();
                // 更新最后一个句子的音频时长
                this.sentenceSynchronizer.updateLatestSentenceAudioDuration(int16Array ? int16Array.length : decodedContent.length);
                await (null === (_this_wavStreamPlayer = this.wavStreamPlayer) || void 0 === _this_wavStreamPlayer ? void 0 : _this_wavStreamPlayer.add16BitPCM(int16Array || arrayBuffer, this.trackId));
                this.audioDeltaList.shift();
                if (this.audioDeltaList.length > 0) await this.handleAudioMessage();
            } catch (error) {
                this.warn('wavStreamPlayer error', error);
            }
        };
        this.api = new CozeAPI({
            baseWsURL: COZE_CN_BASE_WS_URL,
            ...config,
            debug: false
        });
        this.config = config;
        // 初始化音字同步器，传入事件发射器
        this.sentenceSynchronizer = new sentence_synchronizer({
            eventEmitter: (eventName, eventData)=>this.emit(eventName, eventData)
        });
    }
}
/* ESM default export */ const chat_base = BaseWsChatClient;
class WsChatClient extends chat_base {
    async startRecord() {
        var _this_wavStreamPlayer, _this_wavStreamPlayer1;
        if ('recording' === this.recorder.getStatus()) {
            console.warn('Recorder is already recording');
            return;
        }
        // 如果是客户端判停，需要先取消当前的播放
        if ('client_interrupt' === this.turnDetection) this.interrupt();
        // 1. start recorder
        await this.recorder.start(this.inputAudioCodec);
        // 获取原始麦克风输入用于本地回环
        const rawMediaStream = this.recorder.getRawMediaStream();
        if (!rawMediaStream) throw new Error('无法获取原始麦克风输入');
        null === (_this_wavStreamPlayer = this.wavStreamPlayer) || void 0 === _this_wavStreamPlayer || _this_wavStreamPlayer.setMediaStream(rawMediaStream);
        // init stream player
        await (null === (_this_wavStreamPlayer1 = this.wavStreamPlayer) || void 0 === _this_wavStreamPlayer1 ? void 0 : _this_wavStreamPlayer1.add16BitPCM(new ArrayBuffer(0), this.trackId));
        // 2. recording
        await this.recorder.record({
            pcmAudioCallback: (data)=>{
                var _this_ws;
                const { raw } = data;
                // Convert ArrayBuffer to base64 string
                const base64String = btoa(Array.from(new Uint8Array(raw)).map((byte)=>String.fromCharCode(byte)).join(''));
                null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
                    id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
                    event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_APPEND,
                    data: {
                        delta: base64String
                    }
                });
            // this.log('input_audio_buffer_append', performance.now() - startTime);
            // startTime = performance.now();
            },
            wavAudioCallback: (blob, name)=>{
                const event = {
                    event_type: types_ClientEventType.AUDIO_INPUT_DUMP,
                    data: {
                        name,
                        wav: blob
                    }
                };
                this.emit(types_WsChatEventNames.AUDIO_INPUT_DUMP, event);
            },
            dumpAudioCallback: (blob, name)=>{
                const event = {
                    event_type: types_ClientEventType.AUDIO_INPUT_DUMP,
                    data: {
                        name,
                        wav: blob
                    }
                };
                this.emit(types_WsChatEventNames.AUDIO_INPUT_DUMP, event);
            }
        });
    }
    stopRecord() {
        var _this_ws;
        if ('recording' !== this.recorder.getStatus()) {
            console.warn('Recorder is not recording');
            return;
        }
        this.recorder.destroy();
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: Date.now().toString(),
            event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_COMPLETE
        });
    }
    async connect() {
        let { chatUpdate } = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
        var _this_recorder, _event_data_input_audio, _event_data, _event_data_output_audio, _event_data1, _event_data_output_audio1, _event_data2, _this_wavStreamPlayer, _this_wavStreamPlayer1, _event_data_turn_detection, _event_data3;
        const ws = await this.init();
        this.ws = ws;
        const sampleRate = await (null === (_this_recorder = this.recorder) || void 0 === _this_recorder ? void 0 : _this_recorder.getSampleRate());
        const event = {
            id: (null == chatUpdate ? void 0 : chatUpdate.id) || (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.CHAT_UPDATE,
            data: {
                input_audio: {
                    format: 'pcm',
                    codec: 'pcm'
                },
                output_audio: {
                    codec: 'pcm',
                    pcm_config: {
                        sample_rate: 24000
                    }
                },
                turn_detection: {
                    type: 'server_vad'
                },
                need_play_prologue: true,
                ...null == chatUpdate ? void 0 : chatUpdate.data
            }
        };
        if (this.config.voiceId) setValueByPath(event, 'data.output_audio.voice_id', this.config.voiceId);
        // 强制设置输入音频的采样率为系统默认的采样率
        setValueByPath(event, 'data.input_audio.sample_rate', sampleRate);
        this.inputAudioCodec = (null === (_event_data = event.data) || void 0 === _event_data ? void 0 : null === (_event_data_input_audio = _event_data.input_audio) || void 0 === _event_data_input_audio ? void 0 : _event_data_input_audio.codec) || 'pcm';
        this.outputAudioCodec = (null === (_event_data1 = event.data) || void 0 === _event_data1 ? void 0 : null === (_event_data_output_audio = _event_data1.output_audio) || void 0 === _event_data_output_audio ? void 0 : _event_data_output_audio.codec) || 'pcm';
        if ('opus' === this.outputAudioCodec) {
            var _event_data_output_audio_opus_config, _event_data_output_audio2, _event_data4;
            this.outputAudioSampleRate = (null === (_event_data4 = event.data) || void 0 === _event_data4 ? void 0 : null === (_event_data_output_audio2 = _event_data4.output_audio) || void 0 === _event_data_output_audio2 ? void 0 : null === (_event_data_output_audio_opus_config = _event_data_output_audio2.opus_config) || void 0 === _event_data_output_audio_opus_config ? void 0 : _event_data_output_audio_opus_config.sample_rate) || 24000;
            await this.initOpusDecoder();
        } else {
            var _event_data_output_audio_pcm_config, _event_data_output_audio3, _event_data5;
            this.outputAudioSampleRate = (null === (_event_data5 = event.data) || void 0 === _event_data5 ? void 0 : null === (_event_data_output_audio3 = _event_data5.output_audio) || void 0 === _event_data_output_audio3 ? void 0 : null === (_event_data_output_audio_pcm_config = _event_data_output_audio3.pcm_config) || void 0 === _event_data_output_audio_pcm_config ? void 0 : _event_data_output_audio_pcm_config.sample_rate) || 24000;
        }
        null === (_this_wavStreamPlayer = this.wavStreamPlayer) || void 0 === _this_wavStreamPlayer || _this_wavStreamPlayer.setDefaultFormat((null === (_event_data2 = event.data) || void 0 === _event_data2 ? void 0 : null === (_event_data_output_audio1 = _event_data2.output_audio) || void 0 === _event_data_output_audio1 ? void 0 : _event_data_output_audio1.codec) || 'pcm');
        null === (_this_wavStreamPlayer1 = this.wavStreamPlayer) || void 0 === _this_wavStreamPlayer1 || _this_wavStreamPlayer1.setSampleRate(this.outputAudioSampleRate);
        this.sentenceSynchronizer.setOutputAudioConfig(this.outputAudioSampleRate, this.outputAudioCodec);
        // Turn detection mode: server_vad (server-side detection) or client_interrupt (client-side detection; requires manual startRecord/stopRecord)
        this.turnDetection = (null === (_event_data3 = event.data) || void 0 === _event_data3 ? void 0 : null === (_event_data_turn_detection = _event_data3.turn_detection) || void 0 === _event_data_turn_detection ? void 0 : _event_data_turn_detection.type) || 'server_vad';
        this.ws.send(event);
        if (!this.isMuted && 'client_interrupt' !== this.turnDetection) await this.startRecord();
        this.emit(types_WsChatEventNames.CONNECTED, event);
        this.isConnected = true;
    }
    async disconnect() {
        var _this_ws, _this_recorder, _this_wavStreamPlayer, _this_opusDecoder;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.CONVERSATION_CHAT_CANCEL
        });
        await (null === (_this_recorder = this.recorder) || void 0 === _this_recorder ? void 0 : _this_recorder.destroy());
        await (null === (_this_wavStreamPlayer = this.wavStreamPlayer) || void 0 === _this_wavStreamPlayer ? void 0 : _this_wavStreamPlayer.destroy());
        await (null === (_this_opusDecoder = this.opusDecoder) || void 0 === _this_opusDecoder ? void 0 : _this_opusDecoder.destroy());
        this.emit(types_WsChatEventNames.DISCONNECTED, void 0);
        await new Promise((resolve)=>setTimeout(resolve, 500));
        this.listeners.clear();
        this.closeWs();
        this.isConnected = false;
    }
    /**
   * en: Set the audio enable
   * zh: 设置是否静音
   * @param enable - The enable to set
   */ async setAudioEnable(enable) {
        var _this_recorder;
        if ('client_interrupt' === this.turnDetection) throw new Error('Client interrupt mode does not support setAudioEnable');
        const status = await (null === (_this_recorder = this.recorder) || void 0 === _this_recorder ? void 0 : _this_recorder.getStatus());
        if (enable) {
            if ('ended' === status) {
                if (this.recorder.audioTrack) {
                    var _this_recorder1;
                    await (null === (_this_recorder1 = this.recorder) || void 0 === _this_recorder1 ? void 0 : _this_recorder1.resume());
                } else await this.startRecord();
                this.isMuted = false;
                this.emit(types_WsChatEventNames.AUDIO_UNMUTED, void 0);
            } else this.warn('recorder is not ended with status', status);
        } else if ('recording' === status) {
            await this.recorder.pause();
            this.isMuted = true;
            this.emit(types_WsChatEventNames.AUDIO_MUTED, void 0);
        } else this.warn('recorder is not recording with status', status);
    }
    /**
   * en: Set the audio input device
   * zh: 设置音频输入设备
   * @param deviceId - The device ID to set
   */ async setAudioInputDevice(deviceId) {
        if ('ended' !== this.recorder.getStatus()) await this.recorder.destroy();
        const devices = await getAudioDevices();
        if ('default' === deviceId) {
            this.recorder.config.deviceId = void 0;
            if (!this.isMuted) await this.startRecord();
            this.emit(types_WsChatEventNames.AUDIO_INPUT_DEVICE_CHANGED, void 0);
        } else {
            const device = devices.audioInputs.find((d)=>d.deviceId === deviceId);
            if (!device) throw new Error(`Device with id ${deviceId} not found`);
            this.recorder.config.deviceId = device.deviceId;
            if (!this.isMuted) await this.startRecord();
            this.emit(types_WsChatEventNames.AUDIO_INPUT_DEVICE_CHANGED, void 0);
        }
    }
    /**
   * en: Interrupt the conversation
   * zh: 打断对话
   */ interrupt() {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.CONVERSATION_CHAT_CANCEL
        });
        this.emit(types_WsChatEventNames.INTERRUPTED, void 0);
    }
    /**
   * en: Set the playback volume
   * zh: 设置播放音量
   * @param volume - The volume level to set (0.0 to 1.0)
   */ setPlaybackVolume(volume) {
        var _this_wavStreamPlayer;
        this.playbackVolume = Math.max(0, Math.min(1, volume));
        null === (_this_wavStreamPlayer = this.wavStreamPlayer) || void 0 === _this_wavStreamPlayer || _this_wavStreamPlayer.setVolume(this.playbackVolume);
    }
    /**
   * en: Get the playback volume
   * zh: 获取播放音量
   * @returns The current volume level (0.0 to 1.0)
   */ getPlaybackVolume() {
        var _this_wavStreamPlayer;
        var _this_wavStreamPlayer_getVolume;
        return null !== (_this_wavStreamPlayer_getVolume = null === (_this_wavStreamPlayer = this.wavStreamPlayer) || void 0 === _this_wavStreamPlayer ? void 0 : _this_wavStreamPlayer.getVolume()) && void 0 !== _this_wavStreamPlayer_getVolume ? _this_wavStreamPlayer_getVolume : 0;
    }
    /**
   * en: Set the denoiser enabled
   * zh: 设置是否启用降噪
   * @param enabled - The enabled to set
   */ setDenoiserEnabled(enabled) {
        this.recorder.setDenoiserEnabled(enabled);
        if (enabled) this.emit(types_WsChatEventNames.DENOISER_ENABLED, void 0);
        else this.emit(types_WsChatEventNames.DENOISER_DISABLED, void 0);
    }
    /**
   * en: Set the denoiser level
   * zh: 设置降噪等级
   * @param level - The level to set
   */ setDenoiserLevel(level) {
        this.log('setDenoiserLevel', level);
        this.recorder.setDenoiserLevel(level);
    }
    /**
   * en: Set the denoiser mode
   * zh: 设置降噪模式
   * @param mode - The mode to set
   */ setDenoiserMode(mode) {
        this.log('setDenoiserMode', mode);
        this.recorder.setDenoiserMode(mode);
    }
    constructor(config){
        super(config), this.isMuted = false, this.inputAudioCodec = 'pcm', this.turnDetection = 'server_vad', this.playbackVolume = 1.0;
        var _config_enableLocalLoopback;
        const isMobilePlayer = null !== (_config_enableLocalLoopback = config.enableLocalLoopback) && void 0 !== _config_enableLocalLoopback ? _config_enableLocalLoopback : isHarmonOS();
        var _config_playbackVolumeDefault;
        this.wavStreamPlayer = new WavStreamPlayer({
            sampleRate: 24000,
            enableLocalLoopback: isMobilePlayer,
            volume: null !== (_config_playbackVolumeDefault = config.playbackVolumeDefault) && void 0 !== _config_playbackVolumeDefault ? _config_playbackVolumeDefault : 1
        });
        this.recorder = new pcm_recorder({
            audioCaptureConfig: config.audioCaptureConfig,
            aiDenoisingConfig: config.aiDenoisingConfig,
            mediaStreamTrack: config.mediaStreamTrack,
            wavRecordConfig: config.wavRecordConfig,
            debug: config.debug,
            deviceId: config.deviceId
        });
        var _config_audioMutedDefault;
        this.isMuted = null !== (_config_audioMutedDefault = config.audioMutedDefault) && void 0 !== _config_audioMutedDefault && _config_audioMutedDefault;
    }
}
/* ESM default export */ const ws_tools_chat = WsChatClient;
class BaseWsSimultInterpretationClient {
    async init() {
        if (this.ws) return this.ws;
        const ws = await this.api.websockets.audio.simultInterpretation.create(this.config.websocketOptions);
        let isResolved = false;
        return new Promise((resolve, reject)=>{
            ws.onopen = ()=>{
                console.debug('[simult interpretation] ws open');
            };
            ws.onmessage = (data)=>{
                // Trigger all registered event listeners
                this.emit(types_WebsocketsEventType.ALL, data);
                this.emit(data.event_type, data);
                if (data.event_type === types_WebsocketsEventType.ERROR) {
                    this.closeWs();
                    if (isResolved) return;
                    isResolved = true;
                    reject(new error_APIError(data.data.code, {
                        code: data.data.code,
                        msg: data.data.msg,
                        detail: data.detail
                    }, data.data.msg, void 0));
                    return;
                }
                if (data.event_type === types_WebsocketsEventType.SIMULT_INTERPRETATION_CREATED) {
                    resolve(ws);
                    isResolved = true;
                } else if (data.event_type === types_WebsocketsEventType.SIMULT_INTERPRETATION_MESSAGE_COMPLETED) this.closeWs();
            };
            ws.onerror = (error, event)=>{
                console.error('[simult interpretation] WebSocket error', error, event);
                this.emit('data', error);
                this.emit(types_WebsocketsEventType.ERROR, error);
                this.closeWs();
                if (isResolved) return;
                isResolved = true;
                reject(new error_APIError(error.data.code, error, error.data.msg, void 0));
            };
            ws.onclose = ()=>{
                console.debug('[simult interpretation] ws close');
            };
            this.ws = ws;
        });
    }
    /**
   * 监听一个或多个事件
   * @param event 事件名称或事件名称数组
   * @param callback 回调函数
   */ on(event, callback) {
        const events = Array.isArray(event) ? event : [
            event
        ];
        events.forEach((eventName)=>{
            var _this_listeners_get;
            if (!this.listeners.has(eventName)) this.listeners.set(eventName, new Set());
            null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.add(callback);
        });
    }
    /**
   * 移除一个或多个事件的监听
   * @param event 事件名称或事件名称数组
   * @param callback 回调函数
   */ off(event, callback) {
        const events = Array.isArray(event) ? event : [
            event
        ];
        events.forEach((eventName)=>{
            var _this_listeners_get;
            null === (_this_listeners_get = this.listeners.get(eventName)) || void 0 === _this_listeners_get || _this_listeners_get.delete(callback);
        });
    }
    closeWs() {
        var _this_ws;
        if ((null === (_this_ws = this.ws) || void 0 === _this_ws ? void 0 : _this_ws.readyState) === 1) {
            var _this_ws1;
            null === (_this_ws1 = this.ws) || void 0 === _this_ws1 || _this_ws1.close();
        }
        this.ws = null;
    }
    emit(event, data) {
        var _this_listeners_get;
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.forEach((callback)=>callback(data));
    }
    constructor(config){
        this.ws = null;
        this.listeners = new Map();
        this.api = new CozeAPI({
            baseWsURL: COZE_CN_BASE_WS_URL,
            ...config,
            debug: false
        });
        this.recorder = new pcm_recorder({
            audioCaptureConfig: config.audioCaptureConfig,
            aiDenoisingConfig: config.aiDenoisingConfig,
            mediaStreamTrack: config.mediaStreamTrack,
            wavRecordConfig: config.wavRecordConfig,
            deviceId: config.deviceId || 'default',
            debug: config.debug
        });
        this.config = config;
    }
}
/* ESM default export */ const simult_interpretation_base = BaseWsSimultInterpretationClient;
class WsSimultInterpretationClient extends simult_interpretation_base {
    async connect(simultUpdate) {
        var _simultUpdate_data_output_audio, _simultUpdate_data, _this_ws;
        await this.init();
        await this.recorder.start();
        const sampleRate = this.recorder.getSampleRate();
        if ((null == simultUpdate ? void 0 : null === (_simultUpdate_data = simultUpdate.data) || void 0 === _simultUpdate_data ? void 0 : null === (_simultUpdate_data_output_audio = _simultUpdate_data.output_audio) || void 0 === _simultUpdate_data_output_audio ? void 0 : _simultUpdate_data_output_audio.voice_id) === '') simultUpdate.data.output_audio.voice_id = void 0;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.SIMULT_INTERPRETATION_UPDATE,
            data: {
                input_audio: {
                    format: 'pcm',
                    codec: 'pcm',
                    sample_rate: sampleRate,
                    channel: 1,
                    bit_depth: 16
                },
                output_audio: {
                    codec: 'pcm',
                    pcm_config: {
                        sample_rate: 24000
                    }
                },
                ...null == simultUpdate ? void 0 : simultUpdate.data
            }
        });
    }
    destroy() {
        this.recorder.destroy();
        this.listeners.clear();
        this.closeWs();
    }
    getStatus() {
        if (this.isRecording) {
            if ('ended' === this.recorder.getStatus()) return 'paused';
            return 'recording';
        }
        return 'ended';
    }
    async start(chatUpdate) {
        if ('recording' === this.getStatus()) {
            console.warn('Recording is already started');
            return;
        }
        await this.connect(chatUpdate);
        await this.recorder.record({
            pcmAudioCallback: (data)=>{
                var _this_ws;
                const { raw } = data;
                // Convert ArrayBuffer to base64 string
                const base64String = btoa(Array.from(new Uint8Array(raw)).map((byte)=>String.fromCharCode(byte)).join(''));
                null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
                    id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
                    event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_APPEND,
                    data: {
                        delta: base64String
                    }
                });
            }
        });
        this.isRecording = true;
    }
    /**
   * 停止录音，提交结果
   */ stop() {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_COMPLETE
        });
        this.recorder.destroy();
        this.closeWs();
        this.isRecording = false;
    }
    /**
   * 暂停录音（保留上下文）
   */ pause() {
        if ('recording' !== this.getStatus()) throw new Error('Recording is not started');
        return this.recorder.pause();
    }
    /**
   * 恢复录音
   */ resume() {
        if ('paused' !== this.getStatus()) throw new Error('Recording is not paused');
        return this.recorder.resume();
    }
    getDenoiserEnabled() {
        return this.recorder.getDenoiserEnabled();
    }
    setDenoiserEnabled(enabled) {
        return this.recorder.setDenoiserEnabled(enabled);
    }
    setDenoiserMode(mode) {
        return this.recorder.setDenoiserMode(mode);
    }
    setDenoiserLevel(level) {
        return this.recorder.setDenoiserLevel(level);
    }
    constructor(...args){
        super(...args), this.isRecording = false;
    }
}
/* ESM default export */ const simult_interpretation = WsSimultInterpretationClient;
class PcmPlayer {
    /**
   * Initializes the PCM player with a new track ID. This method must be called before using append.
   * After calling interrupt, init must be called again to reinitialize the player.
   * @param {Object} options - Initialization options
   * @param {boolean} [options.isPauseDefault=false] - Whether to start in paused state
   */ init() {
        this.trackId = `my-track-id-${(0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)()}`;
        this.totalDuration = 0;
        if (this.playbackTimeout) {
            clearTimeout(this.playbackTimeout);
            this.playbackTimeout = null;
        }
        this.playbackStartTime = null;
        this.wavStreamPlayer.add16BitPCM(new ArrayBuffer(0), this.trackId);
        if (this.isPauseDefault) this.pause();
    }
    /**
   * Destroys the PCM player instance and cleans up resources.
   * Should be called when the page is unloaded or when the instance is no longer needed.
   */ async destroy() {
        if (this.playbackTimeout) clearTimeout(this.playbackTimeout);
        await this.wavStreamPlayer.interrupt();
    }
    /**
   * Completes the current playback and triggers the onCompleted callback after remaining duration.
   * @private
   */ complete() {
        if (this.playbackStartTime) {
            // 剩余时间 = 总时间 - 已播放时间 - 已暂停时间
            const now = new Date().getTime();
            const remaining = this.totalDuration - (now - this.playbackStartTime) / 1000 - this.elapsedBeforePause;
            this.playbackTimeout = setTimeout(()=>{
                this.onCompleted();
                this.playbackStartTime = null;
                this.elapsedBeforePause = 0;
            }, 1000 * remaining);
        }
    }
    /**
   * Interrupts the current playback. Any audio appended after interrupt will not play
   * until init is called again to reinitialize the player.
   */ async interrupt() {
        await this.destroy();
        this.onCompleted();
    }
    /**
   * Pauses the current playback. The playback can be resumed from the paused position
   * by calling resume.
   */ async pause() {
        if (this.playbackTimeout) {
            clearTimeout(this.playbackTimeout);
            this.playbackTimeout = null;
        }
        if (this.playbackStartTime && !this.playbackPauseTime) {
            this.playbackPauseTime = Date.now();
            this.elapsedBeforePause += (this.playbackPauseTime - this.playbackStartTime) / 1000;
        }
        await this.wavStreamPlayer.pause();
    }
    /**
   * Resumes playback from the last paused position.
   */ async resume() {
        if (this.playbackPauseTime) {
            this.playbackStartTime = Date.now();
            this.playbackPauseTime = null;
            // Update the timeout with remaining duration
            if (this.playbackTimeout) clearTimeout(this.playbackTimeout);
            const remaining = this.totalDuration - this.elapsedBeforePause;
            this.playbackTimeout = setTimeout(()=>{
                this.onCompleted();
                console.debug('[pcm player] completed', this.totalDuration);
                this.playbackStartTime = null;
                this.elapsedBeforePause = 0;
            }, 1000 * remaining);
        }
        await this.wavStreamPlayer.resume();
    }
    /**
   * Toggles between play and pause states.
   * If currently playing, it will pause; if paused, it will resume playback.
   */ async togglePlay() {
        if (this.isPlaying()) await this.pause();
        else await this.resume();
    }
    /**
   * Checks if audio is currently playing.
   * @returns {boolean} True if audio is playing, false otherwise
   */ isPlaying() {
        return this.wavStreamPlayer.isPlaying();
    }
    /**
   * Appends and plays a base64 encoded PCM audio chunk.
   * Must call init before using this method.
   * @param {string} message - Base64 encoded PCM audio data
   */ async append(message) {
        const decodedContent = atob(message);
        const arrayBuffer = new ArrayBuffer(decodedContent.length);
        const view = new Uint8Array(arrayBuffer);
        for(let i = 0; i < decodedContent.length; i++)view[i] = decodedContent.charCodeAt(i);
        // Calculate duration in seconds
        const bytesPerSecond = 48000; // sampleRate * channels * (bitDepth/8)
        const duration = arrayBuffer.byteLength / bytesPerSecond;
        this.totalDuration += duration;
        try {
            await this.wavStreamPlayer.add16BitPCM(arrayBuffer, this.trackId);
            // Start or update the playback timer
            if (!this.playbackStartTime && !this.playbackPauseTime) {
                this.playbackStartTime = Date.now();
                this.elapsedBeforePause = 0;
            }
        } catch (error) {
            console.warn('[pcm player] error', error);
        }
    }
    constructor({ onCompleted, isPauseDefault = false }){
        this.trackId = 'default';
        this.totalDuration = 0;
        this.playbackStartTime = null;
        this.playbackPauseTime = null;
        this.playbackTimeout = null;
        this.elapsedBeforePause = 0;
        this.isPauseDefault = false;
        this.wavStreamPlayer = new WavStreamPlayer({
            sampleRate: 24000
        });
        this.onCompleted = onCompleted;
        this.isPauseDefault = isPauseDefault;
    }
}
/* ESM default export */ const pcm_player = PcmPlayer;
export { pcm_recorder_AIDenoiserProcessorLevel as AIDenoiserProcessorLevel, pcm_recorder_AIDenoiserProcessorMode as AIDenoiserProcessorMode, types_ClientEventType as ClientEventType, pcm_player as PcmPlayer, pcm_recorder as PcmRecorder, ws_tools_chat as WsChatClient, types_WsChatEventNames as WsChatEventNames, simult_interpretation as WsSimultInterpretationClient, speech as WsSpeechClient, utils_namespaceObject as WsToolsUtils, transcription as WsTranscriptionClient };
